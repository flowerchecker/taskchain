{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TaskChain What is a TaskChain? TaskChain is a tool for managing data processing pipelines. It was created to reduce chaos in machine learning projects . Goals and features: separate computation logic and configuration every result should be reproducible brake down computation to individual steps in DAG structure brake down whole project to smaller pipelines which can be easily configured and reused never compute same thing twice - result of computation steps is saved automatically (data persistence) easy access to all intermediate results Install pip install taskchain From source git clone https://github.com/flowerchecker/taskchain cd taskchain python setup.py install Where to start? read this documentation check example project go through CheatSheet with the most common constructions. Main concepts task - one step in computation (data transformation) represented by python class. Every task can define two type of inputs: input tasks - other task on which the task depends and take their outputs (data) parameter - additional values which influence computation pipeline - group of tasks which are closely connected and together represent more complex computation, e.g. project can be split to pipeline for data preparation, pipeline for feature extraction and pipeline for model training and evaluation. Pipelines are only virtual concept and they not have a strict representation in the framework. chain - instance of pipeline or multiple pipelines, i.e. tasks connected by their dependencies into DAG ( directed acyclic graph ) with all required parameter values config - description (usually YAML file) with information needed to instantiate a chain. i.e.: description of tasks which should be part of a chain (e.g. pipeline) parameter values needed by these tasks eventual dependencies on other configs Typical project structure project_name \u251c\u2500\u2500 configs pipeline configuration files \u2502 \u251c\u2500\u2500 pipeline_1 usualy organized to dirs, one per pipeline \u2502 \u251c\u2500\u2500 pipeline_2 \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 data important data, which should be kept in repo, e.g. annotations \u251c\u2500\u2500 scripts runscripts, jupyter notebooks, etc. \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 project_name \u2502 \u251c\u2500\u2500 tasks definistions of tasks \u2502 \u2502 \u251c\u2500\u2500 pipeline_1.py \u2502 \u2502 \u251c\u2500\u2500 pipeline_2.py \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 utils other code \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 config.py global project configuration, \u251c\u2500\u2500 README.md e.g. path to big data or presistence dir \u2514\u2500\u2500 setup.py","title":"Introduction"},{"location":"#taskchain","text":"","title":"TaskChain"},{"location":"#what-is-a-taskchain","text":"TaskChain is a tool for managing data processing pipelines. It was created to reduce chaos in machine learning projects . Goals and features: separate computation logic and configuration every result should be reproducible brake down computation to individual steps in DAG structure brake down whole project to smaller pipelines which can be easily configured and reused never compute same thing twice - result of computation steps is saved automatically (data persistence) easy access to all intermediate results","title":"What is a TaskChain?"},{"location":"#install","text":"pip install taskchain","title":"Install"},{"location":"#from-source","text":"git clone https://github.com/flowerchecker/taskchain cd taskchain python setup.py install","title":"From source"},{"location":"#where-to-start","text":"read this documentation check example project go through CheatSheet with the most common constructions.","title":"Where to start?"},{"location":"#main-concepts","text":"task - one step in computation (data transformation) represented by python class. Every task can define two type of inputs: input tasks - other task on which the task depends and take their outputs (data) parameter - additional values which influence computation pipeline - group of tasks which are closely connected and together represent more complex computation, e.g. project can be split to pipeline for data preparation, pipeline for feature extraction and pipeline for model training and evaluation. Pipelines are only virtual concept and they not have a strict representation in the framework. chain - instance of pipeline or multiple pipelines, i.e. tasks connected by their dependencies into DAG ( directed acyclic graph ) with all required parameter values config - description (usually YAML file) with information needed to instantiate a chain. i.e.: description of tasks which should be part of a chain (e.g. pipeline) parameter values needed by these tasks eventual dependencies on other configs","title":"Main concepts"},{"location":"#typical-project-structure","text":"project_name \u251c\u2500\u2500 configs pipeline configuration files \u2502 \u251c\u2500\u2500 pipeline_1 usualy organized to dirs, one per pipeline \u2502 \u251c\u2500\u2500 pipeline_2 \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 data important data, which should be kept in repo, e.g. annotations \u251c\u2500\u2500 scripts runscripts, jupyter notebooks, etc. \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 project_name \u2502 \u251c\u2500\u2500 tasks definistions of tasks \u2502 \u2502 \u251c\u2500\u2500 pipeline_1.py \u2502 \u2502 \u251c\u2500\u2500 pipeline_2.py \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 utils other code \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 config.py global project configuration, \u251c\u2500\u2500 README.md e.g. path to big data or presistence dir \u2514\u2500\u2500 setup.py","title":"Typical project structure"},{"location":"chains/","text":"Chains Chain si where tasks are instantiated, configured, connected together and allow to compute and access data. Key feature of chain is that data of tasks are saved, i.e. each task is computed only once. Moreover, location of saved data is based only on inputs to computation (input tasks and parameters) and therefore data are shared also between chains. In this way, the exactly same computation is never executed twice. Chain creation Chain is always created from a config which contains all necessary information: list of tasks parameter values dependencies on other configs dir, where data are saved to Instantiation of a chain is simple as calling config.chain() or Chain(config) . Whole code to create chain object typically looks like this Chain creation chain.py from taskchain import Config from project import project_config chain = Config ( project_config . TASKS_DIR , project_config . CONFIGS_DIR / 'config.yaml' , global_vars = project_config ) . chain () project_config.py from pathlib import Path REPO_DIR = Path ( __file__ ) . resolve () . parent . parent . parent DATA_DIR = Path ( '/path/to/project_data/source_data' ) TASKS_DIR = Path ( '/path/to/project_data/task_data' ) CONFIGS_DIR = REPO_DIR / 'configs' Accessing tasks Tasks are accessible from a chain by their name ether as attribute chain.task_name or as dict key chain['task_name'] . If there are more tasks of the same name, you can use its full name with group or namespace or both chain['namespace::group:task_name'] . List of tasks chain.tasks contains dict of all tasks. Keys are full names and value are task objects. For better overview of all tasks in chain there is chain.tasks_df which contains pandas DataFrame with basic information about each task: Index is full name of the task name group namespace computed - True when task is computed and output data are saved data_path - path to data, if data are not computed yet, location is empty parameters - list of names of input parameters input_tasks - list of full names of input tasks config - file name of config which declared this task Draw tasks and dependencies For more visual overview there is chain . draw () Nodes are tasks, edges dependencies. color is based on tasks' group border none - is not persisting data ( InMemoryData ) dashed - data not computed solid - data computed Tip You can pass name of group or list of groups as argument to show only selected tasks. Note, that also neighbours are shown to give context for selected group. Note There are additional python dependencies: graphviz and seaborn . pip install graphviz seaborn Run tasks and data access Task object have attribute value . So, when task.value is requested one of three thinks happens data are not available yet - run method of the task is called and data are saved data are already computed and saved on disk - data are loaded to memory data are already in memory - just return the value Accessing data requested_data = chain . tasks_name . value requested_data = chain [ 'group:task_name' ] . value Note Calling run method of a task trigger .value on its input tasks (as long as, you use input_tasks as argument of run method or call .value manually in run method). This process is rucrsive, thus single call of .value can trigger loading or computation of many tasks. You can also access Data object which handle saving and loading data to and from disk by task.data Recomputing data Sometimes some persisted data become invalid, e.g. due to change in tasks' logic. Both Chain and Task have force method which forces tasks to recompute their data instead of loading from disk. If you need recompute only one task, you can simply call (note that force return task itself, so you can use it in chain with other commands): chain . my_tasks . force () . value If data of a task turn out to be faulty, we usually want force recomputation also all dependant tasks in chain. chain . force ( 'my_task' ) Method takes name of a task or list of names of tasks which should be forced. These and all dependant tasks are marked as forced and will be recomputed when their value is requested. Note, that this \"forced\" status is lost when chain is recreated. If you want to be sure, that all data are recomputed, use recompute=True argument which recompute all forced tasks or delete_data=True which delete data of all forced tasks right away. Warning Be careful with forcing. If you not handle recomputation correctly, your chain can end in incosistent state. Using delete_data=True solves this problem but can lead to deletion of \"expensive\" data. Saved files TaskChain is design to handle saving and loading data itself and to minimize accessing files directly. Here is general location of data computed by a task /path/to/tasks_data/group_name/task_name/{hash}.{extension} /path/to/tasks_data is path given in creation of config. group_name/ - data are organized according to tasks groups (usually this is same as pipelines). If task is in multiple level group group:subgroup path is group/subgroup/ . task_name/ ale data generated by this task are in this directory. {hash}.{extension} is final file (or in some cases directory, e.g. DirData ). extension is determined by tasks return type, see more here hash is computed based on hashes of all input tasks and all parameter values. I.e. it is unique and determined fully by all inputs going to computation of these data. Log files Along the file with computed value, there is {hash}.run_info.yaml with run info and {hash}.log with task's log output. See more here . Human readable files Persisted data are nicely structured in directories based on tasks' names and groups, but names of files are unreadable hashes. Data are mostly access through TaskChain, but sometimes it is useful accessed data directly, e.g. copy out final output. To simplify direct access, chain offers method create_readable_filenames which creates human-readable symlinks for all tasks in chain. e.g.: /path/to/tasks/group_name/task_name/nice_name.pd -> /path/to/tasks/group_name/task_name/a4c5d45a6....pd nice_name is name of task's config by default. See more details . Advanced topics MultiChains MultiChain is only envelope over multiple chains. Advantage over list or dict of Chain objects is that MultiChain create chains in such way, that they share identical tasks (with same inputs). TaskChain guarantee that identical tasks share data on disk but in this way there is only one instance of the task, i.e. data are shared also in memory. You can create multi-chain by passing list of configs. Individual chains can be accessed by config name (usually filename without extension): from taskchain import Config , MultiChain , Chain config1 = Config ( '/path/to/tasks_data' , '/path/to/configs/config1.yaml' ) config2 = Config ( '/path/to/tasks_data' , '/path/to/configs/config2.yaml' ) chains = MultiChain ([ config1 , config2 , ... ]) chain1 = chains . config1 chain2 = chains [ 'config2' ] chain_dict : dict [ str , Chain ] = chains . chains You can also create multi-chain from all configs in a dir from taskchain import MultiChain chains = MultiChain . from_dir ( '/path/to/tasks_data' , '/path/to/configs' , # other kwargs passed to configs such as `global_vars` ) Note You can use multi-chain also with contexts , e.g. one config with different contexts. However, you have to handle names of configs manually (add extra argument name to your configs) to avoid name colision in multi-chain. Disabling parameter mode As described above, location of saved files is based only parameters and input tasks (which are also determined by parameters). This is called parameter mode. You can disable this mode by config.chain(parameter_mode=False) this replaces hashes in filenames by config names. This approach is little more comprehensible, but has some disadvantages: some computations can be done more time, because same task with same parameters can be defined in many configs you must be careful with names of your configs, e.g. you cannot rename your configs using contexts can and probably would break persistence Migration to parameter mode You can migrate to parameter mode later using taskchain.utils.task.migration.migrate_to_parameter_mode .","title":"Chains"},{"location":"chains/#chains","text":"Chain si where tasks are instantiated, configured, connected together and allow to compute and access data. Key feature of chain is that data of tasks are saved, i.e. each task is computed only once. Moreover, location of saved data is based only on inputs to computation (input tasks and parameters) and therefore data are shared also between chains. In this way, the exactly same computation is never executed twice.","title":"Chains"},{"location":"chains/#chain-creation","text":"Chain is always created from a config which contains all necessary information: list of tasks parameter values dependencies on other configs dir, where data are saved to Instantiation of a chain is simple as calling config.chain() or Chain(config) . Whole code to create chain object typically looks like this Chain creation chain.py from taskchain import Config from project import project_config chain = Config ( project_config . TASKS_DIR , project_config . CONFIGS_DIR / 'config.yaml' , global_vars = project_config ) . chain () project_config.py from pathlib import Path REPO_DIR = Path ( __file__ ) . resolve () . parent . parent . parent DATA_DIR = Path ( '/path/to/project_data/source_data' ) TASKS_DIR = Path ( '/path/to/project_data/task_data' ) CONFIGS_DIR = REPO_DIR / 'configs'","title":"Chain creation"},{"location":"chains/#accessing-tasks","text":"Tasks are accessible from a chain by their name ether as attribute chain.task_name or as dict key chain['task_name'] . If there are more tasks of the same name, you can use its full name with group or namespace or both chain['namespace::group:task_name'] .","title":"Accessing tasks"},{"location":"chains/#list-of-tasks","text":"chain.tasks contains dict of all tasks. Keys are full names and value are task objects. For better overview of all tasks in chain there is chain.tasks_df which contains pandas DataFrame with basic information about each task: Index is full name of the task name group namespace computed - True when task is computed and output data are saved data_path - path to data, if data are not computed yet, location is empty parameters - list of names of input parameters input_tasks - list of full names of input tasks config - file name of config which declared this task","title":"List of tasks"},{"location":"chains/#draw-tasks-and-dependencies","text":"For more visual overview there is chain . draw () Nodes are tasks, edges dependencies. color is based on tasks' group border none - is not persisting data ( InMemoryData ) dashed - data not computed solid - data computed Tip You can pass name of group or list of groups as argument to show only selected tasks. Note, that also neighbours are shown to give context for selected group. Note There are additional python dependencies: graphviz and seaborn . pip install graphviz seaborn","title":"Draw tasks and dependencies"},{"location":"chains/#run-tasks-and-data-access","text":"Task object have attribute value . So, when task.value is requested one of three thinks happens data are not available yet - run method of the task is called and data are saved data are already computed and saved on disk - data are loaded to memory data are already in memory - just return the value Accessing data requested_data = chain . tasks_name . value requested_data = chain [ 'group:task_name' ] . value Note Calling run method of a task trigger .value on its input tasks (as long as, you use input_tasks as argument of run method or call .value manually in run method). This process is rucrsive, thus single call of .value can trigger loading or computation of many tasks. You can also access Data object which handle saving and loading data to and from disk by task.data","title":"Run tasks and data access"},{"location":"chains/#recomputing-data","text":"Sometimes some persisted data become invalid, e.g. due to change in tasks' logic. Both Chain and Task have force method which forces tasks to recompute their data instead of loading from disk. If you need recompute only one task, you can simply call (note that force return task itself, so you can use it in chain with other commands): chain . my_tasks . force () . value If data of a task turn out to be faulty, we usually want force recomputation also all dependant tasks in chain. chain . force ( 'my_task' ) Method takes name of a task or list of names of tasks which should be forced. These and all dependant tasks are marked as forced and will be recomputed when their value is requested. Note, that this \"forced\" status is lost when chain is recreated. If you want to be sure, that all data are recomputed, use recompute=True argument which recompute all forced tasks or delete_data=True which delete data of all forced tasks right away. Warning Be careful with forcing. If you not handle recomputation correctly, your chain can end in incosistent state. Using delete_data=True solves this problem but can lead to deletion of \"expensive\" data.","title":"Recomputing data"},{"location":"chains/#saved-files","text":"TaskChain is design to handle saving and loading data itself and to minimize accessing files directly. Here is general location of data computed by a task /path/to/tasks_data/group_name/task_name/{hash}.{extension} /path/to/tasks_data is path given in creation of config. group_name/ - data are organized according to tasks groups (usually this is same as pipelines). If task is in multiple level group group:subgroup path is group/subgroup/ . task_name/ ale data generated by this task are in this directory. {hash}.{extension} is final file (or in some cases directory, e.g. DirData ). extension is determined by tasks return type, see more here hash is computed based on hashes of all input tasks and all parameter values. I.e. it is unique and determined fully by all inputs going to computation of these data.","title":"Saved files"},{"location":"chains/#log-files","text":"Along the file with computed value, there is {hash}.run_info.yaml with run info and {hash}.log with task's log output. See more here .","title":"Log files"},{"location":"chains/#human-readable-files","text":"Persisted data are nicely structured in directories based on tasks' names and groups, but names of files are unreadable hashes. Data are mostly access through TaskChain, but sometimes it is useful accessed data directly, e.g. copy out final output. To simplify direct access, chain offers method create_readable_filenames which creates human-readable symlinks for all tasks in chain. e.g.: /path/to/tasks/group_name/task_name/nice_name.pd -> /path/to/tasks/group_name/task_name/a4c5d45a6....pd nice_name is name of task's config by default. See more details .","title":"Human readable files"},{"location":"chains/#advanced-topics","text":"","title":"Advanced topics"},{"location":"chains/#multichains","text":"MultiChain is only envelope over multiple chains. Advantage over list or dict of Chain objects is that MultiChain create chains in such way, that they share identical tasks (with same inputs). TaskChain guarantee that identical tasks share data on disk but in this way there is only one instance of the task, i.e. data are shared also in memory. You can create multi-chain by passing list of configs. Individual chains can be accessed by config name (usually filename without extension): from taskchain import Config , MultiChain , Chain config1 = Config ( '/path/to/tasks_data' , '/path/to/configs/config1.yaml' ) config2 = Config ( '/path/to/tasks_data' , '/path/to/configs/config2.yaml' ) chains = MultiChain ([ config1 , config2 , ... ]) chain1 = chains . config1 chain2 = chains [ 'config2' ] chain_dict : dict [ str , Chain ] = chains . chains You can also create multi-chain from all configs in a dir from taskchain import MultiChain chains = MultiChain . from_dir ( '/path/to/tasks_data' , '/path/to/configs' , # other kwargs passed to configs such as `global_vars` ) Note You can use multi-chain also with contexts , e.g. one config with different contexts. However, you have to handle names of configs manually (add extra argument name to your configs) to avoid name colision in multi-chain.","title":"MultiChains"},{"location":"chains/#disabling-parameter-mode","text":"As described above, location of saved files is based only parameters and input tasks (which are also determined by parameters). This is called parameter mode. You can disable this mode by config.chain(parameter_mode=False) this replaces hashes in filenames by config names. This approach is little more comprehensible, but has some disadvantages: some computations can be done more time, because same task with same parameters can be defined in many configs you must be careful with names of your configs, e.g. you cannot rename your configs using contexts can and probably would break persistence","title":"Disabling parameter mode"},{"location":"chains/#migration-to-parameter-mode","text":"You can migrate to parameter mode later using taskchain.utils.task.migration.migrate_to_parameter_mode .","title":"Migration to parameter mode"},{"location":"cheatsheet/","text":"CheatSheet Basics Task class ExampleTask ( Task ): class Meta : input_tasks = [ ... ] parameters = [ Parameter ( 'name' , default = None ), ] def run ( self , ... ) -> ... : ... Config tasks : ....* uses : \"{CONFIGS_DIR}/....yaml as ...\" parameter : value ... parameter_object : class : path.to.class args : - arg1 - arg2 kwargs : kwarg1 : value1 kwarg2 : value2 ... Chain from taskchain import Config from project import config chain = Config ( config . TASKS_DIR , config . CONFIGS_DIR / 'config_name.yaml' , global_vars = config , ) . chain () chain . set_log_level ( 'DEBUG' ) chain . tasks_df chain . draw () chain . force ( 'my_task' , delete_data = True ) chain . my_task . value chain . my_task . force () . value chain . my_task . has_data chain . my_task . data_path chain . my_task . run_info chain . my_task . log Other DirData def run ( self ) -> DirData : data_object = self . get_data_object () working_dir = data_object . dir ... return data_object","title":"CheatSheet"},{"location":"cheatsheet/#cheatsheet","text":"","title":"CheatSheet"},{"location":"cheatsheet/#basics","text":"","title":"Basics"},{"location":"cheatsheet/#task","text":"class ExampleTask ( Task ): class Meta : input_tasks = [ ... ] parameters = [ Parameter ( 'name' , default = None ), ] def run ( self , ... ) -> ... : ...","title":"Task"},{"location":"cheatsheet/#config","text":"tasks : ....* uses : \"{CONFIGS_DIR}/....yaml as ...\" parameter : value ... parameter_object : class : path.to.class args : - arg1 - arg2 kwargs : kwarg1 : value1 kwarg2 : value2 ...","title":"Config"},{"location":"cheatsheet/#chain","text":"from taskchain import Config from project import config chain = Config ( config . TASKS_DIR , config . CONFIGS_DIR / 'config_name.yaml' , global_vars = config , ) . chain () chain . set_log_level ( 'DEBUG' ) chain . tasks_df chain . draw () chain . force ( 'my_task' , delete_data = True ) chain . my_task . value chain . my_task . force () . value chain . my_task . has_data chain . my_task . data_path chain . my_task . run_info chain . my_task . log","title":"Chain"},{"location":"cheatsheet/#other","text":"","title":"Other"},{"location":"cheatsheet/#dirdata","text":"def run ( self ) -> DirData : data_object = self . get_data_object () working_dir = data_object . dir ... return data_object","title":"DirData"},{"location":"configs/","text":"Configs Generally, configs defines all necessary parameter values, which are needed to run a pipeline and tasks in it. In TaskChain, config is also entry point for creating a chain and describes how pipelines are connected to a chain. Therefore, a config has to define: description of tasks which should be part of a chain (e.g. pipeline) parameter values needed by these tasks dependencies on other configs (pipelines) Usual setup is one config in one file which defines one pipeline . This allows effective reuse of the pipeline in multiple chains without need of repeating parameter values. What actually is config? Config in TaskChain has dual meaning. First, config as YAML file containing information described above. Second, config as an instance of taskchain.Config which is usually based on the config file and adds other information necessary for computation. Config definition Simple example of config config.yaml tasks : my_project.tasks.pipeline.* string_parameter : value int_parameter : 123 complex_parameter : key1 : value1 key2 : - v1 - v2 code.py from taskchain import Config config = Config ( '/path/to/task_data' , '/path/to/config.yaml' , ) Config is basically map from strings to arbitrary values . In addition to YAML files, you can define your config also in JSON file or directly in code by passing dict like object to Config() in parameter data . More examples of configs can be found in example project . You can access values of config object by attribute config.string_parameter or in dict like fashion config['string_parameter'] . Actually, config is descendant of dict . However, direct access to the values is rarely needed because parameters are handled by TaskChain. Tasks Each config should define which tasks are configured. This is a way how a config defines a pipeline. The special parameter tasks is string or list of strings describing tasks. Task is described by a path to task's class: 'my_project.tasks.pipeline.ProcessTask' . This description corresponds exactly to python imports. To import all tasks from pipeline (defined in single file) at once you can use wildcard * in last place of description: 'my_project.tasks.pipeline.*' . Task exclusion For more flexibility, you can also exclude tasks with special parameter excluded_tasks with same syntax as tasks parameter. Config dependencies More complicated chains are split to multiple pipelines with corresponding configs. Parameter uses defines how configs of these pipelines are connected together. For example, project is split to data preparation pipeline and model pipeline . Config of data preparation have no prerequisites, and thus it doesn't need uses . Some tasks of the model pipeline depends on tasks of data pipeline and therefore model config has to depend on data config. Example model_config.yaml tasks : my_project.tasks.model.* uses : \"/path/to/data_config.yaml\" model : ... data_config.yaml tasks : my_project.tasks.data.* source_file : ... uses is string or list of strings if there are multiple dependency configs. Placeholders & Global vars Placeholders and global variables is a mechanism which allows TaskChain projects work in multiple environments. The same project can be moved to different directory or machine or can be run by multiple peoples with different setups. This is especially useful for handle paths in configs. To make configs independent on environment it is possible to use placeholders in them which are later replaced by values provided in Config object in instantiation in parameter global_vars . Basic usage of global vars config.yaml tasks : my_project.tasks.pipeline.* uses : \"{CONFIGS_DIR}/dependency_config.yaml\" source_data : { DATA_DIR } /data.csv code.py from pathlib import Path from taskchain import Config CONFIGS_DIR = Path ( '/path/to/configs' ) config = Config ( '/path/to/task_data' , CONFIGS_DIR / 'config.yaml' , global_vars = { 'CONFIGS_DIR' : CONFIGS_DIR , 'DATA_DIR' : '/path/to/data' } ) Parameter global_var can be a dict with placeholders as keys or an object with placeholders as attributes. This allows following typical construction: Typical usage of global vars code.py from pathlib import Path from taskchain import Config from project import project_config config = Config ( project_config . TASKS_DIR , project_config . CONFIGS_DIR / 'config.yaml' , global_vars = project_config ) project_config.py from pathlib import Path REPO_DIR = Path ( __file__ ) . resolve () . parent . parent . parent DATA_DIR = Path ( '/path/to/project_data/source_data' ) TASKS_DIR = Path ( '/path/to/project_data/task_data' ) CONFIGS_DIR = REPO_DIR / 'configs' config.yaml tasks : my_project.tasks.pipeline.* uses : \"{CONFIGS_DIR}/dependency_config.yaml\" source_data : { DATA_DIR } /data.csv Parameter objects Sometimes configuration using only json-like parameter values is not enough, or it is not practical. For these cases, you can include definition of a object instance as parameter value to your config. Object instance is defined by class and args and kwargs passed to constructor. Class has to be derived class of taskchain.paramater.ParameterObject , i.e. has to define repr method which should return unique string representation of object. This value is then used by taskchain to keep track of changes in configs, and it is necessary for correct function of data persistence. Common pattern is that there is base class which defines interface which is used by tasks and parameter objects are instances of child classes. Good example is Model class which define abstract methods train , predict , save and load . Children of this base class (e.g. NeuralModel , LinearRegressionModel , ...) implement these methods, are configurable by their constructor and are used in configs. Here is example of this pattern. Definition of object instance in config is dict containing key class with fully-qualified name of class as value. Additionally, dict can contain args with a list and kwargs with a dict. Definition of object in config model : class : my_project . models . LinearRegressionModel kwargs : normalize : True regularization : 1 In last example, config provide parameter model with value LinearRegressionModel(normalize=True, regularization=1) . In config, objects can be defined inside other structures, such as list, dict or definitions of other objects. I.e. you can define parameter which is list of objects. AutoParameterObject Writing of repr method for parameter object can repetitive and omitting a argument can lead to mistakes. Therefore, there is AutoParameterObject which defines repr for you and it is based on class name and arguments of __init__ method. To make it work, all arguments values of constructor has to be saved in object attributes. For argument named my_argument , AutoParameterObject is looking for its value at self._my_argument or self.my_argument . To allow more flexibility and ease adding new arguments, you can also define ignore_persistence_args or dont_persist_default_value_args which return list of string names of arguments and have similar meaning as Parameter arguments . ChainObject In case that your parameter object need to access the chain directly (e.g. take a task's data), you can inherit also from taskchain.chain.ChainObject and implement init_chain(self, chain) method which is called after chain creation and pass chain itself. Namespaces If you need to use one pipeline with different configs in one chain, or you just make your larger chains more structured, you can use namespaces. You can put part of your chain to a namespace and all tasks in that part will be referenced not but their name task_name but by namespace and task name namespace_name::task_name . Creating namespaces is really simple, in referencing other config in config definition ( uses clause) just add as namespace_name . Example model_config.yaml tasks : my_project.tasks.model.* uses : - \"/path/to/data_configs/train_data.yaml as train\" - \"/path/to/data_configs/valid_data.yaml as valid\" - \"/path/to/data_configs/test_data.yaml as test\" model : ... model.py ... class TrainModel ( Task ): class Meta : parameters = [ Parameter ( 'model' )] input_tasks = [ 'train::features' , 'valid::features' ] def run ( self , model ) -> ... : train_data = self . input_tasks [ 'train::features' ] . value ... class EvalModel ( Task ): class Meta : parameters = [ Parameter ( 'model' )] input_tasks = [ TrainModel , 'test::features] def run ( self , model , train_model , features ) -> dict : ... ... Notes if a config is in a namespace, also configs used by this config are in the same namespace namespaces can be nested, e.g. task features can be in nested namespace main_model::train you can still reference task without namespace as long as there is only one task of that name this is the case of input_tasks of Evalmodel task but not input_tasks of TrainModel task in example above this applies to referencing tasks in chain, in input_tasks and run method arguments. Advanced topics Multi-config files It is possible to save multiple configs to one file. This can be useful, when chain has multiple pipelines, and you need one file configuration. Example of multi-config file configs : data_config : tasks : ... input_data_file : ... ... model_config : main_part : True tasks : ... uses : #data_config as data model : ... ... other_config_name : ... Single config can be taken from this file using part argument: config = Config ( '/path/to/task_data' , 'multi_config.yaml' , part = 'data_config' ) It is possible omit part argument if one of defined configs specify main_part: True . To access a config from another config (in uses ) use /path/to/multi_config.yaml#data_config or if you refer to a part of the same multi-config, you can use only #data_donfig as shown in example. Contexts Context is mechanism which allows rewrite parameter values in configs in the time of their instantiation. Under the hood Context is special case of Config which is used in specific way. Example config = Config ( '/path/to/task_data' , CONFIGS_DIR / 'config.yaml' , context = { 'verbose' : True , 'batch_size' : 32 } ) This can be useful for ad-hock experiment hyper-parameter optimization tuning parameters, which are not used in persistence, e.g. batch_size consider long data processing chain consisting of multiple dependent pipelines each with own config file. When we get new input data, it usually leads to recreating all configs which are very similar (only input_data parameter is changed and config paths in uses ). Other approach is omit input_data parameter value in config and provide it context, which allows run pipeline with same configuration on multiple inputs easily. What can be context dict of parameters and their values file path with json on yaml file - this is analogous to context files Context object list of previous - in that case context are merged together. In case of parameter conflict later has higher priority. Warning Parameters in context are applied globaly, i.e. on all configs in chain. Be cearful with parameters of the same name in different pipelines. Namespaces In the case of more complicated chains which uses namespaces you can run to problems, when one pipeline is in chain multiple times with different configuration (under different namespaces). For these cases, context can have for_namespaces field. It's valued should be dict with namespaces as keys and parameters to overwrite in this namespace as value. Context YAML file using for_namespace for_namespaces : train : input_data : '/path/to/data1' other_param : 42 test : input_data : '/path/to/data2' batch_size : 32 Uses It is possible to join multiple context in one file with uses field. Syntax is the same as in configs, but meaning slight different. In contexts files in uses are just merged to the main context. If ... as namespace format is used, loaded context is applied only for given namespace. Following example is equivalent to the previous one. Context YAML files using uses context.yaml uses : - /path/to/train.context.yaml as train - /path/to/test.context.yaml as test batch_size : 32 train.context.yaml input_data : '/path/to/data1' other_param : 42 test.context.yaml input_data : '/path/to/data2'","title":"Configs"},{"location":"configs/#configs","text":"Generally, configs defines all necessary parameter values, which are needed to run a pipeline and tasks in it. In TaskChain, config is also entry point for creating a chain and describes how pipelines are connected to a chain. Therefore, a config has to define: description of tasks which should be part of a chain (e.g. pipeline) parameter values needed by these tasks dependencies on other configs (pipelines) Usual setup is one config in one file which defines one pipeline . This allows effective reuse of the pipeline in multiple chains without need of repeating parameter values. What actually is config? Config in TaskChain has dual meaning. First, config as YAML file containing information described above. Second, config as an instance of taskchain.Config which is usually based on the config file and adds other information necessary for computation.","title":"Configs"},{"location":"configs/#config-definition","text":"Simple example of config config.yaml tasks : my_project.tasks.pipeline.* string_parameter : value int_parameter : 123 complex_parameter : key1 : value1 key2 : - v1 - v2 code.py from taskchain import Config config = Config ( '/path/to/task_data' , '/path/to/config.yaml' , ) Config is basically map from strings to arbitrary values . In addition to YAML files, you can define your config also in JSON file or directly in code by passing dict like object to Config() in parameter data . More examples of configs can be found in example project . You can access values of config object by attribute config.string_parameter or in dict like fashion config['string_parameter'] . Actually, config is descendant of dict . However, direct access to the values is rarely needed because parameters are handled by TaskChain.","title":"Config definition"},{"location":"configs/#tasks","text":"Each config should define which tasks are configured. This is a way how a config defines a pipeline. The special parameter tasks is string or list of strings describing tasks. Task is described by a path to task's class: 'my_project.tasks.pipeline.ProcessTask' . This description corresponds exactly to python imports. To import all tasks from pipeline (defined in single file) at once you can use wildcard * in last place of description: 'my_project.tasks.pipeline.*' .","title":"Tasks"},{"location":"configs/#task-exclusion","text":"For more flexibility, you can also exclude tasks with special parameter excluded_tasks with same syntax as tasks parameter.","title":"Task exclusion"},{"location":"configs/#config-dependencies","text":"More complicated chains are split to multiple pipelines with corresponding configs. Parameter uses defines how configs of these pipelines are connected together. For example, project is split to data preparation pipeline and model pipeline . Config of data preparation have no prerequisites, and thus it doesn't need uses . Some tasks of the model pipeline depends on tasks of data pipeline and therefore model config has to depend on data config. Example model_config.yaml tasks : my_project.tasks.model.* uses : \"/path/to/data_config.yaml\" model : ... data_config.yaml tasks : my_project.tasks.data.* source_file : ... uses is string or list of strings if there are multiple dependency configs.","title":"Config dependencies"},{"location":"configs/#placeholders-global-vars","text":"Placeholders and global variables is a mechanism which allows TaskChain projects work in multiple environments. The same project can be moved to different directory or machine or can be run by multiple peoples with different setups. This is especially useful for handle paths in configs. To make configs independent on environment it is possible to use placeholders in them which are later replaced by values provided in Config object in instantiation in parameter global_vars . Basic usage of global vars config.yaml tasks : my_project.tasks.pipeline.* uses : \"{CONFIGS_DIR}/dependency_config.yaml\" source_data : { DATA_DIR } /data.csv code.py from pathlib import Path from taskchain import Config CONFIGS_DIR = Path ( '/path/to/configs' ) config = Config ( '/path/to/task_data' , CONFIGS_DIR / 'config.yaml' , global_vars = { 'CONFIGS_DIR' : CONFIGS_DIR , 'DATA_DIR' : '/path/to/data' } ) Parameter global_var can be a dict with placeholders as keys or an object with placeholders as attributes. This allows following typical construction: Typical usage of global vars code.py from pathlib import Path from taskchain import Config from project import project_config config = Config ( project_config . TASKS_DIR , project_config . CONFIGS_DIR / 'config.yaml' , global_vars = project_config ) project_config.py from pathlib import Path REPO_DIR = Path ( __file__ ) . resolve () . parent . parent . parent DATA_DIR = Path ( '/path/to/project_data/source_data' ) TASKS_DIR = Path ( '/path/to/project_data/task_data' ) CONFIGS_DIR = REPO_DIR / 'configs' config.yaml tasks : my_project.tasks.pipeline.* uses : \"{CONFIGS_DIR}/dependency_config.yaml\" source_data : { DATA_DIR } /data.csv","title":"Placeholders &amp; Global vars"},{"location":"configs/#parameter-objects","text":"Sometimes configuration using only json-like parameter values is not enough, or it is not practical. For these cases, you can include definition of a object instance as parameter value to your config. Object instance is defined by class and args and kwargs passed to constructor. Class has to be derived class of taskchain.paramater.ParameterObject , i.e. has to define repr method which should return unique string representation of object. This value is then used by taskchain to keep track of changes in configs, and it is necessary for correct function of data persistence. Common pattern is that there is base class which defines interface which is used by tasks and parameter objects are instances of child classes. Good example is Model class which define abstract methods train , predict , save and load . Children of this base class (e.g. NeuralModel , LinearRegressionModel , ...) implement these methods, are configurable by their constructor and are used in configs. Here is example of this pattern. Definition of object instance in config is dict containing key class with fully-qualified name of class as value. Additionally, dict can contain args with a list and kwargs with a dict. Definition of object in config model : class : my_project . models . LinearRegressionModel kwargs : normalize : True regularization : 1 In last example, config provide parameter model with value LinearRegressionModel(normalize=True, regularization=1) . In config, objects can be defined inside other structures, such as list, dict or definitions of other objects. I.e. you can define parameter which is list of objects.","title":"Parameter objects"},{"location":"configs/#autoparameterobject","text":"Writing of repr method for parameter object can repetitive and omitting a argument can lead to mistakes. Therefore, there is AutoParameterObject which defines repr for you and it is based on class name and arguments of __init__ method. To make it work, all arguments values of constructor has to be saved in object attributes. For argument named my_argument , AutoParameterObject is looking for its value at self._my_argument or self.my_argument . To allow more flexibility and ease adding new arguments, you can also define ignore_persistence_args or dont_persist_default_value_args which return list of string names of arguments and have similar meaning as Parameter arguments .","title":"AutoParameterObject"},{"location":"configs/#chainobject","text":"In case that your parameter object need to access the chain directly (e.g. take a task's data), you can inherit also from taskchain.chain.ChainObject and implement init_chain(self, chain) method which is called after chain creation and pass chain itself.","title":"ChainObject"},{"location":"configs/#namespaces","text":"If you need to use one pipeline with different configs in one chain, or you just make your larger chains more structured, you can use namespaces. You can put part of your chain to a namespace and all tasks in that part will be referenced not but their name task_name but by namespace and task name namespace_name::task_name . Creating namespaces is really simple, in referencing other config in config definition ( uses clause) just add as namespace_name . Example model_config.yaml tasks : my_project.tasks.model.* uses : - \"/path/to/data_configs/train_data.yaml as train\" - \"/path/to/data_configs/valid_data.yaml as valid\" - \"/path/to/data_configs/test_data.yaml as test\" model : ... model.py ... class TrainModel ( Task ): class Meta : parameters = [ Parameter ( 'model' )] input_tasks = [ 'train::features' , 'valid::features' ] def run ( self , model ) -> ... : train_data = self . input_tasks [ 'train::features' ] . value ... class EvalModel ( Task ): class Meta : parameters = [ Parameter ( 'model' )] input_tasks = [ TrainModel , 'test::features] def run ( self , model , train_model , features ) -> dict : ... ... Notes if a config is in a namespace, also configs used by this config are in the same namespace namespaces can be nested, e.g. task features can be in nested namespace main_model::train you can still reference task without namespace as long as there is only one task of that name this is the case of input_tasks of Evalmodel task but not input_tasks of TrainModel task in example above this applies to referencing tasks in chain, in input_tasks and run method arguments.","title":"Namespaces"},{"location":"configs/#advanced-topics","text":"","title":"Advanced topics"},{"location":"configs/#multi-config-files","text":"It is possible to save multiple configs to one file. This can be useful, when chain has multiple pipelines, and you need one file configuration. Example of multi-config file configs : data_config : tasks : ... input_data_file : ... ... model_config : main_part : True tasks : ... uses : #data_config as data model : ... ... other_config_name : ... Single config can be taken from this file using part argument: config = Config ( '/path/to/task_data' , 'multi_config.yaml' , part = 'data_config' ) It is possible omit part argument if one of defined configs specify main_part: True . To access a config from another config (in uses ) use /path/to/multi_config.yaml#data_config or if you refer to a part of the same multi-config, you can use only #data_donfig as shown in example.","title":"Multi-config files"},{"location":"configs/#contexts","text":"Context is mechanism which allows rewrite parameter values in configs in the time of their instantiation. Under the hood Context is special case of Config which is used in specific way. Example config = Config ( '/path/to/task_data' , CONFIGS_DIR / 'config.yaml' , context = { 'verbose' : True , 'batch_size' : 32 } ) This can be useful for ad-hock experiment hyper-parameter optimization tuning parameters, which are not used in persistence, e.g. batch_size consider long data processing chain consisting of multiple dependent pipelines each with own config file. When we get new input data, it usually leads to recreating all configs which are very similar (only input_data parameter is changed and config paths in uses ). Other approach is omit input_data parameter value in config and provide it context, which allows run pipeline with same configuration on multiple inputs easily.","title":"Contexts"},{"location":"configs/#what-can-be-context","text":"dict of parameters and their values file path with json on yaml file - this is analogous to context files Context object list of previous - in that case context are merged together. In case of parameter conflict later has higher priority. Warning Parameters in context are applied globaly, i.e. on all configs in chain. Be cearful with parameters of the same name in different pipelines.","title":"What can be context"},{"location":"configs/#namespaces_1","text":"In the case of more complicated chains which uses namespaces you can run to problems, when one pipeline is in chain multiple times with different configuration (under different namespaces). For these cases, context can have for_namespaces field. It's valued should be dict with namespaces as keys and parameters to overwrite in this namespace as value. Context YAML file using for_namespace for_namespaces : train : input_data : '/path/to/data1' other_param : 42 test : input_data : '/path/to/data2' batch_size : 32","title":"Namespaces"},{"location":"configs/#uses","text":"It is possible to join multiple context in one file with uses field. Syntax is the same as in configs, but meaning slight different. In contexts files in uses are just merged to the main context. If ... as namespace format is used, loaded context is applied only for given namespace. Following example is equivalent to the previous one. Context YAML files using uses context.yaml uses : - /path/to/train.context.yaml as train - /path/to/test.context.yaml as test batch_size : 32 train.context.yaml input_data : '/path/to/data1' other_param : 42 test.context.yaml input_data : '/path/to/data2'","title":"Uses"},{"location":"example/","text":"Example project - movie ratings Example project is small demonstration of TaskChain capabilities and try to show its main features and constructions. This project allows quick hands-on experience and can serve as template for new projects. You can start by running this notebook . Keep in mind, that goal of the project is showcase of various features, so chosen solutions for given problems are not always optimal. Install pip install taskchain git clone https://github.com/flowerchecker/taskchain/ cd taskchain/example python setup.py develop Description Project works with IMDB movie dataset downloaded from Kaggle . Goals of projects is to explore dataset and train a model which is able to predict rating of a new movie. Project is to split to 3 pipelines Movies tasks , configs , notebook This pipeline has the following functions load movies data filter them get basic statistics - year and duration histograms extract directors, movies, genres and countries of movies Features tasks , configs , notebook This pipeline build on movie pipeline and has the following functions select the most relevant actors and directors (to use them as features) prepare all features - year, duration, and features based on movie's genres, countries, actors and directors (binary features) select requested feature types Rating model tasks , configs , notebook This pipeline build on features pipeline and has the following functions create training and eval data from features train a mode - models are defined here evaluate the model Project files example \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 features \u2502 \u2502 \u251c\u2500\u2500 all.yaml \u2502 \u2502 \u2514\u2500\u2500 basic.yaml \u2502 \u251c\u2500\u2500 movies \u2502 \u2502 \u251c\u2500\u2500 imdb.all.yaml \u2502 \u2502 \u2514\u2500\u2500 imdb.filtered.yaml \u2502 \u2514\u2500\u2500 rating_model \u2502 \u251c\u2500\u2500 all_features \u2502 \u2502 \u251c\u2500\u2500 baseline.yaml \u2502 \u2502 \u251c\u2500\u2500 linear_regression.yaml \u2502 \u2502 \u251c\u2500\u2500 nn.yaml \u2502 \u2502 \u2514\u2500\u2500 tf_linear_regression.yaml \u2502 \u2514\u2500\u2500 basic_features \u2502 \u251c\u2500\u2500 baseline.yaml \u2502 \u2514\u2500\u2500 linear_regression.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 source_data \u2502 \u2502 \u251c\u2500\u2500 IMDB_movies.csv \u2502 \u2502 \u2514\u2500\u2500 ratings.Thran.csv \u2502 \u2514\u2500\u2500 task_data # here will be computed data \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 features.ipynb \u2502 \u251c\u2500\u2500 introduction.ipynb \u2502 \u251c\u2500\u2500 movies.ipynb \u2502 \u251c\u2500\u2500 personal_rating_model.ipynb \u2502 \u251c\u2500\u2500 rating_model.ipynb \u2502 \u2514\u2500\u2500 tasks_run.py \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u2514\u2500\u2500 movie_ratings \u251c\u2500\u2500 config.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 core.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 sklearn.py \u2502 \u2514\u2500\u2500 tensorflow.py \u2514\u2500\u2500 tasks \u251c\u2500\u2500 features.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 movies.py \u2514\u2500\u2500 rating_model.py","title":"Example project"},{"location":"example/#example-project-movie-ratings","text":"Example project is small demonstration of TaskChain capabilities and try to show its main features and constructions. This project allows quick hands-on experience and can serve as template for new projects. You can start by running this notebook . Keep in mind, that goal of the project is showcase of various features, so chosen solutions for given problems are not always optimal.","title":"Example project - movie ratings"},{"location":"example/#install","text":"pip install taskchain git clone https://github.com/flowerchecker/taskchain/ cd taskchain/example python setup.py develop","title":"Install"},{"location":"example/#description","text":"Project works with IMDB movie dataset downloaded from Kaggle . Goals of projects is to explore dataset and train a model which is able to predict rating of a new movie. Project is to split to 3 pipelines","title":"Description"},{"location":"example/#movies","text":"tasks , configs , notebook This pipeline has the following functions load movies data filter them get basic statistics - year and duration histograms extract directors, movies, genres and countries of movies","title":"Movies"},{"location":"example/#features","text":"tasks , configs , notebook This pipeline build on movie pipeline and has the following functions select the most relevant actors and directors (to use them as features) prepare all features - year, duration, and features based on movie's genres, countries, actors and directors (binary features) select requested feature types","title":"Features"},{"location":"example/#rating-model","text":"tasks , configs , notebook This pipeline build on features pipeline and has the following functions create training and eval data from features train a mode - models are defined here evaluate the model","title":"Rating model"},{"location":"example/#project-files","text":"example \u251c\u2500\u2500 configs \u2502 \u251c\u2500\u2500 features \u2502 \u2502 \u251c\u2500\u2500 all.yaml \u2502 \u2502 \u2514\u2500\u2500 basic.yaml \u2502 \u251c\u2500\u2500 movies \u2502 \u2502 \u251c\u2500\u2500 imdb.all.yaml \u2502 \u2502 \u2514\u2500\u2500 imdb.filtered.yaml \u2502 \u2514\u2500\u2500 rating_model \u2502 \u251c\u2500\u2500 all_features \u2502 \u2502 \u251c\u2500\u2500 baseline.yaml \u2502 \u2502 \u251c\u2500\u2500 linear_regression.yaml \u2502 \u2502 \u251c\u2500\u2500 nn.yaml \u2502 \u2502 \u2514\u2500\u2500 tf_linear_regression.yaml \u2502 \u2514\u2500\u2500 basic_features \u2502 \u251c\u2500\u2500 baseline.yaml \u2502 \u2514\u2500\u2500 linear_regression.yaml \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 source_data \u2502 \u2502 \u251c\u2500\u2500 IMDB_movies.csv \u2502 \u2502 \u2514\u2500\u2500 ratings.Thran.csv \u2502 \u2514\u2500\u2500 task_data # here will be computed data \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 features.ipynb \u2502 \u251c\u2500\u2500 introduction.ipynb \u2502 \u251c\u2500\u2500 movies.ipynb \u2502 \u251c\u2500\u2500 personal_rating_model.ipynb \u2502 \u251c\u2500\u2500 rating_model.ipynb \u2502 \u2514\u2500\u2500 tasks_run.py \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 src \u2514\u2500\u2500 movie_ratings \u251c\u2500\u2500 config.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 core.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 sklearn.py \u2502 \u2514\u2500\u2500 tensorflow.py \u2514\u2500\u2500 tasks \u251c\u2500\u2500 features.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 movies.py \u2514\u2500\u2500 rating_model.py","title":"Project files"},{"location":"tasks/","text":"Tasks Task defines computation step in a pipeline and has to be class inheriting taskchain.Task . Basic structure Example from taskchain import Task , Parameter import pandas as pd class MyTask ( Task ): class Meta : input_tasks = [ DependencyTask ] parameters = [ Parameter ( 'param' ) ] def run ( dependency , param ) -> pd . DataFrame : # ... compitation ... return result Meta subclass This class has to be defined inside every task and describes how should be the task handled by TaskChain. The Meta class in not meant to define any methods, it should only define some attributes: input_tasks ( List[Task | str] ) - dependency tasks, more below parameters ( List[Parameter] ) - parameters required by this task, which come from your configs, more below abstract ( bool ) - if True this task isn't instantiated and is never part of a chain, useful for inheritance name ( str ) - if not defined, name is derived from class name group ( str ) data_class ( Type[Data] ) - custom class to use for output data persistence data_type ( Type ) - output type of run method, alternative to typing notation run method This method is called by TaskChain when output of this task is requested and data are not already persisted. Definition of the method has to contain return type (using typing -> or data_type in Meta class). Value returned by run method is checked by TaskChain if matches defined type. Return type is important for data persistence . Tip The method can have arguments . TaskChain try to match and fill these arguments by values of parameters and input task. Warning Avoid expensive computation or loading data in __init__ . TaskChain can create task object multiple times and often task is not used at all. Put all expensive operation to run method. You can use @persistent decorator . Task names and groups Name of the task ( str ) is either defined by name attribute of Meta or it is derived from class name (converting to underscore notation and removing task at the end). Here are some examples: DataTask -> data FilteredDataTask -> filtered_data FilteredData -> filtered_data LongNameOfTheTask -> long_name_of_the Group of task allows keep some order in larger projects and have impact on location of persisted data. Usually task with a same group defines pipeline. The group can be defined in Meta class. and if it is the fullname of the task is group_name:task_name . If you need more rich structure of groups, you can use : to separate multiple levels of groups, e.g. group:subgroup . Tip Usually all task of a pipeline are defined in one module (file). To avoid defining same group in all tasks, it is possible inherit from ModuleTask or DoubleModuleTask instead of Task . In that case group is set to module name. Parameters Parameters are connection between tasks and configs. Parameters defined in Meta tell TaskChain which values should be extracted from configs and provided for run method. Parameter can be accessed through arguments of run method or directly from class's ParameterRegistry : self.params.my_param_name or self.params['my_param_name'] . Example class AllDataTask ( Task ): class Meta : parameters = [ Parameter ( 'input_file' ) ] def run ( self , input_file ) -> pd . DataFrame assert input_file == self . params . input_file return pd . read_csv ( input_file ) class FilteredDataTask ( Tasks ): class Meta : input_tasks = [ AllDataTask ] parameters = [ Parameter ( 'min_value' , default = 0 ) Parameter ( 'max_value' ) ] def run ( self , all_data , min_value , max_value ) -> pd . DataFrame return all_data . query ( ' {min_Value} <= value <= {max_value} ' ) Parameter's arguments name - name for referencing from task default - value used if not provided in config, defaults to NO_DEFAULT meaning that param is required name_in_config - name used for search in config, defaults to name argument dtype - expected datatype ignore_persistence - do not use this parameter in persistence, useful for params without influence on output, e.g. verbose or debug dont_persist_default_value - if value of parameter is same as the default, do not use it in persistence. This is useful for adding new parameters without recomputation of old data Tip You can use pathlib.Path as datatype. Expected value in config is str , however, value provided by the parameter has type of Path . Reserved parameter names Following names have special meaning in configs and cannot be used as parameter names: tasks , excluded_tasks uses human_readable_data_name configs , for_namespaces , main_part Input tasks Input tasks are connection between tasks. This Meta argument tells TaskChain which other tasks are prerequisites of this task. Values (data) of input tasks can be accessed through arguments of run method or directly from class's InputTasks : self.input_task['my_task'].value . It is also possible access input task by index: self.input_task[0].value . This can be useful if task inheritance is used. Then run method can stay unchanged and only input_tasks can be redefined. Input task can be defined in following ways: by class : input_tasks = [MyDataTask] - this way is preferred if possible by name : input_tasks = ['my_data'] by name and group : input_tasks = ['group:my_data'] by name, group and namespace : input_tasks = ['namespace::group:my_data'] Data persistence Persisting output of tasks is main feature of the TaskChain. When run method produce value (data) TaskChain saves this value and later when the value of the task is requested again, value is just loaded instead of calling run method again. Data class Saving and loading of values is handled by inheritors of taskchain.task.Data class. Witch class is used is determined automatically by return data type of run method or by data_class attribute of Meta . These Data classes are determined automatically: JSONData persists str , int , float , bool , dict , list types into .json files NumpyData persists np.ndarray type into .npy file PandasData persists pd.DataFrame or pd.Series type into .pd file FigureData persists plt.Figure type into pickle but also saves plot as .png and .svg for easy sharing. Use pylab or seaborn as usual and just return plt.gcf() . GeneratedData is used if return type is Generator . It is assumed that generated values are json-like. Values are saved to .jsonl file (json lines). Other useful Data classes which have to be explicitly defined in data_class attribute. InMemoryData - this special type is not persisting and value is saved only in memory of process. Example class MyTask ( Task ): class Meta : data_class = InMemoryData def run () -> WhatEver : # ... return whatever DirData - this class allows save arbitrary data to provided directory, but data have to be handled inside run method manually. Value of the task is Path of this directory. Example class MyTask ( Task ): def run ( self ) -> DirData : # ... data_object = self . get_data_object () self . save ( my_data_1 , data_object . dir / 'my_data_1.pickle' ) self . save ( my_data_2 , data_object . dir / 'my_data_2.pickle' ) return data_object def save ( self , data , dir_path ): ... ContinuesData - for task with large run time, e.g. training of large models, it is possible to make computation on multiple runs. This allows to save partial results and next call of run method starts from last checkpoint when computation is interrupted. Example class TrainModel ( Task ): class Meta : ... def run ( self ) -> ContinuesData : data : ContinuesData = self . get_data_object () working_dir = data . dir self . prepare_model () checkpoint_path = working_dir / 'checkpoint' if checkpoint_path . exists (): self . load_checkpoint ( checkpoint_path ) self . train ( save_path = working_dir / 'trained_model' checkpoint_path = checkpoint_path ) # training should save checkpoint periodically and trained model at the end data . finished () return data H5Data - special case of ContinuesData which allows to compute large data files. Example class Embeddings ( Task ): class Meta : ... def run ( self ) -> H5Data : data : H5Data = self . get_data_object () with data . data_file () as data_file : h5_emb_dataset = data . dataset ( 'embedding' , data_file , maxshape = ( None , embedding_size ), dtype = np . float32 ) progress = h5_emb_dataset . len () for i , row in enumerate ( my_dataset [ progress :]): if i % 1000 == 0 : gc . collect () emb = self . get_embedding ( row ) data . append_data ( h5_emb_dataset , [ emb ], dataset_len = progress ) data_file . flush () progress += 1 data . finished () return data You can define ad hoc Data classes to handle other data types. Returning Data object directly In some cases it is convenient to return (by run method) Data object directly. DirData is one example. Other use case is custom object which inherits from InMemoryData . See TrainedModel task in example project which returns RatingModel directly. This is the way to easily expose a important object to other tasks in the pipeline. Logging TaskChain offer two ways to save addition information about computation mainly for debug purposes. Run info After run method finishes and result value is saved to disk Data object also save additional information about computation. It is possible add any json-like information to this info. class MyTask ( Task ): ... def run ( self ): ... self . save_to_run_info ( 'some important information' ) self . save_to_run_info ({ 'records_processes' : 42 , 'errors' : 0 }) The run info is saved as YAML and is available under task_object.run_info in json-like form. hash.run_info.yaml task : class : Movies module : movie_ratings.tasks.movies name : movies:movies config : context : null name : imdb.filtered/movies:movies namespace : null input_tasks : movies:all_movies : 436f7a5e06e540716b275a5f84499a78 log : - some important information - records_processes : 42 errors' : 0 parameters : from_year : '1945' min_vote_count : '1000' to_year : None started : '2021-07-11 11:34:01.520866' ended : '2021-07-11 11:34:01.850913' time : 0.3300471305847168 user : name : your_system_name logger Each task has its own standard python logger, which can be used from run method. class MyTask ( Task ): ... def run ( self ): ... self . logger . debug ( 'not so important information' ) This logger has two handlers File handler managed by Data object which saves log along value produced by task. Logging level of this handler is set to DEBUG . Other handler is managed by chain object and log to console. Logging level of this handler is set to WARNING and can be changed from chain by chain.set_log_level('DEBUG') . Advanced topics Tasks inheritance Tasks are classes and can be inherited. This simplifies cases when pipeline contains tasks with similar functionality. You can inherit a task and change his behaviour by changing Meta class you can change input tasks and then computation will be done with different input. In this case, it is not possible have input tasks in run arguments, and they can access by self.input_tasks[0].value . This way, use of the task name, which is changing, is avoided. you can add custom attribute to Meta class and access it by self.meta.my_attribute and make computation based on its value. you can override some methods used by run method It is possible declare in Meta class abstract = True . In that case, task will be not recognized by project.tasks.pipeline.* in config and will not be part of your pipeline. This can be useful for tasks, which will be inherited from. Example of task inheritance can be found in example project movies pipeline - search for ExtractFeatureTask . model pipeline - search for DataSelectionTask .","title":"Tasks"},{"location":"tasks/#tasks","text":"Task defines computation step in a pipeline and has to be class inheriting taskchain.Task .","title":"Tasks"},{"location":"tasks/#basic-structure","text":"Example from taskchain import Task , Parameter import pandas as pd class MyTask ( Task ): class Meta : input_tasks = [ DependencyTask ] parameters = [ Parameter ( 'param' ) ] def run ( dependency , param ) -> pd . DataFrame : # ... compitation ... return result","title":"Basic structure"},{"location":"tasks/#meta-subclass","text":"This class has to be defined inside every task and describes how should be the task handled by TaskChain. The Meta class in not meant to define any methods, it should only define some attributes: input_tasks ( List[Task | str] ) - dependency tasks, more below parameters ( List[Parameter] ) - parameters required by this task, which come from your configs, more below abstract ( bool ) - if True this task isn't instantiated and is never part of a chain, useful for inheritance name ( str ) - if not defined, name is derived from class name group ( str ) data_class ( Type[Data] ) - custom class to use for output data persistence data_type ( Type ) - output type of run method, alternative to typing notation","title":"Meta subclass"},{"location":"tasks/#run-method","text":"This method is called by TaskChain when output of this task is requested and data are not already persisted. Definition of the method has to contain return type (using typing -> or data_type in Meta class). Value returned by run method is checked by TaskChain if matches defined type. Return type is important for data persistence . Tip The method can have arguments . TaskChain try to match and fill these arguments by values of parameters and input task. Warning Avoid expensive computation or loading data in __init__ . TaskChain can create task object multiple times and often task is not used at all. Put all expensive operation to run method. You can use @persistent decorator .","title":"run method"},{"location":"tasks/#task-names-and-groups","text":"Name of the task ( str ) is either defined by name attribute of Meta or it is derived from class name (converting to underscore notation and removing task at the end). Here are some examples: DataTask -> data FilteredDataTask -> filtered_data FilteredData -> filtered_data LongNameOfTheTask -> long_name_of_the Group of task allows keep some order in larger projects and have impact on location of persisted data. Usually task with a same group defines pipeline. The group can be defined in Meta class. and if it is the fullname of the task is group_name:task_name . If you need more rich structure of groups, you can use : to separate multiple levels of groups, e.g. group:subgroup . Tip Usually all task of a pipeline are defined in one module (file). To avoid defining same group in all tasks, it is possible inherit from ModuleTask or DoubleModuleTask instead of Task . In that case group is set to module name.","title":"Task names and groups"},{"location":"tasks/#parameters","text":"Parameters are connection between tasks and configs. Parameters defined in Meta tell TaskChain which values should be extracted from configs and provided for run method. Parameter can be accessed through arguments of run method or directly from class's ParameterRegistry : self.params.my_param_name or self.params['my_param_name'] . Example class AllDataTask ( Task ): class Meta : parameters = [ Parameter ( 'input_file' ) ] def run ( self , input_file ) -> pd . DataFrame assert input_file == self . params . input_file return pd . read_csv ( input_file ) class FilteredDataTask ( Tasks ): class Meta : input_tasks = [ AllDataTask ] parameters = [ Parameter ( 'min_value' , default = 0 ) Parameter ( 'max_value' ) ] def run ( self , all_data , min_value , max_value ) -> pd . DataFrame return all_data . query ( ' {min_Value} <= value <= {max_value} ' )","title":"Parameters"},{"location":"tasks/#parameters-arguments","text":"name - name for referencing from task default - value used if not provided in config, defaults to NO_DEFAULT meaning that param is required name_in_config - name used for search in config, defaults to name argument dtype - expected datatype ignore_persistence - do not use this parameter in persistence, useful for params without influence on output, e.g. verbose or debug dont_persist_default_value - if value of parameter is same as the default, do not use it in persistence. This is useful for adding new parameters without recomputation of old data Tip You can use pathlib.Path as datatype. Expected value in config is str , however, value provided by the parameter has type of Path .","title":"Parameter's arguments"},{"location":"tasks/#reserved-parameter-names","text":"Following names have special meaning in configs and cannot be used as parameter names: tasks , excluded_tasks uses human_readable_data_name configs , for_namespaces , main_part","title":"Reserved parameter names"},{"location":"tasks/#input-tasks","text":"Input tasks are connection between tasks. This Meta argument tells TaskChain which other tasks are prerequisites of this task. Values (data) of input tasks can be accessed through arguments of run method or directly from class's InputTasks : self.input_task['my_task'].value . It is also possible access input task by index: self.input_task[0].value . This can be useful if task inheritance is used. Then run method can stay unchanged and only input_tasks can be redefined. Input task can be defined in following ways: by class : input_tasks = [MyDataTask] - this way is preferred if possible by name : input_tasks = ['my_data'] by name and group : input_tasks = ['group:my_data'] by name, group and namespace : input_tasks = ['namespace::group:my_data']","title":"Input tasks"},{"location":"tasks/#data-persistence","text":"Persisting output of tasks is main feature of the TaskChain. When run method produce value (data) TaskChain saves this value and later when the value of the task is requested again, value is just loaded instead of calling run method again.","title":"Data persistence"},{"location":"tasks/#data-class","text":"Saving and loading of values is handled by inheritors of taskchain.task.Data class. Witch class is used is determined automatically by return data type of run method or by data_class attribute of Meta . These Data classes are determined automatically: JSONData persists str , int , float , bool , dict , list types into .json files NumpyData persists np.ndarray type into .npy file PandasData persists pd.DataFrame or pd.Series type into .pd file FigureData persists plt.Figure type into pickle but also saves plot as .png and .svg for easy sharing. Use pylab or seaborn as usual and just return plt.gcf() . GeneratedData is used if return type is Generator . It is assumed that generated values are json-like. Values are saved to .jsonl file (json lines). Other useful Data classes which have to be explicitly defined in data_class attribute. InMemoryData - this special type is not persisting and value is saved only in memory of process. Example class MyTask ( Task ): class Meta : data_class = InMemoryData def run () -> WhatEver : # ... return whatever DirData - this class allows save arbitrary data to provided directory, but data have to be handled inside run method manually. Value of the task is Path of this directory. Example class MyTask ( Task ): def run ( self ) -> DirData : # ... data_object = self . get_data_object () self . save ( my_data_1 , data_object . dir / 'my_data_1.pickle' ) self . save ( my_data_2 , data_object . dir / 'my_data_2.pickle' ) return data_object def save ( self , data , dir_path ): ... ContinuesData - for task with large run time, e.g. training of large models, it is possible to make computation on multiple runs. This allows to save partial results and next call of run method starts from last checkpoint when computation is interrupted. Example class TrainModel ( Task ): class Meta : ... def run ( self ) -> ContinuesData : data : ContinuesData = self . get_data_object () working_dir = data . dir self . prepare_model () checkpoint_path = working_dir / 'checkpoint' if checkpoint_path . exists (): self . load_checkpoint ( checkpoint_path ) self . train ( save_path = working_dir / 'trained_model' checkpoint_path = checkpoint_path ) # training should save checkpoint periodically and trained model at the end data . finished () return data H5Data - special case of ContinuesData which allows to compute large data files. Example class Embeddings ( Task ): class Meta : ... def run ( self ) -> H5Data : data : H5Data = self . get_data_object () with data . data_file () as data_file : h5_emb_dataset = data . dataset ( 'embedding' , data_file , maxshape = ( None , embedding_size ), dtype = np . float32 ) progress = h5_emb_dataset . len () for i , row in enumerate ( my_dataset [ progress :]): if i % 1000 == 0 : gc . collect () emb = self . get_embedding ( row ) data . append_data ( h5_emb_dataset , [ emb ], dataset_len = progress ) data_file . flush () progress += 1 data . finished () return data You can define ad hoc Data classes to handle other data types.","title":"Data class"},{"location":"tasks/#returning-data-object-directly","text":"In some cases it is convenient to return (by run method) Data object directly. DirData is one example. Other use case is custom object which inherits from InMemoryData . See TrainedModel task in example project which returns RatingModel directly. This is the way to easily expose a important object to other tasks in the pipeline.","title":"Returning Data object directly"},{"location":"tasks/#logging","text":"TaskChain offer two ways to save addition information about computation mainly for debug purposes.","title":"Logging"},{"location":"tasks/#run-info","text":"After run method finishes and result value is saved to disk Data object also save additional information about computation. It is possible add any json-like information to this info. class MyTask ( Task ): ... def run ( self ): ... self . save_to_run_info ( 'some important information' ) self . save_to_run_info ({ 'records_processes' : 42 , 'errors' : 0 }) The run info is saved as YAML and is available under task_object.run_info in json-like form. hash.run_info.yaml task : class : Movies module : movie_ratings.tasks.movies name : movies:movies config : context : null name : imdb.filtered/movies:movies namespace : null input_tasks : movies:all_movies : 436f7a5e06e540716b275a5f84499a78 log : - some important information - records_processes : 42 errors' : 0 parameters : from_year : '1945' min_vote_count : '1000' to_year : None started : '2021-07-11 11:34:01.520866' ended : '2021-07-11 11:34:01.850913' time : 0.3300471305847168 user : name : your_system_name","title":"Run info"},{"location":"tasks/#logger","text":"Each task has its own standard python logger, which can be used from run method. class MyTask ( Task ): ... def run ( self ): ... self . logger . debug ( 'not so important information' ) This logger has two handlers File handler managed by Data object which saves log along value produced by task. Logging level of this handler is set to DEBUG . Other handler is managed by chain object and log to console. Logging level of this handler is set to WARNING and can be changed from chain by chain.set_log_level('DEBUG') .","title":"logger"},{"location":"tasks/#advanced-topics","text":"","title":"Advanced topics"},{"location":"tasks/#tasks-inheritance","text":"Tasks are classes and can be inherited. This simplifies cases when pipeline contains tasks with similar functionality. You can inherit a task and change his behaviour by changing Meta class you can change input tasks and then computation will be done with different input. In this case, it is not possible have input tasks in run arguments, and they can access by self.input_tasks[0].value . This way, use of the task name, which is changing, is avoided. you can add custom attribute to Meta class and access it by self.meta.my_attribute and make computation based on its value. you can override some methods used by run method It is possible declare in Meta class abstract = True . In that case, task will be not recognized by project.tasks.pipeline.* in config and will not be part of your pipeline. This can be useful for tasks, which will be inherited from. Example of task inheritance can be found in example project movies pipeline - search for ExtractFeatureTask . model pipeline - search for DataSelectionTask .","title":"Tasks inheritance"},{"location":"testing/","text":"Testing To help with testing of tasks and chains TaskChain offers some helper tools. Testing of a single task A task usually exists in a context of chain with other tasks and configs. Helper function create_test_task create tasks and mocks the context. Testing of a task ... class MyTask ( Task ): class Meta : input_tasks = [ ATask , BTask ] parameters = [ Parameter ( 'p1' ), Parameter ( 'p2' , default = 3 ), ] def run ( self , a , b , p1 , p2 ) -> int : ... return a + b + p1 + p2 from taskchain.utils.testing import create_test_task task = create_test_task ( MyTask , # class of tested task input_tasks = { # mocked input task values 'a' : 7 , # task can be referenced by name BTask : 6 , # or by class }, parameters = { 'p1' : 42 , # parameters provided to tested task } ) assert task . value == 7 + 6 + 42 + 3 Testing of a part of a chain You can also test more tasks together. Class TestChain creates chain where some tasks are mocked, i.e. their values are not computed but provided on creation of test chain. Testing of a part of chain ... class KTask ( Task ): class Meta : input_tasks = [ JTask , BTask ] parameters = [ Parameter ( 'p2' )] def run ( self , j , b , p2 ) -> int : ... class LTask ( Task ): class Meta : input_tasks = [ KTask ] parameters = [ Parameter ( 'p2' )] def run ( self , k , pc ) -> int : ... from taskchain.utils.testing import TestChain chain = TestChain ( tasks = [ KTask , LTask ], mock_tasks = { 'j' : ... , BTask : ... , }, parameters = { 'p1' : ... , 'p2' : ... , }, ) assert chain . k . value == ... assert chain . l . value == ...","title":"Testing"},{"location":"testing/#testing","text":"To help with testing of tasks and chains TaskChain offers some helper tools.","title":"Testing"},{"location":"testing/#testing-of-a-single-task","text":"A task usually exists in a context of chain with other tasks and configs. Helper function create_test_task create tasks and mocks the context. Testing of a task ... class MyTask ( Task ): class Meta : input_tasks = [ ATask , BTask ] parameters = [ Parameter ( 'p1' ), Parameter ( 'p2' , default = 3 ), ] def run ( self , a , b , p1 , p2 ) -> int : ... return a + b + p1 + p2 from taskchain.utils.testing import create_test_task task = create_test_task ( MyTask , # class of tested task input_tasks = { # mocked input task values 'a' : 7 , # task can be referenced by name BTask : 6 , # or by class }, parameters = { 'p1' : 42 , # parameters provided to tested task } ) assert task . value == 7 + 6 + 42 + 3","title":"Testing of a single task"},{"location":"testing/#testing-of-a-part-of-a-chain","text":"You can also test more tasks together. Class TestChain creates chain where some tasks are mocked, i.e. their values are not computed but provided on creation of test chain. Testing of a part of chain ... class KTask ( Task ): class Meta : input_tasks = [ JTask , BTask ] parameters = [ Parameter ( 'p2' )] def run ( self , j , b , p2 ) -> int : ... class LTask ( Task ): class Meta : input_tasks = [ KTask ] parameters = [ Parameter ( 'p2' )] def run ( self , k , pc ) -> int : ... from taskchain.utils.testing import TestChain chain = TestChain ( tasks = [ KTask , LTask ], mock_tasks = { 'j' : ... , BTask : ... , }, parameters = { 'p1' : ... , 'p2' : ... , }, ) assert chain . k . value == ... assert chain . l . value == ...","title":"Testing of a part of a chain"},{"location":"utils/","text":"Utils ic ( IceCream ) If you import taskschain ic is installed and you can use it whithout import. Caching For ease up saving \"expensive\" computation, TaskChain have simple caching tools. This can be used for saving e.g. api calls. from taskchain.cache import JsonCache cache = JsonCache ( '/path/to/cache_dir' ) input_ = 42 def _computation (): return expensive ( input_ ) cache_key = f 'key_ { input_ } ' result = cache . get_or_compute ( cache_key , _computation ) Result is loaded from cache if presented or computed and saved in cache. You can also use @cached decorator which can handle creation of cache key automatically from arguments. from taskchain.cache import JsonCache , cached class MyClass : @cached ( JsonCache ( '/path/to/cache_dir' )) def cached_method ( self , input_ ): return expensive ( input_ ) my = MyClass () result = my . cached_method ( 42 ) There are multiple Cache classes prepared DummyCache - no caching InMemoryCache - values are cached only in memory, all types are allowed JsonCache - saves json-like objects to json DataFrameCache NumpyArrayCache FileCache - abstract class useful for implementing own type of caches @persistent decorator This decorator can be used on class methods without arguments . Result of this method is stored in self.__method_name attribute after first call. On other calls stored value is returned. Tip You can also combine @persistent with @property decorator, just make sure that @property is before @persistent . This can be useful in implementation of lazy behaviour of your classes. Example lazy solution class MyClass : @property @persistent def foo ( self ): return expensive_computation () classic solution class MyClass : def __init__ ( self ): self . foo = expensive_computation () parallel_map You can use parallel_map for easy utilization of threading. from taskchain.utils.threading import parallel_map def download_urls ( urls : list , threads = 2 ): def _fun ( url ): return download ( url ) return parallel_map ( _fun , urls , threads = threads , desc = 'Downloading' , total = len ( urls )) @repeat_on_error decorator This decorator is useful for calling api or downloading data from internet. It tries to run a method again if error occurs. from taskchain.utils.clazz import repeat_on_error class Downloader : # first retry is after 2 second, second after 4, third after 8 @repeat_on_error ( waiting_time = 2 , wait_extension = 2 , retries = 3 ) def download ( self , url , exact_match = True ): ...","title":"Utils"},{"location":"utils/#utils","text":"","title":"Utils"},{"location":"utils/#ic-icecream","text":"If you import taskschain ic is installed and you can use it whithout import.","title":"ic ( IceCream )"},{"location":"utils/#caching","text":"For ease up saving \"expensive\" computation, TaskChain have simple caching tools. This can be used for saving e.g. api calls. from taskchain.cache import JsonCache cache = JsonCache ( '/path/to/cache_dir' ) input_ = 42 def _computation (): return expensive ( input_ ) cache_key = f 'key_ { input_ } ' result = cache . get_or_compute ( cache_key , _computation ) Result is loaded from cache if presented or computed and saved in cache. You can also use @cached decorator which can handle creation of cache key automatically from arguments. from taskchain.cache import JsonCache , cached class MyClass : @cached ( JsonCache ( '/path/to/cache_dir' )) def cached_method ( self , input_ ): return expensive ( input_ ) my = MyClass () result = my . cached_method ( 42 ) There are multiple Cache classes prepared DummyCache - no caching InMemoryCache - values are cached only in memory, all types are allowed JsonCache - saves json-like objects to json DataFrameCache NumpyArrayCache FileCache - abstract class useful for implementing own type of caches","title":"Caching"},{"location":"utils/#persistent-decorator","text":"This decorator can be used on class methods without arguments . Result of this method is stored in self.__method_name attribute after first call. On other calls stored value is returned. Tip You can also combine @persistent with @property decorator, just make sure that @property is before @persistent . This can be useful in implementation of lazy behaviour of your classes. Example lazy solution class MyClass : @property @persistent def foo ( self ): return expensive_computation () classic solution class MyClass : def __init__ ( self ): self . foo = expensive_computation ()","title":"@persistent decorator"},{"location":"utils/#parallel_map","text":"You can use parallel_map for easy utilization of threading. from taskchain.utils.threading import parallel_map def download_urls ( urls : list , threads = 2 ): def _fun ( url ): return download ( url ) return parallel_map ( _fun , urls , threads = threads , desc = 'Downloading' , total = len ( urls ))","title":"parallel_map"},{"location":"utils/#repeat_on_error-decorator","text":"This decorator is useful for calling api or downloading data from internet. It tries to run a method again if error occurs. from taskchain.utils.clazz import repeat_on_error class Downloader : # first retry is after 2 second, second after 4, third after 8 @repeat_on_error ( waiting_time = 2 , wait_extension = 2 , retries = 3 ) def download ( self , url , exact_match = True ): ...","title":"@repeat_on_error decorator"},{"location":"code/chain/","text":"Chain Chain takes a config, recursively load prerequisite configs, initialize tasks connect them to DAG vie input tasks. tasks_df : DataFrame property readonly Dataframe with rows ass all tasks in chain. set_log_level ( level ) classmethod Set log level to log handler responsible for console output of task loggers. Source code in taskchain/chain.py @classmethod def set_log_level ( cls , level ): \"\"\" Set log level to log handler responsible for console output of task loggers. \"\"\" Chain . log_handler . setLevel ( level ) __getitem__ ( self , item ) special Get task by name in dict-like fashion. Source code in taskchain/chain.py def __getitem__ ( self , item ): \"\"\" Get task by name in dict-like fashion. \"\"\" return self . get ( item ) __getattr__ ( self , item ) special Get task by name as atribute. Source code in taskchain/chain.py def __getattr__ ( self , item ): \"\"\" Get task by name as atribute. \"\"\" if item in self : return self . get ( item ) return self . __getattribute__ ( item ) get ( self , item , default = None ) Get task by name. Source code in taskchain/chain.py def get ( self , item , default = None ): \"\"\" Get task by name. \"\"\" if default is not None : raise ValueError ( 'Default task is not allowed' ) return self . tasks . get ( _find_task_full_name ( item , self . tasks . keys ())) is_task_dependent_on ( self , task , dependency_task ) Check whether a task is dependant on dependency task. Source code in taskchain/chain.py def is_task_dependent_on ( self , task : Union [ str , Task ], dependency_task : Union [ str , Task ]) -> bool : \"\"\" Check whether a task is dependant on dependency task. \"\"\" task = self . get_task ( task ) dependency_task = self . get_task ( dependency_task ) return nx . has_path ( self . graph , dependency_task , task ) dependent_tasks ( self , task , include_self = False ) Get all tasks which depend ald given task. Source code in taskchain/chain.py def dependent_tasks ( self , task : Union [ str , Task ], include_self : bool = False ) -> Set [ Task ]: \"\"\" Get all tasks which depend ald given task. \"\"\" task = self . get_task ( task ) descendants = nx . descendants ( self . graph , task ) if include_self : descendants . add ( task ) return descendants required_tasks ( self , task , include_self = False ) Get all task which are required fot given task. Source code in taskchain/chain.py def required_tasks ( self , task : Union [ str , Task ], include_self : bool = False ) -> Set [ Task ]: \"\"\" Get all task which are required fot given task. \"\"\" task = self . get_task ( task ) ancestors = nx . ancestors ( self . graph , task ) if include_self : ancestors . add ( task ) return ancestors force ( self , tasks , recompute = False , delete_data = False ) Force recomputation of of given tasks and all dependant tasks. If either additional argument is used, recomputation must be done manually, e.g. by calling chain.my_task.value for each task. Parameters: Name Type Description Default tasks Union[str, taskchain.task.Task, Iterable[Union[str, taskchain.task.Task]]] task as objects or names or list of them required recompute automatically recompute all forced tasks False delete_data also delete persisted data of forced tasks False Source code in taskchain/chain.py def force ( self , tasks : Union [ str , Task , Iterable [ Union [ str , Task ]]], recompute = False , delete_data = False ): \"\"\" Force recomputation of of given tasks and all dependant tasks. If either additional argument is used, recomputation must be done manually, e.g. by calling `chain.my_task.value` for each task. Args: tasks: task as objects or names or list of them recompute: automatically recompute all forced tasks delete_data: also delete persisted data of forced tasks \"\"\" if type ( tasks ) is str or isinstance ( tasks , Task ): tasks = [ tasks ] forced_tasks = set () for task in tasks : forced_tasks |= self . dependent_tasks ( task , include_self = True ) for task in forced_tasks : task . force ( delete_data = delete_data ) if recompute : for task in list ( forced_tasks )[:: - 1 ]: _ = task . value draw ( self , groups_to_show = None ) Draw graph of tasks. Color is based on tasks' group. Border is based on data state: none - is not persisting data ( InMemoryData ) dashed - data not computed solid - data computed Parameters: Name Type Description Default groups_to_show str or list[str] limit drawn tasks to given groups and their neighbours None Source code in taskchain/chain.py def draw ( self , groups_to_show = None ): \"\"\" Draw graph of tasks. Color is based on tasks' group. Border is based on data state: - **none** - is not persisting data (`InMemoryData`) - **dashed** - data not computed - **solid** - data computed Args: groups_to_show (str or list[str]): limit drawn tasks to given groups and their neighbours \"\"\" import graphviz as gv import seaborn as sns groups_to_show = list_or_str_to_list ( groups_to_show ) node_attr = { 'shape' : 'box' , 'width' : '2' } graph_attr = { 'splines' : 'ortho' } edge_attr = {} groups = list ({( n . get_config () . namespace , n . group ) for n in self . graph . nodes }) colors = sns . color_palette ( 'pastel' , len ( groups )) . as_hex () G = gv . Digraph ( format = 'png' , engine = 'dot' , graph_attr = graph_attr , node_attr = node_attr , edge_attr = edge_attr ) def _is_node_in_groups ( node ): if not groups_to_show : return True return node . group in groups_to_show nodes = set () for edge in self . graph . edges : if _is_node_in_groups ( edge [ 0 ]) or _is_node_in_groups ( edge [ 1 ]): nodes . add ( edge [ 0 ]) nodes . add ( edge [ 1 ]) def _get_slugname ( task : Task ): return f ' { task . slugname . split ( \":\" )[ - 1 ] } # { task . get_config () . get_name_for_persistence ( task ) } ' for node in nodes : color = colors [ groups . index (( node . get_config () . namespace , node . group ))] style = [ 'filled' ] if not ( node . has_data or issubclass ( node . data_class , InMemoryData )): style . append ( 'dashed' ) attrs = { 'label' : f \"<<FONT POINT-SIZE='10'> { ':' . join ( node . fullname . split ( ':' )[: - 1 ]) } </FONT> <BR/> { node . fullname . split ( ':' )[ - 1 ] } >\" , 'fillcolor' : color , 'color' : color if issubclass ( node . data_class , InMemoryData ) else 'black' , 'style' : ',' . join ( style ), } if not _is_node_in_groups ( node ): attrs [ 'shape' ] = 'note' G . node ( _get_slugname ( node ), ** attrs , ) for edge in self . graph . edges : if edge [ 0 ] in nodes and edge [ 1 ] in nodes : G . edge ( _get_slugname ( edge [ 0 ]), _get_slugname ( edge [ 1 ])) return G create_readable_filenames ( self , groups = None , name = None , verbose = False , keep_existing = False ) Create human readable symlink to data of tasks in the chain. Symlink is in same directory as data, i.e. in directory with all task's data. Name of link is taken from first available in order: this method's parameter task's config, parameter human_readable_data_name name of task's config Parameters: Name Type Description Default groups optional name of group or list of names of groups, for which should be symlinks created None name optional name of links None verbose False keep_existing rewrite link if link already exists False Source code in taskchain/chain.py def create_readable_filenames ( self , groups = None , name = None , verbose = False , keep_existing = False ): \"\"\" Create human readable symlink to data of tasks in the chain. Symlink is in same directory as data, i.e. in directory with all task's data. Name of link is taken from first available in order: - this method's parameter - task's config, parameter `human_readable_data_name` - name of task's config Args: groups (optional): name of group or list of names of groups, for which should be symlinks created name (optional): name of links verbose: keep_existing: rewrite link if link already exists \"\"\" symlink_actions = defaultdict ( list ) groups = list_or_str_to_list ( groups ) for task_name , task in self . tasks . items (): if groups is not None and task . group not in groups : continue if not task . has_data : continue an , n , sp = self . _create_softlink_to_task_data ( task , name , keep_existing = keep_existing ) symlink_actions [ n ] . append (( an , sp )) if verbose : for name , actions in symlink_actions . items (): print ( f ' { name } ' ) for action_name , symlink_path in actions : print ( f ' { action_name } link to { symlink_path } ' ) ChainObject If ParameterObject inherits this class, chain call init_chain on initialization and allow object to access whole chain. MultiChain Hold multiple chains which share task object, i.e. it can be more memory efficient then dict of chains. Otherwise behaves as dict of chains. from_dir ( data_dir , dir_path , ** kwargs ) classmethod Create MultiConfig from directory of configs. Parameters: Name Type Description Default data_dir Path tasks data persistence path required dir_path Path directory with configs required **kwargs other arguments passed to Config, e.g. global_vars {} Returns: Type Description MultiChain MultiChain based on all configs in dir Source code in taskchain/chain.py @classmethod def from_dir ( cls , data_dir : Path , dir_path : Path , ** kwargs ) -> 'MultiChain' : \"\"\" Create MultiConfig from directory of configs. Args: data_dir: tasks data persistence path dir_path: directory with configs **kwargs: other arguments passed to Config, e.g. global_vars Returns: MultiChain based on all configs in dir \"\"\" configs = [] for config_file in dir_path . iterdir (): configs . append ( Config ( data_dir , config_file , ** kwargs ) ) return MultiChain ( configs ) __init__ ( self , configs , parameter_mode = True ) special Parameters: Name Type Description Default configs Sequence[taskchain.config.Config] list of Config objects from which Chains are created. required parameter_mode bool True Source code in taskchain/chain.py def __init__ ( self , configs : Sequence [ Config ], parameter_mode : bool = True ): \"\"\" Args: configs: list of Config objects from which Chains are created. parameter_mode: \"\"\" self . _tasks : Dict [ Tuple [ str , str ], Task ] = {} self . chains : Dict [ str , Chain ] = {} self . _base_configs = configs self . parameter_mode = parameter_mode self . _prepare () force ( self , tasks , ** kwargs ) Pass force to all chains. Source code in taskchain/chain.py def force ( self , tasks : Union [ str , Iterable [ Union [ str , Task ]]], ** kwargs ): \"\"\" Pass force to all chains. \"\"\" for chain in self . chains . values (): chain . force ( tasks , ** kwargs ) latest ( self , chain_name = None ) Get latest chain based on name (alphabetically last) Parameters: Name Type Description Default chain_name str return latest chain from chain with name containing chain_name None Source code in taskchain/chain.py def latest ( self , chain_name : str = None ): \"\"\" Get latest chain based on name (alphabetically last) Args: chain_name: return latest chain from chain with name containing `chain_name` \"\"\" for fullname , chain in sorted ( self . chains . items (), reverse = True ): if chain_name is None or chain_name in fullname : return chain set_log_level ( level ) classmethod Pass log level to all chains. Source code in taskchain/chain.py @classmethod def set_log_level ( cls , level ): \"\"\" Pass log level to all chains. \"\"\" Chain . log_handler . setLevel ( level ) TaskParameterConfig Helper config used in parameter mode. It takes instance of Task and create config with only parameters which are used by this task input tasks (already with TaskParameterConfig) From this data creates unique hash used in persistence Note: These configs creates a kind of blockchain. Each config is block, parameters are content of blocks and input_tasks are dependencies between blocks. Change in one config invalidates all dependant configs, which is property desired and required for correct functionality of TaskChains data persistence. repr_name : str property readonly Should be unique representation of this config repr_name_without_namespace property readonly Unique representation of this config without namespace get_name_for_persistence ( self , task ) Used for creating filename in task data persistence, should uniquely define config Source code in taskchain/chain.py def get_name_for_persistence ( self , task : Task ) -> str : def _get_input_task_repr ( _name , _task ): if outer_namespace := task . get_config () . namespace : # remove namespace of this task from task name assert _name . startswith ( outer_namespace ) _name = _name [ len ( outer_namespace ) + 2 :] return f ' { _name } = { _task } ' parameter_repr = task . parameters . repr input_tasks_repr = '###' . join ( _get_input_task_repr ( n , it ) for n , it in sorted ( self . input_tasks . items ())) return sha256 ( f ' { parameter_repr } $$$ { input_tasks_repr } ' . encode ()) . hexdigest ()[: 32 ] get_original_config ( self ) Get self of config from which this one is derived Source code in taskchain/chain.py def get_original_config ( self ): return self . original_config","title":"Chain"},{"location":"code/chain/#taskchain.chain.Chain","text":"Chain takes a config, recursively load prerequisite configs, initialize tasks connect them to DAG vie input tasks.","title":"Chain"},{"location":"code/chain/#taskchain.chain.Chain.tasks_df","text":"Dataframe with rows ass all tasks in chain.","title":"tasks_df"},{"location":"code/chain/#taskchain.chain.Chain.set_log_level","text":"Set log level to log handler responsible for console output of task loggers. Source code in taskchain/chain.py @classmethod def set_log_level ( cls , level ): \"\"\" Set log level to log handler responsible for console output of task loggers. \"\"\" Chain . log_handler . setLevel ( level )","title":"set_log_level()"},{"location":"code/chain/#taskchain.chain.Chain.__getitem__","text":"Get task by name in dict-like fashion. Source code in taskchain/chain.py def __getitem__ ( self , item ): \"\"\" Get task by name in dict-like fashion. \"\"\" return self . get ( item )","title":"__getitem__()"},{"location":"code/chain/#taskchain.chain.Chain.__getattr__","text":"Get task by name as atribute. Source code in taskchain/chain.py def __getattr__ ( self , item ): \"\"\" Get task by name as atribute. \"\"\" if item in self : return self . get ( item ) return self . __getattribute__ ( item )","title":"__getattr__()"},{"location":"code/chain/#taskchain.chain.Chain.get","text":"Get task by name. Source code in taskchain/chain.py def get ( self , item , default = None ): \"\"\" Get task by name. \"\"\" if default is not None : raise ValueError ( 'Default task is not allowed' ) return self . tasks . get ( _find_task_full_name ( item , self . tasks . keys ()))","title":"get()"},{"location":"code/chain/#taskchain.chain.Chain.is_task_dependent_on","text":"Check whether a task is dependant on dependency task. Source code in taskchain/chain.py def is_task_dependent_on ( self , task : Union [ str , Task ], dependency_task : Union [ str , Task ]) -> bool : \"\"\" Check whether a task is dependant on dependency task. \"\"\" task = self . get_task ( task ) dependency_task = self . get_task ( dependency_task ) return nx . has_path ( self . graph , dependency_task , task )","title":"is_task_dependent_on()"},{"location":"code/chain/#taskchain.chain.Chain.dependent_tasks","text":"Get all tasks which depend ald given task. Source code in taskchain/chain.py def dependent_tasks ( self , task : Union [ str , Task ], include_self : bool = False ) -> Set [ Task ]: \"\"\" Get all tasks which depend ald given task. \"\"\" task = self . get_task ( task ) descendants = nx . descendants ( self . graph , task ) if include_self : descendants . add ( task ) return descendants","title":"dependent_tasks()"},{"location":"code/chain/#taskchain.chain.Chain.required_tasks","text":"Get all task which are required fot given task. Source code in taskchain/chain.py def required_tasks ( self , task : Union [ str , Task ], include_self : bool = False ) -> Set [ Task ]: \"\"\" Get all task which are required fot given task. \"\"\" task = self . get_task ( task ) ancestors = nx . ancestors ( self . graph , task ) if include_self : ancestors . add ( task ) return ancestors","title":"required_tasks()"},{"location":"code/chain/#taskchain.chain.Chain.force","text":"Force recomputation of of given tasks and all dependant tasks. If either additional argument is used, recomputation must be done manually, e.g. by calling chain.my_task.value for each task. Parameters: Name Type Description Default tasks Union[str, taskchain.task.Task, Iterable[Union[str, taskchain.task.Task]]] task as objects or names or list of them required recompute automatically recompute all forced tasks False delete_data also delete persisted data of forced tasks False Source code in taskchain/chain.py def force ( self , tasks : Union [ str , Task , Iterable [ Union [ str , Task ]]], recompute = False , delete_data = False ): \"\"\" Force recomputation of of given tasks and all dependant tasks. If either additional argument is used, recomputation must be done manually, e.g. by calling `chain.my_task.value` for each task. Args: tasks: task as objects or names or list of them recompute: automatically recompute all forced tasks delete_data: also delete persisted data of forced tasks \"\"\" if type ( tasks ) is str or isinstance ( tasks , Task ): tasks = [ tasks ] forced_tasks = set () for task in tasks : forced_tasks |= self . dependent_tasks ( task , include_self = True ) for task in forced_tasks : task . force ( delete_data = delete_data ) if recompute : for task in list ( forced_tasks )[:: - 1 ]: _ = task . value","title":"force()"},{"location":"code/chain/#taskchain.chain.Chain.draw","text":"Draw graph of tasks. Color is based on tasks' group. Border is based on data state: none - is not persisting data ( InMemoryData ) dashed - data not computed solid - data computed Parameters: Name Type Description Default groups_to_show str or list[str] limit drawn tasks to given groups and their neighbours None Source code in taskchain/chain.py def draw ( self , groups_to_show = None ): \"\"\" Draw graph of tasks. Color is based on tasks' group. Border is based on data state: - **none** - is not persisting data (`InMemoryData`) - **dashed** - data not computed - **solid** - data computed Args: groups_to_show (str or list[str]): limit drawn tasks to given groups and their neighbours \"\"\" import graphviz as gv import seaborn as sns groups_to_show = list_or_str_to_list ( groups_to_show ) node_attr = { 'shape' : 'box' , 'width' : '2' } graph_attr = { 'splines' : 'ortho' } edge_attr = {} groups = list ({( n . get_config () . namespace , n . group ) for n in self . graph . nodes }) colors = sns . color_palette ( 'pastel' , len ( groups )) . as_hex () G = gv . Digraph ( format = 'png' , engine = 'dot' , graph_attr = graph_attr , node_attr = node_attr , edge_attr = edge_attr ) def _is_node_in_groups ( node ): if not groups_to_show : return True return node . group in groups_to_show nodes = set () for edge in self . graph . edges : if _is_node_in_groups ( edge [ 0 ]) or _is_node_in_groups ( edge [ 1 ]): nodes . add ( edge [ 0 ]) nodes . add ( edge [ 1 ]) def _get_slugname ( task : Task ): return f ' { task . slugname . split ( \":\" )[ - 1 ] } # { task . get_config () . get_name_for_persistence ( task ) } ' for node in nodes : color = colors [ groups . index (( node . get_config () . namespace , node . group ))] style = [ 'filled' ] if not ( node . has_data or issubclass ( node . data_class , InMemoryData )): style . append ( 'dashed' ) attrs = { 'label' : f \"<<FONT POINT-SIZE='10'> { ':' . join ( node . fullname . split ( ':' )[: - 1 ]) } </FONT> <BR/> { node . fullname . split ( ':' )[ - 1 ] } >\" , 'fillcolor' : color , 'color' : color if issubclass ( node . data_class , InMemoryData ) else 'black' , 'style' : ',' . join ( style ), } if not _is_node_in_groups ( node ): attrs [ 'shape' ] = 'note' G . node ( _get_slugname ( node ), ** attrs , ) for edge in self . graph . edges : if edge [ 0 ] in nodes and edge [ 1 ] in nodes : G . edge ( _get_slugname ( edge [ 0 ]), _get_slugname ( edge [ 1 ])) return G","title":"draw()"},{"location":"code/chain/#taskchain.chain.Chain.create_readable_filenames","text":"Create human readable symlink to data of tasks in the chain. Symlink is in same directory as data, i.e. in directory with all task's data. Name of link is taken from first available in order: this method's parameter task's config, parameter human_readable_data_name name of task's config Parameters: Name Type Description Default groups optional name of group or list of names of groups, for which should be symlinks created None name optional name of links None verbose False keep_existing rewrite link if link already exists False Source code in taskchain/chain.py def create_readable_filenames ( self , groups = None , name = None , verbose = False , keep_existing = False ): \"\"\" Create human readable symlink to data of tasks in the chain. Symlink is in same directory as data, i.e. in directory with all task's data. Name of link is taken from first available in order: - this method's parameter - task's config, parameter `human_readable_data_name` - name of task's config Args: groups (optional): name of group or list of names of groups, for which should be symlinks created name (optional): name of links verbose: keep_existing: rewrite link if link already exists \"\"\" symlink_actions = defaultdict ( list ) groups = list_or_str_to_list ( groups ) for task_name , task in self . tasks . items (): if groups is not None and task . group not in groups : continue if not task . has_data : continue an , n , sp = self . _create_softlink_to_task_data ( task , name , keep_existing = keep_existing ) symlink_actions [ n ] . append (( an , sp )) if verbose : for name , actions in symlink_actions . items (): print ( f ' { name } ' ) for action_name , symlink_path in actions : print ( f ' { action_name } link to { symlink_path } ' )","title":"create_readable_filenames()"},{"location":"code/chain/#taskchain.chain.ChainObject","text":"If ParameterObject inherits this class, chain call init_chain on initialization and allow object to access whole chain.","title":"ChainObject"},{"location":"code/chain/#taskchain.chain.MultiChain","text":"Hold multiple chains which share task object, i.e. it can be more memory efficient then dict of chains. Otherwise behaves as dict of chains.","title":"MultiChain"},{"location":"code/chain/#taskchain.chain.MultiChain.from_dir","text":"Create MultiConfig from directory of configs. Parameters: Name Type Description Default data_dir Path tasks data persistence path required dir_path Path directory with configs required **kwargs other arguments passed to Config, e.g. global_vars {} Returns: Type Description MultiChain MultiChain based on all configs in dir Source code in taskchain/chain.py @classmethod def from_dir ( cls , data_dir : Path , dir_path : Path , ** kwargs ) -> 'MultiChain' : \"\"\" Create MultiConfig from directory of configs. Args: data_dir: tasks data persistence path dir_path: directory with configs **kwargs: other arguments passed to Config, e.g. global_vars Returns: MultiChain based on all configs in dir \"\"\" configs = [] for config_file in dir_path . iterdir (): configs . append ( Config ( data_dir , config_file , ** kwargs ) ) return MultiChain ( configs )","title":"from_dir()"},{"location":"code/chain/#taskchain.chain.MultiChain.__init__","text":"Parameters: Name Type Description Default configs Sequence[taskchain.config.Config] list of Config objects from which Chains are created. required parameter_mode bool True Source code in taskchain/chain.py def __init__ ( self , configs : Sequence [ Config ], parameter_mode : bool = True ): \"\"\" Args: configs: list of Config objects from which Chains are created. parameter_mode: \"\"\" self . _tasks : Dict [ Tuple [ str , str ], Task ] = {} self . chains : Dict [ str , Chain ] = {} self . _base_configs = configs self . parameter_mode = parameter_mode self . _prepare ()","title":"__init__()"},{"location":"code/chain/#taskchain.chain.MultiChain.force","text":"Pass force to all chains. Source code in taskchain/chain.py def force ( self , tasks : Union [ str , Iterable [ Union [ str , Task ]]], ** kwargs ): \"\"\" Pass force to all chains. \"\"\" for chain in self . chains . values (): chain . force ( tasks , ** kwargs )","title":"force()"},{"location":"code/chain/#taskchain.chain.MultiChain.latest","text":"Get latest chain based on name (alphabetically last) Parameters: Name Type Description Default chain_name str return latest chain from chain with name containing chain_name None Source code in taskchain/chain.py def latest ( self , chain_name : str = None ): \"\"\" Get latest chain based on name (alphabetically last) Args: chain_name: return latest chain from chain with name containing `chain_name` \"\"\" for fullname , chain in sorted ( self . chains . items (), reverse = True ): if chain_name is None or chain_name in fullname : return chain","title":"latest()"},{"location":"code/chain/#taskchain.chain.MultiChain.set_log_level","text":"Pass log level to all chains. Source code in taskchain/chain.py @classmethod def set_log_level ( cls , level ): \"\"\" Pass log level to all chains. \"\"\" Chain . log_handler . setLevel ( level )","title":"set_log_level()"},{"location":"code/chain/#taskchain.chain.TaskParameterConfig","text":"Helper config used in parameter mode. It takes instance of Task and create config with only parameters which are used by this task input tasks (already with TaskParameterConfig) From this data creates unique hash used in persistence Note: These configs creates a kind of blockchain. Each config is block, parameters are content of blocks and input_tasks are dependencies between blocks. Change in one config invalidates all dependant configs, which is property desired and required for correct functionality of TaskChains data persistence.","title":"TaskParameterConfig"},{"location":"code/chain/#taskchain.chain.TaskParameterConfig.repr_name","text":"Should be unique representation of this config","title":"repr_name"},{"location":"code/chain/#taskchain.chain.TaskParameterConfig.repr_name_without_namespace","text":"Unique representation of this config without namespace","title":"repr_name_without_namespace"},{"location":"code/chain/#taskchain.chain.TaskParameterConfig.get_name_for_persistence","text":"Used for creating filename in task data persistence, should uniquely define config Source code in taskchain/chain.py def get_name_for_persistence ( self , task : Task ) -> str : def _get_input_task_repr ( _name , _task ): if outer_namespace := task . get_config () . namespace : # remove namespace of this task from task name assert _name . startswith ( outer_namespace ) _name = _name [ len ( outer_namespace ) + 2 :] return f ' { _name } = { _task } ' parameter_repr = task . parameters . repr input_tasks_repr = '###' . join ( _get_input_task_repr ( n , it ) for n , it in sorted ( self . input_tasks . items ())) return sha256 ( f ' { parameter_repr } $$$ { input_tasks_repr } ' . encode ()) . hexdigest ()[: 32 ]","title":"get_name_for_persistence()"},{"location":"code/chain/#taskchain.chain.TaskParameterConfig.get_original_config","text":"Get self of config from which this one is derived Source code in taskchain/chain.py def get_original_config ( self ): return self . original_config","title":"get_original_config()"},{"location":"code/config/","text":"Config Object carrying parameters needed for task execution. Config also describe which tasks configures ( tasks field) and and on which other configs depends ( uses field). Thus, config carry all information needed to assemble task chain. Typical usage: chain = Config ( task_data_dir , 'config.yaml' ) . chain () fullname property readonly Name with namespace repr_name : str property readonly Should be unique representation of this config repr_name_without_namespace : str property readonly Unique representation of this config without namespace __init__ ( self , base_dir = None , filepath = None , global_vars = None , context = None , name = None , namespace = None , data = None , part = None ) special Parameters: Name Type Description Default base_dir Union[Path, str, None] dir with task data, required for task data persistence None filepath Union[Path, str] json or yaml with config data None global_vars Union[Any, None] data to fill placeholders inf config data such as {DATA_DIR} None context Union[None, dict, str, Path, Context, Iterable] config which amend or overwrite data of this config None name str specify name of config directly, required when not using filepath None namespace str used by chains, allow work with same tasks with multiple configs in one chain None data Dict alternative for filepath , inject data directly None part str for multi config files, name of file part None Source code in taskchain/config.py def __init__ ( self , base_dir : Union [ Path , str , None ] = None , filepath : Union [ Path , str ] = None , global_vars : Union [ Any , None ] = None , context : Union [ None , dict , str , Path , Context , Iterable ] = None , name : str = None , namespace : str = None , data : Dict = None , part : str = None , ): \"\"\" Args: base_dir: dir with task data, required for task data persistence filepath: json or yaml with config data global_vars: data to fill placeholders inf config data such as `{DATA_DIR}` context: config which amend or overwrite data of this config name: specify name of config directly, required when not using filepath namespace: used by chains, allow work with same tasks with multiple configs in one chain data: alternative for `filepath`, inject data directly part: for multi config files, name of file part \"\"\" super () . __init__ () self . base_dir = base_dir self . _name = None self . namespace = namespace self . _data = None self . context = Context . prepare_context ( context , global_vars = global_vars ) self . global_vars = global_vars self . _filepath = filepath self . _part = part if filepath is not None : if '#' in str ( self . _filepath ): self . _filepath , self . _part = str ( self . _filepath ) . split ( '#' ) filepath = Path ( self . _filepath ) name_parts = filepath . name . split ( '.' ) extension = name_parts [ - 1 ] self . _name = '.' . join ( name_parts [: - 1 ]) if extension == 'json' : self . _data = json . load ( filepath . open ()) elif extension == 'yaml' : self . _data = yaml . load ( filepath . open (), Loader = yaml . Loader ) else : raise ValueError ( f 'Unknown file extension for config file ` { filepath } `' ) if data is not None : self . _data = data if name is not None : self . _name = name self . _prepare () get_name_for_persistence ( self , * args , ** kwargs ) Used for creating filename in task data persistence, should uniquely define config Source code in taskchain/config.py def get_name_for_persistence ( self , * args , ** kwargs ) -> str : \"\"\" Used for creating filename in task data persistence, should uniquely define config \"\"\" return self . name get ( self , item , default = None ) Return the value for key if key is in the dictionary, else default. Source code in taskchain/config.py def get ( self , item , default = None ): return self . data . get ( item , default ) apply_context ( self , context ) Amend or rewrite data of config by data from context Source code in taskchain/config.py def apply_context ( self , context : Context ): \"\"\" Amend or rewrite data of config by data from context\"\"\" self . _data . update ( deepcopy ( context . data )) if self . namespace : for namespace , data in context . for_namespaces . items (): if self . namespace == namespace : self . _data . update ( deepcopy ( data )) prepare_objects ( self ) Instantiate objects described in config Source code in taskchain/config.py def prepare_objects ( self ): \"\"\" Instantiate objects described in config \"\"\" if self . _data is None : return def _instancelize_clazz ( clazz , args , kwargs ): obj = instantiate_clazz ( clazz , args , kwargs ) if not isinstance ( obj , ParameterObject ): LOGGER . warning ( f 'Object ` { obj } ` in config ` { self } ` is not instance of ParameterObject' ) if not hasattr ( obj , 'repr' ): raise Exception ( f 'Object ` { obj } ` does not implement `repr` property' ) return obj for key , value in self . _data . items (): self . _data [ key ] = find_and_instantiate_clazz ( value , instancelize_clazz_fce = _instancelize_clazz ) chain ( self , parameter_mode = True , ** kwargs ) Create chain from this config Source code in taskchain/config.py def chain ( self , parameter_mode = True , ** kwargs ): \"\"\" Create chain from this config \"\"\" from .chain import Chain return Chain ( self , parameter_mode = parameter_mode , ** kwargs ) get_original_config ( self ) Get self of config from which this one is derived Source code in taskchain/config.py def get_original_config ( self ): \"\"\" Get self of config from which this one is derived \"\"\" return self Context Config intended for amend or rewrite other configs prepare_context ( context_config , namespace = None , global_vars = None ) staticmethod Helper function for instantiating Context from various sources Source code in taskchain/config.py @staticmethod def prepare_context ( context_config : Union [ None , dict , str , Path , Context , Iterable ], namespace = None , global_vars = None ) -> Union [ Context , None ]: \"\"\" Helper function for instantiating Context from various sources\"\"\" context = None if context_config is None : return elif type ( context_config ) is str or isinstance ( context_config , Path ): context = Context ( filepath = context_config , namespace = namespace ) elif type ( context_config ) is dict : value_reprs = [ f ' { k } : { v } ' for k , v in sorted ( context_config . items ())] context = Context ( data = context_config , name = f 'dict_context( { \",\" . join ( value_reprs ) } )' , namespace = namespace ) elif isinstance ( context_config , Context ): context = context_config elif isinstance ( context_config , Iterable ): contexts = map ( partial ( Context . prepare_context , namespace = namespace , global_vars = global_vars ), context_config ) context = Context . merge_contexts ( contexts ) if context is None : raise ValueError ( f 'Unknown context type ` { type ( context_config ) } `' ) current_context_data = context . for_namespaces [ namespace ] if namespace else context if 'uses' not in current_context_data : return context if global_vars is not None : search_and_replace_placeholders ( current_context_data [ 'uses' ], global_vars ) contexts = [ context ] for use in list_or_str_to_list ( current_context_data [ 'uses' ]): if matched := re . match ( r '(.*) as (.*)' , use ): # uses context with namespace filepath = matched [ 1 ] sub_namespace = f ' { context . namespace } :: { matched [ 2 ] } ' if context . namespace else matched [ 2 ] else : filepath = use sub_namespace = context . namespace if context . namespace else None contexts . append ( Context . prepare_context ( filepath , sub_namespace , global_vars = global_vars )) if namespace : del context . for_namespaces [ namespace ][ 'uses' ] else : del context . _data [ 'uses' ] return Context . prepare_context ( contexts ) merge_contexts ( contexts ) staticmethod Helper function for merging multiple Context to one Later contexts have higher priority and rewrite data of earlier contexts if there is conflict in data. Source code in taskchain/config.py @staticmethod def merge_contexts ( contexts : Iterable [ Context ]) -> Context : \"\"\" Helper function for merging multiple Context to one Later contexts have higher priority and rewrite data of earlier contexts if there is conflict in data. \"\"\" data = {} names = [] for_namespaces = defaultdict ( dict ) for context in contexts : data . update ( context . data ) names . append ( context . name ) for namespace , values in context . for_namespaces . items (): for_namespaces [ namespace ] . update ( values ) data [ 'for_namespaces' ] = for_namespaces return Context ( data = data , name = ';' . join ( names ))","title":"Config"},{"location":"code/config/#taskchain.config.Config","text":"Object carrying parameters needed for task execution. Config also describe which tasks configures ( tasks field) and and on which other configs depends ( uses field). Thus, config carry all information needed to assemble task chain. Typical usage: chain = Config ( task_data_dir , 'config.yaml' ) . chain ()","title":"Config"},{"location":"code/config/#taskchain.config.Config.fullname","text":"Name with namespace","title":"fullname"},{"location":"code/config/#taskchain.config.Config.repr_name","text":"Should be unique representation of this config","title":"repr_name"},{"location":"code/config/#taskchain.config.Config.repr_name_without_namespace","text":"Unique representation of this config without namespace","title":"repr_name_without_namespace"},{"location":"code/config/#taskchain.config.Config.__init__","text":"Parameters: Name Type Description Default base_dir Union[Path, str, None] dir with task data, required for task data persistence None filepath Union[Path, str] json or yaml with config data None global_vars Union[Any, None] data to fill placeholders inf config data such as {DATA_DIR} None context Union[None, dict, str, Path, Context, Iterable] config which amend or overwrite data of this config None name str specify name of config directly, required when not using filepath None namespace str used by chains, allow work with same tasks with multiple configs in one chain None data Dict alternative for filepath , inject data directly None part str for multi config files, name of file part None Source code in taskchain/config.py def __init__ ( self , base_dir : Union [ Path , str , None ] = None , filepath : Union [ Path , str ] = None , global_vars : Union [ Any , None ] = None , context : Union [ None , dict , str , Path , Context , Iterable ] = None , name : str = None , namespace : str = None , data : Dict = None , part : str = None , ): \"\"\" Args: base_dir: dir with task data, required for task data persistence filepath: json or yaml with config data global_vars: data to fill placeholders inf config data such as `{DATA_DIR}` context: config which amend or overwrite data of this config name: specify name of config directly, required when not using filepath namespace: used by chains, allow work with same tasks with multiple configs in one chain data: alternative for `filepath`, inject data directly part: for multi config files, name of file part \"\"\" super () . __init__ () self . base_dir = base_dir self . _name = None self . namespace = namespace self . _data = None self . context = Context . prepare_context ( context , global_vars = global_vars ) self . global_vars = global_vars self . _filepath = filepath self . _part = part if filepath is not None : if '#' in str ( self . _filepath ): self . _filepath , self . _part = str ( self . _filepath ) . split ( '#' ) filepath = Path ( self . _filepath ) name_parts = filepath . name . split ( '.' ) extension = name_parts [ - 1 ] self . _name = '.' . join ( name_parts [: - 1 ]) if extension == 'json' : self . _data = json . load ( filepath . open ()) elif extension == 'yaml' : self . _data = yaml . load ( filepath . open (), Loader = yaml . Loader ) else : raise ValueError ( f 'Unknown file extension for config file ` { filepath } `' ) if data is not None : self . _data = data if name is not None : self . _name = name self . _prepare ()","title":"__init__()"},{"location":"code/config/#taskchain.config.Config.get_name_for_persistence","text":"Used for creating filename in task data persistence, should uniquely define config Source code in taskchain/config.py def get_name_for_persistence ( self , * args , ** kwargs ) -> str : \"\"\" Used for creating filename in task data persistence, should uniquely define config \"\"\" return self . name","title":"get_name_for_persistence()"},{"location":"code/config/#taskchain.config.Config.get","text":"Return the value for key if key is in the dictionary, else default. Source code in taskchain/config.py def get ( self , item , default = None ): return self . data . get ( item , default )","title":"get()"},{"location":"code/config/#taskchain.config.Config.apply_context","text":"Amend or rewrite data of config by data from context Source code in taskchain/config.py def apply_context ( self , context : Context ): \"\"\" Amend or rewrite data of config by data from context\"\"\" self . _data . update ( deepcopy ( context . data )) if self . namespace : for namespace , data in context . for_namespaces . items (): if self . namespace == namespace : self . _data . update ( deepcopy ( data ))","title":"apply_context()"},{"location":"code/config/#taskchain.config.Config.prepare_objects","text":"Instantiate objects described in config Source code in taskchain/config.py def prepare_objects ( self ): \"\"\" Instantiate objects described in config \"\"\" if self . _data is None : return def _instancelize_clazz ( clazz , args , kwargs ): obj = instantiate_clazz ( clazz , args , kwargs ) if not isinstance ( obj , ParameterObject ): LOGGER . warning ( f 'Object ` { obj } ` in config ` { self } ` is not instance of ParameterObject' ) if not hasattr ( obj , 'repr' ): raise Exception ( f 'Object ` { obj } ` does not implement `repr` property' ) return obj for key , value in self . _data . items (): self . _data [ key ] = find_and_instantiate_clazz ( value , instancelize_clazz_fce = _instancelize_clazz )","title":"prepare_objects()"},{"location":"code/config/#taskchain.config.Config.chain","text":"Create chain from this config Source code in taskchain/config.py def chain ( self , parameter_mode = True , ** kwargs ): \"\"\" Create chain from this config \"\"\" from .chain import Chain return Chain ( self , parameter_mode = parameter_mode , ** kwargs )","title":"chain()"},{"location":"code/config/#taskchain.config.Config.get_original_config","text":"Get self of config from which this one is derived Source code in taskchain/config.py def get_original_config ( self ): \"\"\" Get self of config from which this one is derived \"\"\" return self","title":"get_original_config()"},{"location":"code/config/#taskchain.config.Context","text":"Config intended for amend or rewrite other configs","title":"Context"},{"location":"code/config/#taskchain.config.Context.prepare_context","text":"Helper function for instantiating Context from various sources Source code in taskchain/config.py @staticmethod def prepare_context ( context_config : Union [ None , dict , str , Path , Context , Iterable ], namespace = None , global_vars = None ) -> Union [ Context , None ]: \"\"\" Helper function for instantiating Context from various sources\"\"\" context = None if context_config is None : return elif type ( context_config ) is str or isinstance ( context_config , Path ): context = Context ( filepath = context_config , namespace = namespace ) elif type ( context_config ) is dict : value_reprs = [ f ' { k } : { v } ' for k , v in sorted ( context_config . items ())] context = Context ( data = context_config , name = f 'dict_context( { \",\" . join ( value_reprs ) } )' , namespace = namespace ) elif isinstance ( context_config , Context ): context = context_config elif isinstance ( context_config , Iterable ): contexts = map ( partial ( Context . prepare_context , namespace = namespace , global_vars = global_vars ), context_config ) context = Context . merge_contexts ( contexts ) if context is None : raise ValueError ( f 'Unknown context type ` { type ( context_config ) } `' ) current_context_data = context . for_namespaces [ namespace ] if namespace else context if 'uses' not in current_context_data : return context if global_vars is not None : search_and_replace_placeholders ( current_context_data [ 'uses' ], global_vars ) contexts = [ context ] for use in list_or_str_to_list ( current_context_data [ 'uses' ]): if matched := re . match ( r '(.*) as (.*)' , use ): # uses context with namespace filepath = matched [ 1 ] sub_namespace = f ' { context . namespace } :: { matched [ 2 ] } ' if context . namespace else matched [ 2 ] else : filepath = use sub_namespace = context . namespace if context . namespace else None contexts . append ( Context . prepare_context ( filepath , sub_namespace , global_vars = global_vars )) if namespace : del context . for_namespaces [ namespace ][ 'uses' ] else : del context . _data [ 'uses' ] return Context . prepare_context ( contexts )","title":"prepare_context()"},{"location":"code/config/#taskchain.config.Context.merge_contexts","text":"Helper function for merging multiple Context to one Later contexts have higher priority and rewrite data of earlier contexts if there is conflict in data. Source code in taskchain/config.py @staticmethod def merge_contexts ( contexts : Iterable [ Context ]) -> Context : \"\"\" Helper function for merging multiple Context to one Later contexts have higher priority and rewrite data of earlier contexts if there is conflict in data. \"\"\" data = {} names = [] for_namespaces = defaultdict ( dict ) for context in contexts : data . update ( context . data ) names . append ( context . name ) for namespace , values in context . for_namespaces . items (): for_namespaces [ namespace ] . update ( values ) data [ 'for_namespaces' ] = for_namespaces return Context ( data = data , name = ';' . join ( names ))","title":"merge_contexts()"},{"location":"code/parameter/","text":"AbstractParameter __init__ ( self , default =< class ' taskchain . parameter . NO_DEFAULT '>, ignore_persistence=False, dont_persist_default_value=True) special Parameters: Name Type Description Default default Any value used if not provided in config, default to NO_DEFAULT meaning that param is required <class 'taskchain.parameter.NO_DEFAULT'> ignore_persistence bool do not use this parameter in persistence, useful params without influence on output False dont_persist_default_value bool if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data True Source code in taskchain/parameter.py def __init__ ( self , default : Any = NO_DEFAULT , ignore_persistence : bool = False , dont_persist_default_value : bool = True , ): \"\"\" Args: default: value used if not provided in config, default to NO_DEFAULT meaning that param is required ignore_persistence: do not use this parameter in persistence, useful params without influence on output dont_persist_default_value: if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data \"\"\" self . default = default self . ignore_persistence = ignore_persistence self . dont_persist_default_value = dont_persist_default_value AutoParameterObject ParameterObject with automatic repr method based on arguments of init method. For correct functionality, is necessary store all init argument values as self.arg_name or self._arg_name repr ( self ) Representation which should uniquely describe object, i.e. be based on all arguments of init . Source code in taskchain/parameter.py def repr ( self ) -> str : parameters = signature ( self . __init__ ) . parameters ignore_persistence_args = self . ignore_persistence_args () dont_persist_default_value_args = self . dont_persist_default_value_args () args = {} for i , ( arg , parameter ) in enumerate ( parameters . items ()): if arg in ignore_persistence_args : continue if hasattr ( self , '_' + arg ): value = getattr ( self , '_' + arg ) elif hasattr ( self , arg ): value = getattr ( self , arg ) else : raise AttributeError ( f 'Value of __init__ argument ` { arg } ` not found for class ` { fullname ( self . __class__ ) } `, ' f 'make sure that value is saved in `self. { arg } ` or `self._ { arg } `' ) args [ arg ] = value if arg in dont_persist_default_value_args and args [ arg ] == parameter . default : del args [ arg ] args_repr = ', ' . join ( f ' { k } = { repr ( v ) } ' for k , v in sorted ( args . items ())) assert 'object at 0x' not in args_repr , f 'repr for arguments is fragile: { args_repr } ' return f ' { self . __class__ . __name__ } ( { args_repr } )' ignore_persistence_args () staticmethod List of init argument names which are ignored in persistence. Source code in taskchain/parameter.py @staticmethod def ignore_persistence_args () -> List [ str ]: \"\"\" List of __init__ argument names which are ignored in persistence. \"\"\" return [ 'verbose' , 'debug' ] dont_persist_default_value_args () staticmethod List of init argument names which are ignored in persistence when they take default value. Source code in taskchain/parameter.py @staticmethod def dont_persist_default_value_args () -> List [ str ]: \"\"\" List of __init__ argument names which are ignored in persistence when they take default value. \"\"\" return [] Parameter __init__ ( self , name , dtype = None , default =< class ' taskchain . parameter . NO_DEFAULT '>, name_in_config=None, ignore_persistence=False, dont_persist_default_value=False) special Parameters: Name Type Description Default name str name for referencing from task required dtype type expected datatype None default Any value used if not provided in config, default to NO_DEFAULT meaning that param is required <class 'taskchain.parameter.NO_DEFAULT'> name_in_config str name used for search in config, defaults to name argument None ignore_persistence bool do not use this parameter in persistence, useful params without influence on output False dont_persist_default_value bool if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data False Source code in taskchain/parameter.py def __init__ ( self , name : str , dtype : Union [ type ] = None , default : Any = NO_DEFAULT , name_in_config : str = None , ignore_persistence : bool = False , dont_persist_default_value : bool = False , ): \"\"\" Args: name: name for referencing from task dtype: expected datatype default: value used if not provided in config, default to NO_DEFAULT meaning that param is required name_in_config: name used for search in config, defaults to `name` argument ignore_persistence: do not use this parameter in persistence, useful params without influence on output dont_persist_default_value: if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data \"\"\" super () . __init__ ( default = default , ignore_persistence = ignore_persistence , dont_persist_default_value = dont_persist_default_value , ) assert name not in taskchain . config . Config . RESERVED_PARAMETER_NAMES self . _name = name self . dtype = dtype self . name_in_config = name if name_in_config is None else name_in_config self . _value = self . NO_VALUE ParameterObject Every class used in configs has to be inherit from this class. repr ( self ) Representation which should uniquely describe object, i.e. be based on all arguments of init . Source code in taskchain/parameter.py @abc . abstractmethod def repr ( self ) -> str : \"\"\" Representation which should uniquely describe object, i.e. be based on all arguments of __init__. \"\"\" raise NotImplemented","title":"Parameter"},{"location":"code/parameter/#taskchain.parameter.AbstractParameter","text":"","title":"AbstractParameter"},{"location":"code/parameter/#taskchain.parameter.AbstractParameter.__init__","text":"Parameters: Name Type Description Default default Any value used if not provided in config, default to NO_DEFAULT meaning that param is required <class 'taskchain.parameter.NO_DEFAULT'> ignore_persistence bool do not use this parameter in persistence, useful params without influence on output False dont_persist_default_value bool if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data True Source code in taskchain/parameter.py def __init__ ( self , default : Any = NO_DEFAULT , ignore_persistence : bool = False , dont_persist_default_value : bool = True , ): \"\"\" Args: default: value used if not provided in config, default to NO_DEFAULT meaning that param is required ignore_persistence: do not use this parameter in persistence, useful params without influence on output dont_persist_default_value: if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data \"\"\" self . default = default self . ignore_persistence = ignore_persistence self . dont_persist_default_value = dont_persist_default_value","title":"__init__()"},{"location":"code/parameter/#taskchain.parameter.AutoParameterObject","text":"ParameterObject with automatic repr method based on arguments of init method. For correct functionality, is necessary store all init argument values as self.arg_name or self._arg_name","title":"AutoParameterObject"},{"location":"code/parameter/#taskchain.parameter.AutoParameterObject.repr","text":"Representation which should uniquely describe object, i.e. be based on all arguments of init . Source code in taskchain/parameter.py def repr ( self ) -> str : parameters = signature ( self . __init__ ) . parameters ignore_persistence_args = self . ignore_persistence_args () dont_persist_default_value_args = self . dont_persist_default_value_args () args = {} for i , ( arg , parameter ) in enumerate ( parameters . items ()): if arg in ignore_persistence_args : continue if hasattr ( self , '_' + arg ): value = getattr ( self , '_' + arg ) elif hasattr ( self , arg ): value = getattr ( self , arg ) else : raise AttributeError ( f 'Value of __init__ argument ` { arg } ` not found for class ` { fullname ( self . __class__ ) } `, ' f 'make sure that value is saved in `self. { arg } ` or `self._ { arg } `' ) args [ arg ] = value if arg in dont_persist_default_value_args and args [ arg ] == parameter . default : del args [ arg ] args_repr = ', ' . join ( f ' { k } = { repr ( v ) } ' for k , v in sorted ( args . items ())) assert 'object at 0x' not in args_repr , f 'repr for arguments is fragile: { args_repr } ' return f ' { self . __class__ . __name__ } ( { args_repr } )'","title":"repr()"},{"location":"code/parameter/#taskchain.parameter.AutoParameterObject.ignore_persistence_args","text":"List of init argument names which are ignored in persistence. Source code in taskchain/parameter.py @staticmethod def ignore_persistence_args () -> List [ str ]: \"\"\" List of __init__ argument names which are ignored in persistence. \"\"\" return [ 'verbose' , 'debug' ]","title":"ignore_persistence_args()"},{"location":"code/parameter/#taskchain.parameter.AutoParameterObject.dont_persist_default_value_args","text":"List of init argument names which are ignored in persistence when they take default value. Source code in taskchain/parameter.py @staticmethod def dont_persist_default_value_args () -> List [ str ]: \"\"\" List of __init__ argument names which are ignored in persistence when they take default value. \"\"\" return []","title":"dont_persist_default_value_args()"},{"location":"code/parameter/#taskchain.parameter.Parameter","text":"","title":"Parameter"},{"location":"code/parameter/#taskchain.parameter.Parameter.__init__","text":"Parameters: Name Type Description Default name str name for referencing from task required dtype type expected datatype None default Any value used if not provided in config, default to NO_DEFAULT meaning that param is required <class 'taskchain.parameter.NO_DEFAULT'> name_in_config str name used for search in config, defaults to name argument None ignore_persistence bool do not use this parameter in persistence, useful params without influence on output False dont_persist_default_value bool if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data False Source code in taskchain/parameter.py def __init__ ( self , name : str , dtype : Union [ type ] = None , default : Any = NO_DEFAULT , name_in_config : str = None , ignore_persistence : bool = False , dont_persist_default_value : bool = False , ): \"\"\" Args: name: name for referencing from task dtype: expected datatype default: value used if not provided in config, default to NO_DEFAULT meaning that param is required name_in_config: name used for search in config, defaults to `name` argument ignore_persistence: do not use this parameter in persistence, useful params without influence on output dont_persist_default_value: if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data \"\"\" super () . __init__ ( default = default , ignore_persistence = ignore_persistence , dont_persist_default_value = dont_persist_default_value , ) assert name not in taskchain . config . Config . RESERVED_PARAMETER_NAMES self . _name = name self . dtype = dtype self . name_in_config = name if name_in_config is None else name_in_config self . _value = self . NO_VALUE","title":"__init__()"},{"location":"code/parameter/#taskchain.parameter.ParameterObject","text":"Every class used in configs has to be inherit from this class.","title":"ParameterObject"},{"location":"code/parameter/#taskchain.parameter.ParameterObject.repr","text":"Representation which should uniquely describe object, i.e. be based on all arguments of init . Source code in taskchain/parameter.py @abc . abstractmethod def repr ( self ) -> str : \"\"\" Representation which should uniquely describe object, i.e. be based on all arguments of __init__. \"\"\" raise NotImplemented","title":"repr()"},{"location":"code/task/","text":"DoubleModuleTask Task which groups are based on python module name (file with the task) and package (dir with that file). Full name of the task: package_name:module_name:task_name InputTasks Registry of input tasks. Main feature of this class is that it allow access task in multiple ways: by full name of the task (including namespace and groups) by shorter name without namespace or groups as long as it is unambiguous by index, order is given by order in Meta ModuleTask Task which group is based on python module name (file with the task) Task Object representing one computation step in chains. data : Data property readonly Get data object of this tasks. This also triggers loading or computation of data same as .value value : Any property readonly Return result of computation. Load persisted data or compute them by run method. path : Path property readonly Path where all data of this task are persisted. has_data : bool property readonly Check if this task has data already computed and persisted. data_path : Path property readonly Path to data. Path can be not existent if data are not yet computed. Returns None if task does not persisting. input_tasks : InputTasks property readonly Get task's registry which allows access input tasks by name or index. name_for_persistence property readonly Get unique string representation of this object used in persistence. This value is provided by config. Returns: Type Description str hash based on input tasks and parameters in parameter mode, name of config otherwise run_info : Dict property readonly Info about last call of run method. log : Union [ NoneType , List [ str ]] property readonly Log (from self.logger ) from last run as list of rows. __init__ ( self , config = None ) special Parameters: Name Type Description Default config Config config with parameters for this task None Source code in taskchain/task.py def __init__ ( self , config : Config = None ): \"\"\" Args: config: config with parameters for this task \"\"\" self . _config : Config = config self . _data : Union [ None , Data , DirData ] = None self . _input_tasks : Union [ None , InputTasks ] = None self . _forced = False self . meta = self . __class__ . meta self . group = self . __class__ . group self . slugname = self . __class__ . slugname self . fullname = self . __class__ . fullname ( config ) self . data_class = self . __class__ . data_class self . data_type = self . __class__ . data_type self . logger = logging . getLogger ( f 'task_ { self . fullname } ' ) self . logger . setLevel ( logging . DEBUG ) self . _prepare_parameters () run ( self , * args ) Abstract method which is called by a chain when data are needed. This method represents computation. Parameters: Name Type Description Default *args can be names of parameters and input tasks. () Source code in taskchain/task.py @abc . abstractmethod def run ( self , * args ): \"\"\" Abstract method which is called by a chain when data are needed. This method represents computation. Args: *args: can be names of parameters and input tasks. Their values are then provided by the chain. \"\"\" pass get_data_object ( self ) This is meant to be run only from run method. Needed when task return Data object directly, e.g. DirData or ContinuesData. Returns: Type Description Data object handling data persistence of this task. Source code in taskchain/task.py def get_data_object ( self ): \"\"\" This is meant to be run only from `run` method. Needed when task return Data object directly, e.g. DirData or ContinuesData. Returns: Data: object handling data persistence of this task. \"\"\" if not hasattr ( self , '_data' ): raise ValueError ( 'Data object is not initialized, run this only from task' ) return self . _data get_config ( self ) Return config used to configure this task. Source code in taskchain/task.py def get_config ( self ): \"\"\" Return config used to configure this task. \"\"\" return self . _config force ( self , delete_data = False ) Switch task to forced state to allow data recomputation. Next time value is requested persisted data are ignored and computation is triggered. Parameters: Name Type Description Default delete_data bool whether persisted data should be immediately deleted from disk. False Returns: Type Description Task allows chaining task.force().value Source code in taskchain/task.py def force ( self , delete_data = False ): \"\"\" Switch task to forced state to allow data recomputation. Next time value is requested persisted data are ignored and computation is triggered. Args: delete_data (bool): whether persisted data should be immediately deleted from disk. Returns: Task: allows chaining `task.force().value` \"\"\" if delete_data : data = self . _data_without_value if data . exists (): data . delete () self . _forced = True self . _data = None return self save_to_run_info ( self , record ) Save information to run info. Should be called from run method. Parameters: Name Type Description Default record any json-like object required Source code in taskchain/task.py def save_to_run_info ( self , record ): \"\"\" Save information to run info. Should be called from `run` method. Args: record: any json-like object \"\"\" if isinstance ( record , defaultdict ): record = dict ( record ) self . _run_info [ 'log' ] . append ( record )","title":"Task"},{"location":"code/task/#taskchain.task.DoubleModuleTask","text":"Task which groups are based on python module name (file with the task) and package (dir with that file). Full name of the task: package_name:module_name:task_name","title":"DoubleModuleTask"},{"location":"code/task/#taskchain.task.InputTasks","text":"Registry of input tasks. Main feature of this class is that it allow access task in multiple ways: by full name of the task (including namespace and groups) by shorter name without namespace or groups as long as it is unambiguous by index, order is given by order in Meta","title":"InputTasks"},{"location":"code/task/#taskchain.task.ModuleTask","text":"Task which group is based on python module name (file with the task)","title":"ModuleTask"},{"location":"code/task/#taskchain.task.Task","text":"Object representing one computation step in chains.","title":"Task"},{"location":"code/task/#taskchain.task.Task.data","text":"Get data object of this tasks. This also triggers loading or computation of data same as .value","title":"data"},{"location":"code/task/#taskchain.task.Task.value","text":"Return result of computation. Load persisted data or compute them by run method.","title":"value"},{"location":"code/task/#taskchain.task.Task.path","text":"Path where all data of this task are persisted.","title":"path"},{"location":"code/task/#taskchain.task.Task.has_data","text":"Check if this task has data already computed and persisted.","title":"has_data"},{"location":"code/task/#taskchain.task.Task.data_path","text":"Path to data. Path can be not existent if data are not yet computed. Returns None if task does not persisting.","title":"data_path"},{"location":"code/task/#taskchain.task.Task.input_tasks","text":"Get task's registry which allows access input tasks by name or index.","title":"input_tasks"},{"location":"code/task/#taskchain.task.Task.name_for_persistence","text":"Get unique string representation of this object used in persistence. This value is provided by config. Returns: Type Description str hash based on input tasks and parameters in parameter mode, name of config otherwise","title":"name_for_persistence"},{"location":"code/task/#taskchain.task.Task.run_info","text":"Info about last call of run method.","title":"run_info"},{"location":"code/task/#taskchain.task.Task.log","text":"Log (from self.logger ) from last run as list of rows.","title":"log"},{"location":"code/task/#taskchain.task.Task.__init__","text":"Parameters: Name Type Description Default config Config config with parameters for this task None Source code in taskchain/task.py def __init__ ( self , config : Config = None ): \"\"\" Args: config: config with parameters for this task \"\"\" self . _config : Config = config self . _data : Union [ None , Data , DirData ] = None self . _input_tasks : Union [ None , InputTasks ] = None self . _forced = False self . meta = self . __class__ . meta self . group = self . __class__ . group self . slugname = self . __class__ . slugname self . fullname = self . __class__ . fullname ( config ) self . data_class = self . __class__ . data_class self . data_type = self . __class__ . data_type self . logger = logging . getLogger ( f 'task_ { self . fullname } ' ) self . logger . setLevel ( logging . DEBUG ) self . _prepare_parameters ()","title":"__init__()"},{"location":"code/task/#taskchain.task.Task.run","text":"Abstract method which is called by a chain when data are needed. This method represents computation. Parameters: Name Type Description Default *args can be names of parameters and input tasks. () Source code in taskchain/task.py @abc . abstractmethod def run ( self , * args ): \"\"\" Abstract method which is called by a chain when data are needed. This method represents computation. Args: *args: can be names of parameters and input tasks. Their values are then provided by the chain. \"\"\" pass","title":"run()"},{"location":"code/task/#taskchain.task.Task.get_data_object","text":"This is meant to be run only from run method. Needed when task return Data object directly, e.g. DirData or ContinuesData. Returns: Type Description Data object handling data persistence of this task. Source code in taskchain/task.py def get_data_object ( self ): \"\"\" This is meant to be run only from `run` method. Needed when task return Data object directly, e.g. DirData or ContinuesData. Returns: Data: object handling data persistence of this task. \"\"\" if not hasattr ( self , '_data' ): raise ValueError ( 'Data object is not initialized, run this only from task' ) return self . _data","title":"get_data_object()"},{"location":"code/task/#taskchain.task.Task.get_config","text":"Return config used to configure this task. Source code in taskchain/task.py def get_config ( self ): \"\"\" Return config used to configure this task. \"\"\" return self . _config","title":"get_config()"},{"location":"code/task/#taskchain.task.Task.force","text":"Switch task to forced state to allow data recomputation. Next time value is requested persisted data are ignored and computation is triggered. Parameters: Name Type Description Default delete_data bool whether persisted data should be immediately deleted from disk. False Returns: Type Description Task allows chaining task.force().value Source code in taskchain/task.py def force ( self , delete_data = False ): \"\"\" Switch task to forced state to allow data recomputation. Next time value is requested persisted data are ignored and computation is triggered. Args: delete_data (bool): whether persisted data should be immediately deleted from disk. Returns: Task: allows chaining `task.force().value` \"\"\" if delete_data : data = self . _data_without_value if data . exists (): data . delete () self . _forced = True self . _data = None return self","title":"force()"},{"location":"code/task/#taskchain.task.Task.save_to_run_info","text":"Save information to run info. Should be called from run method. Parameters: Name Type Description Default record any json-like object required Source code in taskchain/task.py def save_to_run_info ( self , record ): \"\"\" Save information to run info. Should be called from `run` method. Args: record: any json-like object \"\"\" if isinstance ( record , defaultdict ): record = dict ( record ) self . _run_info [ 'log' ] . append ( record )","title":"save_to_run_info()"},{"location":"code/testing/","text":"MockTask value : Any property readonly Return result of computation. Load persisted data or compute them by run method. TestChain __init__ ( self , tasks , mock_tasks = None , parameters = None , base_dir = None ) special Helper class for testing part of a chain. Some tasks are present fully, some are mocked. Config is not needed, parameters are provided directly. Parameters: Name Type Description Default tasks List[Type[taskchain.task.Task]] list of task classes which should be part of the chain required mock_tasks Dict[Union[str, Type[taskchain.task.Task]], Any] other task which are needed (as input_tasks) in chain but their output is only mocked (by values of this dict) None parameters Dict[str, Any] parameter names and their values None base_dir Path path for data persistence, if None tmp dir is created None Source code in taskchain/utils/testing.py def __init__ ( self , tasks : List [ Type [ Task ]], mock_tasks : Dict [ Union [ str , Type [ Task ]], Any ] = None , parameters : Dict [ str , Any ] = None , base_dir : Path = None , ): \"\"\" Helper class for testing part of a chain. Some tasks are present fully, some are mocked. Config is not needed, parameters are provided directly. Args: tasks: list of task classes which should be part of the chain mock_tasks: other task which are needed (as input_tasks) in chain but their output is only mocked (by values of this dict) parameters: parameter names and their values base_dir: path for data persistence, if None tmp dir is created \"\"\" self . _tasks = tasks self . _mock_tasks = mock_tasks or {} if base_dir is None : base_dir = Path ( tempfile . TemporaryDirectory () . name ) if parameters is None : parameters = {} self . config = Config ( base_dir , name = 'test' , data = parameters ) super () . __init__ ( self . config ) create_test_task ( task , input_tasks = None , parameters = None , base_dir = None ) Helper function which instantiate task in such way, that parameters and input_tasks are given with arguments of this function. Parameters: Name Type Description Default input_tasks Dict[Union[str, Type[taskchain.task.Task]], Any] mocked values of input_tasks of tested class None parameters Dict[str, Any] parameter names and their values None base_dir Path path for data persistence, if None tmp dir is created None Source code in taskchain/utils/testing.py def create_test_task ( task : Type [ Task ], input_tasks : Dict [ Union [ str , Type [ Task ]], Any ] = None , parameters : Dict [ str , Any ] = None , base_dir : Path = None , ) -> Task : \"\"\" Helper function which instantiate task in such way, that parameters and input_tasks are given with arguments of this function. Args: tasks class of tested tasks input_tasks: mocked values of input_tasks of tested class parameters: parameter names and their values base_dir: path for data persistence, if None tmp dir is created \"\"\" test_chain = TestChain ([ task ], parameters = parameters , mock_tasks = input_tasks , base_dir = base_dir ) return test_chain [ task . fullname ( test_chain . config )]","title":"Testing"},{"location":"code/testing/#taskchain.utils.testing.MockTask","text":"","title":"MockTask"},{"location":"code/testing/#taskchain.utils.testing.MockTask.value","text":"Return result of computation. Load persisted data or compute them by run method.","title":"value"},{"location":"code/testing/#taskchain.utils.testing.TestChain","text":"","title":"TestChain"},{"location":"code/testing/#taskchain.utils.testing.TestChain.__init__","text":"Helper class for testing part of a chain. Some tasks are present fully, some are mocked. Config is not needed, parameters are provided directly. Parameters: Name Type Description Default tasks List[Type[taskchain.task.Task]] list of task classes which should be part of the chain required mock_tasks Dict[Union[str, Type[taskchain.task.Task]], Any] other task which are needed (as input_tasks) in chain but their output is only mocked (by values of this dict) None parameters Dict[str, Any] parameter names and their values None base_dir Path path for data persistence, if None tmp dir is created None Source code in taskchain/utils/testing.py def __init__ ( self , tasks : List [ Type [ Task ]], mock_tasks : Dict [ Union [ str , Type [ Task ]], Any ] = None , parameters : Dict [ str , Any ] = None , base_dir : Path = None , ): \"\"\" Helper class for testing part of a chain. Some tasks are present fully, some are mocked. Config is not needed, parameters are provided directly. Args: tasks: list of task classes which should be part of the chain mock_tasks: other task which are needed (as input_tasks) in chain but their output is only mocked (by values of this dict) parameters: parameter names and their values base_dir: path for data persistence, if None tmp dir is created \"\"\" self . _tasks = tasks self . _mock_tasks = mock_tasks or {} if base_dir is None : base_dir = Path ( tempfile . TemporaryDirectory () . name ) if parameters is None : parameters = {} self . config = Config ( base_dir , name = 'test' , data = parameters ) super () . __init__ ( self . config )","title":"__init__()"},{"location":"code/testing/#taskchain.utils.testing.create_test_task","text":"Helper function which instantiate task in such way, that parameters and input_tasks are given with arguments of this function. Parameters: Name Type Description Default input_tasks Dict[Union[str, Type[taskchain.task.Task]], Any] mocked values of input_tasks of tested class None parameters Dict[str, Any] parameter names and their values None base_dir Path path for data persistence, if None tmp dir is created None Source code in taskchain/utils/testing.py def create_test_task ( task : Type [ Task ], input_tasks : Dict [ Union [ str , Type [ Task ]], Any ] = None , parameters : Dict [ str , Any ] = None , base_dir : Path = None , ) -> Task : \"\"\" Helper function which instantiate task in such way, that parameters and input_tasks are given with arguments of this function. Args: tasks class of tested tasks input_tasks: mocked values of input_tasks of tested class parameters: parameter names and their values base_dir: path for data persistence, if None tmp dir is created \"\"\" test_chain = TestChain ([ task ], parameters = parameters , mock_tasks = input_tasks , base_dir = base_dir ) return test_chain [ task . fullname ( test_chain . config )]","title":"create_test_task()"},{"location":"code/utils/","text":"taskchain.cache Cache Cache interface. get_or_compute ( self , key , computer , force = False ) Get value for given key if cached or compute and cache it. Parameters: Name Type Description Default key str key under which value is cached required computer Callable function which returns value if not cached required force bool recompute value even if it is in cache False Returns: Type Description Any cached or computed value Source code in taskchain/cache.py @abc . abstractmethod def get_or_compute ( self , key : str , computer : Callable , force : bool = False ) -> Any : \"\"\" Get value for given key if cached or compute and cache it. Args: key: key under which value is cached computer: function which returns value if not cached force: recompute value even if it is in cache Returns: cached or computed value \"\"\" pass subcache ( self , * args ) Create separate sub-cache of this cache. Source code in taskchain/cache.py @abc . abstractmethod def subcache ( self , * args ) -> 'Cache' : \"\"\" Create separate sub-cache of this cache. \"\"\" pass DataFrameCache Cache pandas DataFrame objects in .pd files. DummyCache No caching. FileCache General cache for saving values in files. InMemoryCache Cache only in memory. JsonCache Cache json-like objects in .json files. NumpyArrayCache Cache numpy arrays in .npy files. cached Decorator for automatic caching of method results. Decorated method is for given arguments called only once a result is cached. Cache key is automatically constructed based on method arguments. Cache can be defined in decorator or as attribute of object. __init__ ( self , cache_object = None , key = None , cache_attr = 'cache' , ignore_kwargs = None ) special Parameters: Name Type Description Default cache_object Cache Cache used for caching. None key Callable custom function for computing key from arguments None cache_attr str if cache_object is None, object attribute with this name is used 'cache' ignore_kwargs List[str] kwargs to ignore in key construction, e.g. verbose None Source code in taskchain/cache.py def __init__ ( self , cache_object : Cache = None , key : Callable = None , cache_attr : str = 'cache' , ignore_kwargs : List [ str ] = None ): \"\"\" Args: cache_object: Cache used for caching. key: custom function for computing key from arguments cache_attr: if `cache_object` is None, object attribute with this name is used ignore_kwargs: kwargs to ignore in key construction, e.g. `verbose` \"\"\" if callable ( cache_object ): self . method = cache_object cache_object = None self . cache_object = cache_object self . key = key self . cache_attr = cache_attr self . ignore_params = ignore_kwargs if ignore_kwargs else [] taskchain.utils.clazz persistent Method decorator. Has to be used on decorator without arguments. Saves result in self.__method_name and next time does not call decorated method and only return saved value. repeat_on_error Method decorator which calls method again on error. __init__ ( self , retries = 10 , waiting_time = 1 , wait_extension = 1.0 ) special Parameters: Name Type Description Default retries int how many times try to call again 10 waiting_time int how many seconds wait before first retry 1 wait_extension float how many times increase waiting time after each retry 1.0 Source code in taskchain/utils/clazz.py def __init__ ( self , retries : int = 10 , waiting_time : int = 1 , wait_extension : float = 1. ): \"\"\" Args: retries: how many times try to call again waiting_time: how many seconds wait before first retry wait_extension: how many times increase waiting time after each retry \"\"\" if callable ( retries ): self . method = retries retries = 10 self . retries = retries self . waiting_time = waiting_time self . wait_extension = wait_extension taskchain.utils.io write_jsons ( jsons , filename , use_tqdm = True , overwrite = True , nan_to_null = True , ** kwargs ) Write json-like object to .jsonl file (json lines). Parameters: Name Type Description Default jsons Iterable Iterable of json-like objects. required filename Path | str required use_tqdm bool Show progress bar. True overwrite bool Overwrite existing file. True nan_to_null bool Change nan values to nulls. True **kwargs other arguments to tqdm. {} Source code in taskchain/utils/io.py def write_jsons ( jsons , filename , use_tqdm = True , overwrite = True , nan_to_null = True , ** kwargs ): \"\"\" Write json-like object to `.jsonl` file (json lines). Args: jsons (Iterable): Iterable of json-like objects. filename (Path | str): use_tqdm (bool): Show progress bar. overwrite (bool): Overwrite existing file. nan_to_null (bool): Change nan values to nulls. **kwargs: other arguments to tqdm. \"\"\" filename = Path ( filename ) assert not filename . exists () or overwrite , 'File already exists' with filename . open ( 'w' ) as f : for j in progress_bar ( jsons , disable = not use_tqdm , desc = f 'Writing to { f . name } ' , ** kwargs ): f . write ( json . dumps ( j , ignore_nan = nan_to_null , cls = NumpyEncoder , ensure_ascii = False ) + ' \\n ' ) iter_json_file ( filename , use_tqdm = True , ** kwargs ) Yield loaded jsons from .jsonl file (json lines). Parameters: Name Type Description Default filename Path | str required use_tqdm bool True **kwargs additional arguments to tqdm {} Source code in taskchain/utils/io.py def iter_json_file ( filename , use_tqdm = True , ** kwargs ): \"\"\" Yield loaded jsons from `.jsonl` file (json lines). Args: filename (Path | str): use_tqdm (bool): **kwargs: additional arguments to tqdm Returns: \"\"\" filename = Path ( filename ) with filename . open () as f : for row in progress_bar ( f , disable = not use_tqdm , desc = f 'Reading from { f . name } ' , ** kwargs ): yield json . loads ( row . strip ()) taskchain.utils.iter list_or_str_to_list ( value ) Helper function for cases where list of string is expected but single string is also ok. Parameters: Name Type Description Default value Union[NoneType, List[str], str] required Returns: Type Description List[str] original list or original string in list Source code in taskchain/utils/iter.py def list_or_str_to_list ( value : Union [ None , List [ str ], str ]) -> List [ str ]: \"\"\" Helper function for cases where list of string is expected but single string is also ok. Args: value: Returns: original list or original string in list \"\"\" if isinstance ( value , str ): return [ value ] return value taskchain.utils.migration migrate_to_parameter_mode ( config , target_dir , dry = True , verbose = True ) Migrate a chain to parameter mode. Parameters: Name Type Description Default config Config config defining the chain required target_dir dir to migrate data to required dry bool show only info, do not copy data True verbose bool True Source code in taskchain/utils/migration.py def migrate_to_parameter_mode ( config : Config , target_dir , dry : bool = True , verbose : bool = True ): \"\"\" Migrate a chain to parameter mode. Args: config: config defining the chain target_dir: dir to migrate data to dry: show only info, do not copy data verbose: \"\"\" assert config . base_dir != target_dir , 'target_dir has to be different from configs base_dir' old_chain = { t . fullname : t for t in config . chain ( parameter_mode = False ) . tasks . values () } new_chain = { t . fullname : t for t in Config ( target_dir , config . _filepath , global_vars = config . global_vars , context = config . context ) . chain () . tasks . values () } print ( f 'Set dry=False to make copies' ) for name , old_task in old_chain . items (): print () new_task = new_chain [ name ] print ( f ' { name } - { new_task . name_for_persistence } ' ) if verbose : print ( f ' parameters: ` { new_task . params . repr } `' ) print ( f ' input tasks: ` { \"###\" . join ( f \" { n } = { it } \" for n , it in sorted ( new_task . get_config () . input_tasks . items ())) } `' ) if issubclass ( old_task . data_class , InMemoryData ): print ( ' not persisting' ) continue if not old_task . has_data : print ( ' no data found' ) continue print ( f ' \\n original: ` { old_task . data_path } `' ) print ( f ' target: ` { new_task . data_path } `' ) if new_task . has_data : # HACK: pd files do not have to have the same size with the same data if new_task . data_path . name . endswith ( '.pd' ): assert isclose ( new_task . data_path . stat () . st_size , old_task . data_path . stat () . st_size , rel_tol = 2e-7 , abs_tol = 10 ), f ' { new_task . data_path . stat () . st_size } vs. { old_task . data_path . stat () . st_size } ' else : assert new_task . data_path . stat () . st_size == old_task . data_path . stat () . st_size print ( f ' target already exists' ) continue if dry : print ( ' to copy' ) else : print ( ' copying' ) if old_task . data_path . is_file (): copyfile ( old_task . data_path , new_task . data_path ) else : copytree ( old_task . data_path , new_task . data_path ) print ( ' copied' ) taskchain.utils.threading parallel_map ( fun , iterable , threads = 10 , sort = True , use_tqdm = True , desc = 'Running tasks in parallel.' , total = None ) Map function to iterable in multiple threads. Parameters: Name Type Description Default fun Callable function to apply required iterable Iterable required threads int number of threads 10 sort bool return values in same order as itarable True use_tqdm bool show progressbar True desc str text of progressbar 'Running tasks in parallel.' total int size of iterable to allow show better progressbar None Returns: Type Description list of returned values by fce Source code in taskchain/utils/threading.py def parallel_map ( fun : Callable , iterable : Iterable , threads : int = 10 , sort : bool = True , use_tqdm : bool = True , desc : str = 'Running tasks in parallel.' , total : int = None ): \"\"\" Map function to iterable in multiple threads. Args: fun: function to apply iterable: threads: number of threads sort: return values in same order as itarable use_tqdm: show progressbar desc: text of progressbar total: size of iterable to allow show better progressbar Returns: list: of returned values by fce \"\"\" def _fun ( i , arg ): return i , fun ( arg ) async def _run (): with concurrent . futures . ThreadPoolExecutor ( max_workers = threads ) as executor : loop = asyncio . get_event_loop () futures = [ loop . run_in_executor ( executor , _fun , i , input_value ) for i , input_value in enumerate ( iterable ) ] return [ await output_value for output_value in progress_bar ( asyncio . as_completed ( futures ), use_tqdm = use_tqdm , desc = desc , total = total )] loop = asyncio . get_event_loop () result = loop . run_until_complete ( _run ()) return [ res for _ , res in ( sorted ( result , key = lambda ires : ires [ 0 ]) if sort else result ) ] parallel_starmap ( fun , iterable , ** kwargs ) Allows use parallel_map for function with multiple arguments. Parameters: Name Type Description Default fun Callable function with multiple arguments required iterable Iterable lists or tuples of arguments required Source code in taskchain/utils/threading.py def parallel_starmap ( fun : Callable , iterable : Iterable , ** kwargs ): \"\"\" Allows use `parallel_map` for function with multiple arguments. Args: fun: function with multiple arguments iterable: lists or tuples of arguments \"\"\" def _call ( d ): return fun ( * d ) return parallel_map ( _call , iterable , ** kwargs )","title":"Utils"},{"location":"code/utils/#taskchain.cache","text":"","title":"cache"},{"location":"code/utils/#taskchain.cache.Cache","text":"Cache interface.","title":"Cache"},{"location":"code/utils/#taskchain.cache.Cache.get_or_compute","text":"Get value for given key if cached or compute and cache it. Parameters: Name Type Description Default key str key under which value is cached required computer Callable function which returns value if not cached required force bool recompute value even if it is in cache False Returns: Type Description Any cached or computed value Source code in taskchain/cache.py @abc . abstractmethod def get_or_compute ( self , key : str , computer : Callable , force : bool = False ) -> Any : \"\"\" Get value for given key if cached or compute and cache it. Args: key: key under which value is cached computer: function which returns value if not cached force: recompute value even if it is in cache Returns: cached or computed value \"\"\" pass","title":"get_or_compute()"},{"location":"code/utils/#taskchain.cache.Cache.subcache","text":"Create separate sub-cache of this cache. Source code in taskchain/cache.py @abc . abstractmethod def subcache ( self , * args ) -> 'Cache' : \"\"\" Create separate sub-cache of this cache. \"\"\" pass","title":"subcache()"},{"location":"code/utils/#taskchain.cache.DataFrameCache","text":"Cache pandas DataFrame objects in .pd files.","title":"DataFrameCache"},{"location":"code/utils/#taskchain.cache.DummyCache","text":"No caching.","title":"DummyCache"},{"location":"code/utils/#taskchain.cache.FileCache","text":"General cache for saving values in files.","title":"FileCache"},{"location":"code/utils/#taskchain.cache.InMemoryCache","text":"Cache only in memory.","title":"InMemoryCache"},{"location":"code/utils/#taskchain.cache.JsonCache","text":"Cache json-like objects in .json files.","title":"JsonCache"},{"location":"code/utils/#taskchain.cache.NumpyArrayCache","text":"Cache numpy arrays in .npy files.","title":"NumpyArrayCache"},{"location":"code/utils/#taskchain.cache.cached","text":"Decorator for automatic caching of method results. Decorated method is for given arguments called only once a result is cached. Cache key is automatically constructed based on method arguments. Cache can be defined in decorator or as attribute of object.","title":"cached"},{"location":"code/utils/#taskchain.cache.cached.__init__","text":"Parameters: Name Type Description Default cache_object Cache Cache used for caching. None key Callable custom function for computing key from arguments None cache_attr str if cache_object is None, object attribute with this name is used 'cache' ignore_kwargs List[str] kwargs to ignore in key construction, e.g. verbose None Source code in taskchain/cache.py def __init__ ( self , cache_object : Cache = None , key : Callable = None , cache_attr : str = 'cache' , ignore_kwargs : List [ str ] = None ): \"\"\" Args: cache_object: Cache used for caching. key: custom function for computing key from arguments cache_attr: if `cache_object` is None, object attribute with this name is used ignore_kwargs: kwargs to ignore in key construction, e.g. `verbose` \"\"\" if callable ( cache_object ): self . method = cache_object cache_object = None self . cache_object = cache_object self . key = key self . cache_attr = cache_attr self . ignore_params = ignore_kwargs if ignore_kwargs else []","title":"__init__()"},{"location":"code/utils/#taskchain.utils.clazz","text":"","title":"clazz"},{"location":"code/utils/#taskchain.utils.clazz.persistent","text":"Method decorator. Has to be used on decorator without arguments. Saves result in self.__method_name and next time does not call decorated method and only return saved value.","title":"persistent"},{"location":"code/utils/#taskchain.utils.clazz.repeat_on_error","text":"Method decorator which calls method again on error.","title":"repeat_on_error"},{"location":"code/utils/#taskchain.utils.clazz.repeat_on_error.__init__","text":"Parameters: Name Type Description Default retries int how many times try to call again 10 waiting_time int how many seconds wait before first retry 1 wait_extension float how many times increase waiting time after each retry 1.0 Source code in taskchain/utils/clazz.py def __init__ ( self , retries : int = 10 , waiting_time : int = 1 , wait_extension : float = 1. ): \"\"\" Args: retries: how many times try to call again waiting_time: how many seconds wait before first retry wait_extension: how many times increase waiting time after each retry \"\"\" if callable ( retries ): self . method = retries retries = 10 self . retries = retries self . waiting_time = waiting_time self . wait_extension = wait_extension","title":"__init__()"},{"location":"code/utils/#taskchain.utils.io","text":"","title":"io"},{"location":"code/utils/#taskchain.utils.io.write_jsons","text":"Write json-like object to .jsonl file (json lines). Parameters: Name Type Description Default jsons Iterable Iterable of json-like objects. required filename Path | str required use_tqdm bool Show progress bar. True overwrite bool Overwrite existing file. True nan_to_null bool Change nan values to nulls. True **kwargs other arguments to tqdm. {} Source code in taskchain/utils/io.py def write_jsons ( jsons , filename , use_tqdm = True , overwrite = True , nan_to_null = True , ** kwargs ): \"\"\" Write json-like object to `.jsonl` file (json lines). Args: jsons (Iterable): Iterable of json-like objects. filename (Path | str): use_tqdm (bool): Show progress bar. overwrite (bool): Overwrite existing file. nan_to_null (bool): Change nan values to nulls. **kwargs: other arguments to tqdm. \"\"\" filename = Path ( filename ) assert not filename . exists () or overwrite , 'File already exists' with filename . open ( 'w' ) as f : for j in progress_bar ( jsons , disable = not use_tqdm , desc = f 'Writing to { f . name } ' , ** kwargs ): f . write ( json . dumps ( j , ignore_nan = nan_to_null , cls = NumpyEncoder , ensure_ascii = False ) + ' \\n ' )","title":"write_jsons()"},{"location":"code/utils/#taskchain.utils.io.iter_json_file","text":"Yield loaded jsons from .jsonl file (json lines). Parameters: Name Type Description Default filename Path | str required use_tqdm bool True **kwargs additional arguments to tqdm {} Source code in taskchain/utils/io.py def iter_json_file ( filename , use_tqdm = True , ** kwargs ): \"\"\" Yield loaded jsons from `.jsonl` file (json lines). Args: filename (Path | str): use_tqdm (bool): **kwargs: additional arguments to tqdm Returns: \"\"\" filename = Path ( filename ) with filename . open () as f : for row in progress_bar ( f , disable = not use_tqdm , desc = f 'Reading from { f . name } ' , ** kwargs ): yield json . loads ( row . strip ())","title":"iter_json_file()"},{"location":"code/utils/#taskchain.utils.iter","text":"","title":"iter"},{"location":"code/utils/#taskchain.utils.iter.list_or_str_to_list","text":"Helper function for cases where list of string is expected but single string is also ok. Parameters: Name Type Description Default value Union[NoneType, List[str], str] required Returns: Type Description List[str] original list or original string in list Source code in taskchain/utils/iter.py def list_or_str_to_list ( value : Union [ None , List [ str ], str ]) -> List [ str ]: \"\"\" Helper function for cases where list of string is expected but single string is also ok. Args: value: Returns: original list or original string in list \"\"\" if isinstance ( value , str ): return [ value ] return value","title":"list_or_str_to_list()"},{"location":"code/utils/#taskchain.utils.migration","text":"","title":"migration"},{"location":"code/utils/#taskchain.utils.migration.migrate_to_parameter_mode","text":"Migrate a chain to parameter mode. Parameters: Name Type Description Default config Config config defining the chain required target_dir dir to migrate data to required dry bool show only info, do not copy data True verbose bool True Source code in taskchain/utils/migration.py def migrate_to_parameter_mode ( config : Config , target_dir , dry : bool = True , verbose : bool = True ): \"\"\" Migrate a chain to parameter mode. Args: config: config defining the chain target_dir: dir to migrate data to dry: show only info, do not copy data verbose: \"\"\" assert config . base_dir != target_dir , 'target_dir has to be different from configs base_dir' old_chain = { t . fullname : t for t in config . chain ( parameter_mode = False ) . tasks . values () } new_chain = { t . fullname : t for t in Config ( target_dir , config . _filepath , global_vars = config . global_vars , context = config . context ) . chain () . tasks . values () } print ( f 'Set dry=False to make copies' ) for name , old_task in old_chain . items (): print () new_task = new_chain [ name ] print ( f ' { name } - { new_task . name_for_persistence } ' ) if verbose : print ( f ' parameters: ` { new_task . params . repr } `' ) print ( f ' input tasks: ` { \"###\" . join ( f \" { n } = { it } \" for n , it in sorted ( new_task . get_config () . input_tasks . items ())) } `' ) if issubclass ( old_task . data_class , InMemoryData ): print ( ' not persisting' ) continue if not old_task . has_data : print ( ' no data found' ) continue print ( f ' \\n original: ` { old_task . data_path } `' ) print ( f ' target: ` { new_task . data_path } `' ) if new_task . has_data : # HACK: pd files do not have to have the same size with the same data if new_task . data_path . name . endswith ( '.pd' ): assert isclose ( new_task . data_path . stat () . st_size , old_task . data_path . stat () . st_size , rel_tol = 2e-7 , abs_tol = 10 ), f ' { new_task . data_path . stat () . st_size } vs. { old_task . data_path . stat () . st_size } ' else : assert new_task . data_path . stat () . st_size == old_task . data_path . stat () . st_size print ( f ' target already exists' ) continue if dry : print ( ' to copy' ) else : print ( ' copying' ) if old_task . data_path . is_file (): copyfile ( old_task . data_path , new_task . data_path ) else : copytree ( old_task . data_path , new_task . data_path ) print ( ' copied' )","title":"migrate_to_parameter_mode()"},{"location":"code/utils/#taskchain.utils.threading","text":"","title":"threading"},{"location":"code/utils/#taskchain.utils.threading.parallel_map","text":"Map function to iterable in multiple threads. Parameters: Name Type Description Default fun Callable function to apply required iterable Iterable required threads int number of threads 10 sort bool return values in same order as itarable True use_tqdm bool show progressbar True desc str text of progressbar 'Running tasks in parallel.' total int size of iterable to allow show better progressbar None Returns: Type Description list of returned values by fce Source code in taskchain/utils/threading.py def parallel_map ( fun : Callable , iterable : Iterable , threads : int = 10 , sort : bool = True , use_tqdm : bool = True , desc : str = 'Running tasks in parallel.' , total : int = None ): \"\"\" Map function to iterable in multiple threads. Args: fun: function to apply iterable: threads: number of threads sort: return values in same order as itarable use_tqdm: show progressbar desc: text of progressbar total: size of iterable to allow show better progressbar Returns: list: of returned values by fce \"\"\" def _fun ( i , arg ): return i , fun ( arg ) async def _run (): with concurrent . futures . ThreadPoolExecutor ( max_workers = threads ) as executor : loop = asyncio . get_event_loop () futures = [ loop . run_in_executor ( executor , _fun , i , input_value ) for i , input_value in enumerate ( iterable ) ] return [ await output_value for output_value in progress_bar ( asyncio . as_completed ( futures ), use_tqdm = use_tqdm , desc = desc , total = total )] loop = asyncio . get_event_loop () result = loop . run_until_complete ( _run ()) return [ res for _ , res in ( sorted ( result , key = lambda ires : ires [ 0 ]) if sort else result ) ]","title":"parallel_map()"},{"location":"code/utils/#taskchain.utils.threading.parallel_starmap","text":"Allows use parallel_map for function with multiple arguments. Parameters: Name Type Description Default fun Callable function with multiple arguments required iterable Iterable lists or tuples of arguments required Source code in taskchain/utils/threading.py def parallel_starmap ( fun : Callable , iterable : Iterable , ** kwargs ): \"\"\" Allows use `parallel_map` for function with multiple arguments. Args: fun: function with multiple arguments iterable: lists or tuples of arguments \"\"\" def _call ( d ): return fun ( * d ) return parallel_map ( _call , iterable , ** kwargs )","title":"parallel_starmap()"}]}