{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TaskChain","text":""},{"location":"#what-is-a-taskchain","title":"What is a TaskChain?","text":"<p>TaskChain is a tool for managing data processing pipelines. It was created to reduce chaos in machine learning projects.</p> <p>Goals and features:</p> <ul> <li>separate computation logic and configuration</li> <li>every result should be reproducible</li> <li>brake down computation to individual steps in DAG structure</li> <li>brake down whole project to smaller pipelines which can be easily configured and reused </li> <li>never compute same thing twice - result of computation steps is saved automatically (data persistence)</li> <li>easy access to all intermediate results</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pip install taskchain\n</code></pre>"},{"location":"#from-source","title":"From source","text":"<pre><code>git clone https://github.com/flowerchecker/taskchain\ncd taskchain\npython setup.py install\n</code></pre>"},{"location":"#where-to-start","title":"Where to start?","text":"<ul> <li>read this documentation</li> <li>check example project</li> <li>go through CheatSheet with the most common constructions. </li> </ul>"},{"location":"#main-concepts","title":"Main concepts","text":"<ul> <li> <p>task - one step in computation (data transformation) represented by python class.         Every task can define two type of inputs:</p> <ul> <li>input tasks - other task on which the task depends and take their outputs (data)</li> <li>parameter - additional values which influence computation </li> </ul> </li> <li> <p>pipeline - group of tasks which are closely connected and together represent more complex computation,         e.g. project can be split to pipeline for data preparation,          pipeline for feature extraction and pipeline for model training and evaluation.         Pipelines are only virtual concept and they not have a strict representation in the framework.</p> </li> <li> <p>chain - instance of pipeline or multiple pipelines,          i.e. tasks connected by their dependencies into DAG (directed acyclic graph) with all required parameter values  </p> </li> <li> <p>config - description (usually YAML file) with information needed to instantiate a chain. i.e.:</p> <ul> <li>description of tasks which should be part of a chain (e.g. pipeline)</li> <li>parameter values needed by these tasks</li> <li>eventual dependencies on other configs </li> </ul> </li> </ul>"},{"location":"#typical-project-structure","title":"Typical project structure","text":"<pre><code>project_name\n\u251c\u2500\u2500 configs                     pipeline configuration files\n\u2502   \u251c\u2500\u2500 pipeline_1                  usualy organized to dirs, one per pipeline\n\u2502   \u251c\u2500\u2500 pipeline_2\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 data                        important data, which should be kept in repo, e.g. annotations\n\u251c\u2500\u2500 scripts                     runscripts, jupyter notebooks, etc.\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 project_name\n\u2502       \u251c\u2500\u2500 tasks               definistions of tasks\n\u2502       \u2502   \u251c\u2500\u2500 pipeline_1.py\n\u2502       \u2502   \u251c\u2500\u2500 pipeline_2.py\n\u2502       \u2502   \u2514\u2500\u2500 ...\n\u2502       \u251c\u2500\u2500 utils               other code\n\u2502       \u251c\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 config.py           global project configuration,\n\u251c\u2500\u2500 README.md                       e.g. path to big data or presistence dir\n\u2514\u2500\u2500 setup.py\n</code></pre>"},{"location":"chains/","title":"Chains","text":"<p>Chain si where tasks are instantiated, configured, connected together and allow to compute and access data.</p> <p>Key feature of chain is that data of tasks are saved,  i.e. each task is computed only once.  Moreover, location of saved data is based only on inputs to computation  (input tasks and parameters) and therefore data are shared also between chains. In this way, the exactly same computation is never executed twice. </p>"},{"location":"chains/#chain-creation","title":"Chain creation","text":"<p>Chain is always created from a config which contains all necessary information:</p> <ul> <li>list of tasks</li> <li>parameter values</li> <li>dependencies on other configs</li> <li>dir, where data are saved to</li> </ul> <p>Instantiation of a chain is simple as calling <code>config.chain()</code> or <code>Chain(config)</code>. Whole code to create chain object typically looks like this</p> <p>Chain creation</p> chain.py <pre><code>from taskchain import Config\nfrom project import project_config\n\nchain = Config(\n    project_config.TASKS_DIR,\n    project_config.CONFIGS_DIR / 'config.yaml',\n    global_vars=project_config\n).chain()\n</code></pre> project_config.py <pre><code>from pathlib import Path\n\nREPO_DIR = Path(__file__).resolve().parent.parent.parent\n\nDATA_DIR = Path('/path/to/project_data/source_data')\nTASKS_DIR = Path('/path/to/project_data/task_data')\nCONFIGS_DIR = REPO_DIR / 'configs'\n</code></pre>"},{"location":"chains/#accessing-tasks","title":"Accessing tasks","text":"<p>Tasks are accessible from a chain by their name ether as attribute <code>chain.task_name</code> or  as dict key <code>chain['task_name']</code>.  If there are more tasks of the same name, you can use its full name with group or namespace or both <code>chain['namespace::group:task_name']</code>.</p>"},{"location":"chains/#list-of-tasks","title":"List of tasks","text":"<p><code>chain.tasks</code> contains dict of all tasks. Keys are full names and value are task objects.</p> <p>For better overview of all tasks in chain there is <code>chain.tasks_df</code> which contains pandas DataFrame with basic information about each task:</p> <ul> <li>Index is full name of the task</li> <li><code>name</code> </li> <li><code>group</code> </li> <li><code>namespace</code></li> <li><code>computed</code> - <code>True</code> when task is computed and output data are saved</li> <li><code>data_path</code> - path to data, if data are not computed yet, location is empty</li> <li><code>parameters</code> - list of names of input parameters</li> <li><code>input_tasks</code> - list of full names of input tasks</li> <li><code>config</code> - file name of config which declared this task</li> </ul>"},{"location":"chains/#draw-tasks-and-dependencies","title":"Draw tasks and dependencies","text":"<p>For more visual overview there is</p> <pre><code>chain.draw()\n</code></pre> <p></p> <p>Nodes are tasks, edges dependencies.</p> <ul> <li>color is based on tasks' group</li> <li>border<ul> <li>none - is not persisting data (<code>InMemoryData</code>) </li> <li>dashed - data not computed </li> <li>solid - data computed </li> </ul> </li> </ul> <p>Tip</p> <p>You can pass name of group or list of groups as argument to show only selected tasks. Note, that also neighbours are shown to give context for selected group.</p> <p>Note</p> <p>There are additional python dependencies: <code>graphviz</code> and <code>seaborn</code>.  <pre><code>pip install graphviz seaborn\n</code></pre></p>"},{"location":"chains/#run-tasks-and-data-access","title":"Run tasks and data access","text":"<p>Task object have attribute <code>value</code>. So, when <code>task.value</code> is requested one of three thinks happens</p> <ul> <li>data are not available yet - <code>run</code> method of the task is called and data are saved</li> <li>data are already computed and saved on disk - data are loaded to memory</li> <li>data are already in memory - just return the value</li> </ul> <p>Accessing data</p> <pre><code>requested_data = chain.tasks_name.value\nrequested_data = chain['group:task_name'].value\n</code></pre> <p>Note</p> <p>Calling run method of a task trigger <code>.value</code> on its input tasks  (as long as, you use input_tasks as argument of <code>run</code> method  or call <code>.value</code> manually in <code>run</code> method).  This process is rucrsive, thus single call of <code>.value</code> can trigger loading or computation of many tasks.</p> <p>You can also access <code>Data</code> object which handle saving and loading data  to and from disk by <code>task.data</code></p>"},{"location":"chains/#recomputing-data","title":"Recomputing data","text":"<p>Sometimes some persisted data become invalid, e.g. due to change in tasks' logic. Both <code>Chain</code> and <code>Task</code> have <code>force</code> method which forces tasks to recompute their data instead of loading from disk.</p> <p>If you need recompute only one task, you can simply call  (note that <code>force</code> return task itself, so you can use it in chain with other commands):</p> <pre><code>chain.my_tasks.force().value\n</code></pre> <p>If data of a task turn out to be faulty,  we usually want force recomputation also all dependant tasks in chain.</p> <pre><code>chain.force('my_task')\n</code></pre> <p>Method takes name of a task or list of names of tasks which should be forced. These and all dependant tasks are marked as forced and will be recomputed when their value is requested. Note, that this \"forced\" status is lost when chain is recreated. If you want to be sure, that all data are recomputed,  use <code>recompute=True</code> argument which recompute all forced tasks or <code>delete_data=True</code> which delete data of all forced tasks right away.</p> <p>Warning</p> <p>Be careful with forcing.  If you not handle recomputation correctly, your chain can end in incosistent state. Using <code>delete_data=True</code> solves this problem  but can lead to deletion of \"expensive\" data.</p>"},{"location":"chains/#saved-files","title":"Saved files","text":"<p>TaskChain is design to handle saving and loading data itself and to minimize accessing files directly. Here is general location of data computed by a task</p> <p><code>/path/to/tasks_data/group_name/task_name/{hash}.{extension}</code></p> <ul> <li><code>/path/to/tasks_data</code> is path given in creation of config.</li> <li><code>group_name/</code> - data are organized according to tasks groups (usually this is same as pipelines).    If task is in multiple level group <code>group:subgroup</code> path is <code>group/subgroup/</code>.</li> <li><code>task_name/</code> ale data generated by this task are in this directory.</li> <li><code>{hash}.{extension}</code> is final file (or in some cases directory, e.g. <code>DirData</code>).<ul> <li><code>extension</code> is determined by tasks return type, see more here </li> <li><code>hash</code> is computed based on hashes of all input tasks and all parameter values.     I.e. it is unique and determined fully by all inputs going to computation of these data.</li> </ul> </li> </ul>"},{"location":"chains/#log-files","title":"Log files","text":"<p>Along the file with computed value, there is <code>{hash}.run_info.yaml</code> with run info  and <code>{hash}.log</code> with task's log output. See more here.</p>"},{"location":"chains/#human-readable-files","title":"Human readable files","text":"<p>Persisted data are nicely structured in directories based on tasks' names and groups,  but names of files are unreadable hashes. Data are mostly access through TaskChain, but sometimes it is useful accessed data directly, e.g. copy out final output.  To simplify direct access, chain offers method <code>create_readable_filenames</code>  which creates human-readable symlinks for all tasks in chain. e.g.:</p> <p><code>/path/to/tasks/group_name/task_name/nice_name.pd -&gt; /path/to/tasks/group_name/task_name/a4c5d45a6....pd</code></p> <p><code>nice_name</code> is name of task's config by default.</p> <p>See more details.</p>"},{"location":"chains/#advanced-topics","title":"Advanced topics","text":""},{"location":"chains/#multichains","title":"MultiChains","text":"<p><code>MultiChain</code> is only envelope over multiple chains. Advantage over list or dict of <code>Chain</code> objects is that <code>MultiChain</code> create chains in such way, that they share identical tasks (with same inputs). TaskChain guarantee that identical tasks share data on disk but in this way  there is only one instance of the task, i.e. data are shared also in memory.</p> <p>You can create multi-chain by passing list of configs.  Individual chains can be accessed by config name (usually filename without extension):</p> <pre><code>from taskchain import Config, MultiChain, Chain\n\nconfig1 = Config('/path/to/tasks_data', '/path/to/configs/config1.yaml')\nconfig2 = Config('/path/to/tasks_data', '/path/to/configs/config2.yaml')\nchains = MultiChain([config1, config2, ...])\n\nchain1 = chains.config1\nchain2 = chains['config2']\nchain_dict: dict[str, Chain] = chains.chains\n</code></pre> <p>You can also create multi-chain from all configs in a dir</p> <pre><code>from taskchain import MultiChain\nchains = MultiChain.from_dir(\n    '/path/to/tasks_data', \n    '/path/to/configs',\n    # other kwargs passed to configs such as `global_vars`\n)\n</code></pre> <p>Note</p> <p>You can use multi-chain also with contexts, e.g. one config with different contexts. However, you have to handle names of configs manually (add extra argument <code>name</code> to your configs) to avoid name colision in multi-chain.</p>"},{"location":"chains/#disabling-parameter-mode","title":"Disabling parameter mode","text":"<p>As described above,  location of saved files is based only parameters and input tasks (which are also determined by parameters). This is called parameter mode.</p> <p>You can disable this mode by <code>config.chain(parameter_mode=False)</code> this replaces hashes in filenames by config names.  This approach is little more comprehensible, but has some disadvantages:</p> <ul> <li>some computations can be done more time,      because same task with same parameters can be defined in many configs  </li> <li>you must be careful with names of your configs, e.g. you cannot rename your configs</li> <li>using contexts can and probably would break persistence</li> </ul>"},{"location":"chains/#migration-to-parameter-mode","title":"Migration to parameter mode","text":"<p>You can migrate to parameter mode later using  <code>taskchain.utils.task.migration.migrate_to_parameter_mode</code>.</p>"},{"location":"cheatsheet/","title":"CheatSheet","text":""},{"location":"cheatsheet/#basics","title":"Basics","text":""},{"location":"cheatsheet/#task","title":"Task","text":"<pre><code>class ExampleTask(Task):\n\n    class Meta:\n        input_tasks = [...]\n        parameters = [\n            Parameter('name', default=None),\n        ]\n\n    def run(self, ...) -&gt; ...:\n        ...\n</code></pre>"},{"location":"cheatsheet/#config","title":"Config","text":"<pre><code>tasks: ....*\nuses: \"{CONFIGS_DIR}/....yaml as ...\"\n\nparameter: value\n...\n\nparameter_object:\nclass: path.to.class\nargs:\n- arg1\n- arg2\nkwargs:\nkwarg1: value1\nkwarg2: value2\n...\n</code></pre>"},{"location":"cheatsheet/#chain","title":"Chain","text":"<pre><code>from taskchain import Config\nfrom project import config\n\nchain = Config(\n    config.TASKS_DIR, \n    config.CONFIGS_DIR / 'config_name.yaml', \n    global_vars=config,\n).chain()\nchain.set_log_level('DEBUG')\n\nchain.tasks_df\nchain.draw()\nchain.force('my_task', delete_data=True)\n</code></pre> <pre><code>chain.my_task.value\nchain.my_task.force().value\n\nchain.my_task.has_data\nchain.my_task.data_path\n\nchain.my_task.run_info\nchain.my_task.log\n</code></pre>"},{"location":"cheatsheet/#other","title":"Other","text":""},{"location":"cheatsheet/#dirdata","title":"DirData","text":"<pre><code>def run(self) -&gt; DirData:\n    data_object = self.get_data_object()\n    working_dir = data_object.dir\n    ...\n    return data_object\n</code></pre>"},{"location":"configs/","title":"Configs","text":"<p>Generally, configs defines all necessary parameter values,  which are needed to run a pipeline and tasks in it.</p> <p>In TaskChain, config is also entry point for creating a chain and describes how pipelines are connected to a chain. Therefore, a config has to define:</p> <ul> <li>description of tasks which should be part of a chain (e.g. pipeline)</li> <li>parameter values needed by these tasks</li> <li>dependencies on other configs (pipelines)</li> </ul> <p>Usual setup is one config in one file which defines one pipeline.  This allows effective reuse of the pipeline in multiple chains without need of repeating parameter values.</p> <p>What actually is config?</p> <p>Config in TaskChain has dual meaning.  First, config as YAML file containing information described above. Second, config as an instance of <code>taskchain.Config</code> which is usually based on the config file and adds other information necessary for computation. </p>"},{"location":"configs/#config-definition","title":"Config definition","text":"<p>Simple example of config</p> config.yaml <pre><code>tasks: my_project.tasks.pipeline.*\n\nstring_parameter: value\nint_parameter: 123\ncomplex_parameter:\nkey1: value1\nkey2: - v1\n- v2\n</code></pre> code.py <pre><code>from taskchain import Config\n\nconfig = Config(\n    '/path/to/task_data', \n    '/path/to/config.yaml', \n)\n</code></pre> <p>Config is basically map from strings to arbitrary values.  In addition to YAML files, you can define your config also in JSON file or directly in code by passing dict like object to <code>Config()</code> in parameter <code>data</code>.</p> <p>More examples of configs can be found in example project.</p> <p>You can access values of config object by attribute <code>config.string_parameter</code>  or in dict like fashion <code>config['string_parameter']</code>. Actually, config is descendant of <code>dict</code>. However, direct access to the values is rarely needed because parameters are handled by TaskChain.   </p>"},{"location":"configs/#tasks","title":"Tasks","text":"<p>Each config should define which tasks are configured. This is a way how a config defines a pipeline. The special parameter <code>tasks</code> is string or list of strings describing tasks. Task is described by a path to task's class: <code>'my_project.tasks.pipeline.ProcessTask'</code>. This description corresponds exactly to python imports.</p> <p>To import all tasks from pipeline (defined in single file) at once you can use wildcard <code>*</code> in last  place of description: <code>'my_project.tasks.pipeline.*'</code>.</p>"},{"location":"configs/#task-exclusion","title":"Task exclusion","text":"<p>For more flexibility, you can also exclude tasks with special parameter <code>excluded_tasks</code> with same syntax as <code>tasks</code> parameter.</p>"},{"location":"configs/#config-dependencies","title":"Config dependencies","text":"<p>More complicated chains are split to multiple pipelines with corresponding configs. Parameter <code>uses</code> defines how configs of these pipelines are connected together.</p> <p>For example, project is split to data preparation pipeline and model pipeline.  Config of data preparation have no prerequisites, and thus it doesn't need <code>uses</code>. Some tasks of the model pipeline depends on tasks of data pipeline and therefore model config has to depend on data config.</p> <p>Example</p> model_config.yaml <pre><code>tasks: my_project.tasks.model.*\nuses: \"/path/to/data_config.yaml\"\n\nmodel: ...\n</code></pre> data_config.yaml <pre><code>tasks: my_project.tasks.data.*\n\nsource_file: ...\n</code></pre> <p><code>uses</code> is string or list of strings if there are multiple dependency configs. </p>"},{"location":"configs/#placeholders-global-vars","title":"Placeholders &amp; Global vars","text":"<p>Placeholders and global variables is a mechanism which allows TaskChain projects work in multiple environments. The same project can be moved to different directory or machine  or can be run by multiple peoples with different setups. This is especially useful for handle paths in configs.</p> <p>To make configs independent on environment it is possible to use placeholders in them which are later replaced by values provided in <code>Config</code> object in instantiation in parameter <code>global_vars</code>.</p> <p>Basic usage of global vars</p> config.yaml <pre><code>tasks: my_project.tasks.pipeline.*\nuses: \"{CONFIGS_DIR}/dependency_config.yaml\"\n\nsource_data: {DATA_DIR}/data.csv\n</code></pre> code.py <pre><code>from pathlib import Path\nfrom taskchain import Config\n\nCONFIGS_DIR = Path('/path/to/configs')\n\nconfig = Config(\n    '/path/to/task_data', \n    CONFIGS_DIR / 'config.yaml',\n    global_vars={\n        'CONFIGS_DIR': CONFIGS_DIR,\n        'DATA_DIR': '/path/to/data'\n    }\n)\n</code></pre> <p>Parameter <code>global_var</code> can be a <code>dict</code> with placeholders as keys or an object with placeholders as attributes. This allows following typical construction:</p> <p>Typical usage of global vars</p> code.py <pre><code>from pathlib import Path\nfrom taskchain import Config\n\nfrom project import project_config\n\nconfig = Config(\n    project_config.TASKS_DIR,\n    project_config.CONFIGS_DIR / 'config.yaml',\n    global_vars=project_config\n)\n</code></pre> project_config.py <pre><code>from pathlib import Path\n\nREPO_DIR = Path(__file__).resolve().parent.parent.parent\n\nDATA_DIR = Path('/path/to/project_data/source_data')\nTASKS_DIR = Path('/path/to/project_data/task_data')\nCONFIGS_DIR = REPO_DIR / 'configs'\n</code></pre> config.yaml <pre><code>tasks: my_project.tasks.pipeline.*\nuses: \"{CONFIGS_DIR}/dependency_config.yaml\"\n\nsource_data: {DATA_DIR}/data.csv\n</code></pre>"},{"location":"configs/#parameter-objects","title":"Parameter objects","text":"<p>Sometimes configuration using only json-like parameter values is not enough, or it is not practical. For these cases, you can include definition of a object instance as parameter value to your config. </p> <p>Object instance is defined by class and <code>args</code> and <code>kwargs</code> passed to constructor. Class has to be derived class of <code>taskchain.paramater.ParameterObject</code>, i.e. has to define <code>repr</code> method which should return unique string representation of object. This value is then used by taskchain to keep track of changes in configs,  and it is necessary for correct function of data persistence. </p> <p>Common pattern is that there is base class which defines interface which is used by tasks and parameter objects are instances of child classes.  Good example is <code>Model</code> class which define abstract methods <code>train</code>, <code>predict</code>, <code>save</code> and<code>load</code>. Children of this base class  (e.g. <code>NeuralModel</code>, <code>LinearRegressionModel</code>, ...) implement these methods, are configurable  by their constructor and are used in configs. Here is example of this pattern.</p> <p>Definition of object instance in config is dict containing key <code>class</code> with fully-qualified name of class as value.  Additionally, dict can contain <code>args</code> with a list and <code>kwargs</code> with a dict.</p> <p>Definition of object in config</p> <pre><code>    model:\n        class: my_project.models.LinearRegressionModel\n        kwargs:\n            normalize: True\n            regularization: 1\n</code></pre> <p>In last example, config provide parameter <code>model</code> with value <code>LinearRegressionModel(normalize=True, regularization=1)</code>.</p> <p>In config, objects can be defined inside other structures, such as list, dict or definitions of other objects. I.e. you can define parameter which is list of objects.</p>"},{"location":"configs/#autoparameterobject","title":"<code>AutoParameterObject</code>","text":"<p>Writing of <code>repr</code> method for parameter object can repetitive and omitting a argument can lead to mistakes. Therefore, there is <code>AutoParameterObject</code> which defines <code>repr</code> for you and it is based on class name and arguments of <code>__init__</code> method.  To make it work, all arguments values of constructor has to be saved in object attributes. For argument named <code>my_argument</code>, <code>AutoParameterObject</code> is looking for its value at <code>self._my_argument</code> or <code>self.my_argument</code>.</p> <p>To allow more flexibility and ease adding new arguments, you can also define <code>ignore_persistence_args</code> or <code>dont_persist_default_value_args</code> which return  list of string names of arguments and have similar meaning as  <code>Parameter</code> arguments.</p>"},{"location":"configs/#chainobject","title":"<code>ChainObject</code>","text":"<p>In case that your parameter object need to access the chain directly (e.g. take a task's data), you can inherit also from <code>taskchain.chain.ChainObject</code> and implement <code>init_chain(self, chain)</code> method which is called after chain creation and pass chain itself.</p>"},{"location":"configs/#namespaces","title":"Namespaces","text":"<p>If you need to use one pipeline with different configs in one chain, or you just make your larger chains more structured, you can use namespaces.</p> <p>You can put part of your chain to a namespace and all tasks in that part  will be referenced not but their name <code>task_name</code> but by namespace and task name <code>namespace_name::task_name</code>. </p> <p>Creating namespaces is really simple, in referencing other config in config definition (<code>uses</code> clause) just add <code>as namespace_name</code>.</p> <p>Example</p> model_config.yaml <pre><code>tasks: my_project.tasks.model.*\nuses: - \"/path/to/data_configs/train_data.yaml as train\"\n- \"/path/to/data_configs/valid_data.yaml as valid\"\n- \"/path/to/data_configs/test_data.yaml as test\"\n\nmodel: ...\n</code></pre> model.py <pre><code>...\n\nclass TrainModel(Task):\n    class Meta:\n        parameters = [Parameter('model')]\n        input_tasks = ['train::features', 'valid::features']\n\n    def run(self, model) -&gt; ...:\n        train_data = self.input_tasks['train::features'].value\n        ...\n\nclass EvalModel(Task):\n    class Meta:\n        parameters = [Parameter('model')]\n        input_tasks = [TrainModel, 'test::features]\n\n    def run(self, model, train_model, features) -&gt; dict:\n        ...\n...\n</code></pre> <p>Notes</p> <ul> <li>if a config is in a namespace, also configs used by this config are in the same namespace</li> <li>namespaces can be nested, e.g. task <code>features</code> can be in nested namespace <code>main_model::train</code></li> <li>you can still reference task without namespace as long as there is only one task of that name<ul> <li>this is the case of <code>input_tasks</code> of <code>Evalmodel</code> task but not <code>input_tasks</code> of <code>TrainModel</code>task in example above</li> <li>this applies to referencing tasks in chain, in <code>input_tasks</code> and <code>run</code> method arguments.</li> </ul> </li> </ul>"},{"location":"configs/#advanced-topics","title":"Advanced topics","text":""},{"location":"configs/#multi-config-files","title":"Multi-config files","text":"<p>It is possible to save multiple configs to one file. This can be useful, when chain has multiple pipelines,  and you need one file configuration.</p> <p>Example of multi-config file</p> <pre><code>configs:\ndata_config:\ntasks: ...\ninput_data_file: ...\n...\nmodel_config:\nmain_part: True\ntasks: ...\nuses: \"#data_config as data\"\nmodel: ...\n...\nother_config_name:\n...\n</code></pre> <p>Single config can be taken from this file using <code>part</code> argument:  <pre><code>config = Config('/path/to/task_data', 'multi_config.yaml', part='data_config')\n</code></pre> It is possible omit <code>part</code> argument if one of defined configs specify <code>main_part: True</code>. To access a config from another config (in <code>uses</code>) use  <code>/path/to/multi_config.yaml#data_config</code> or if you refer to a part of the same multi-config,  you can use only <code>#data_donfig</code> as shown in example.</p>"},{"location":"configs/#contexts","title":"Contexts","text":"<p>Context is mechanism which allows rewrite parameter values in configs in the time of their instantiation. Under the hood <code>Context</code> is special case of <code>Config</code> which is used in specific way. </p> <p>Example</p> <pre><code>config = Config(\n    '/path/to/task_data', \n    CONFIGS_DIR / 'config.yaml',\n    context={'verbose': True, 'batch_size': 32}\n)\n</code></pre> <p>This can be useful for</p> <ul> <li>ad-hock experiment</li> <li>hyper-parameter optimization</li> <li>tuning parameters, which are not used in persistence, e.g. <code>batch_size</code> </li> <li>consider long data processing chain consisting of multiple dependent pipelines each with own config file.    When we get new input data, it usually leads to recreating all configs which are very similar    (only <code>input_data</code> parameter is changed and config paths in <code>uses</code>).   Other approach is omit <code>input_data</code> parameter value in config and provide it context,    which allows run pipeline with same configuration on multiple inputs easily.</li> </ul>"},{"location":"configs/#what-can-be-context","title":"What can be context","text":"<ul> <li>dict of parameters and their values</li> <li>file path with json on yaml file - this is analogous to context files</li> <li>Context object</li> <li>list of previous - in that case context are merged together.    In case of parameter conflict later has higher priority.</li> </ul> <p>Warning</p> <p>Parameters in context are applied globaly, i.e. on all configs in chain.  Be cearful with parameters of the same name in different pipelines.</p>"},{"location":"configs/#namespaces_1","title":"Namespaces","text":"<p>In the case of more complicated chains which uses namespaces you can run to problems,  when one pipeline is in chain multiple times with different configuration (under different namespaces). For these cases, context can have <code>for_namespaces</code> field. It's valued should be dict with namespaces as keys and parameters to overwrite in this namespace as value.</p> <p>Context YAML file using <code>for_namespace</code></p> <pre><code>for_namespaces:\ntrain:\ninput_data: '/path/to/data1'\nother_param: 42\ntest:\ninput_data: '/path/to/data2'\n\nbatch_size: 32\n</code></pre>"},{"location":"configs/#uses","title":"Uses","text":"<p>It is possible to join multiple context in one file with <code>uses</code> field.  Syntax is the same as in configs, but meaning slight different.  In contexts files in <code>uses</code> are just merged to the main context. If <code>... as namespace</code> format is used, loaded context is applied only for given namespace. Following example is equivalent to the previous one.</p> <p>Context YAML files using <code>uses</code></p> context.yaml <pre><code>uses:\n- /path/to/train.context.yaml as train\n- /path/to/test.context.yaml as test\n\nbatch_size: 32\n</code></pre> train.context.yaml <pre><code>input_data: '/path/to/data1'\nother_param: 42\n</code></pre> test.context.yaml <pre><code>input_data: '/path/to/data2'\n</code></pre>"},{"location":"example/","title":"Example project - movie ratings","text":"<p>Example project is small demonstration of TaskChain capabilities and try to show its main features and constructions.</p> <p>This project allows quick hands-on experience  and can serve as template for new projects. You can start by running this notebook. </p> <p>Keep in mind, that goal of the project is showcase of various features, so chosen solutions for given problems are not always optimal.</p>"},{"location":"example/#install","title":"Install","text":"<pre><code>pip install taskchain\n\ngit clone https://github.com/flowerchecker/taskchain\ncd taskchain/example\npython setup.py develop\n</code></pre>"},{"location":"example/#description","title":"Description","text":"<p>Project works with IMDB movie dataset downloaded from Kaggle. Goals of projects is to explore dataset and train a model which is able to predict rating of a new movie. </p> <p>Project is to split to 3 pipelines</p>"},{"location":"example/#movies","title":"Movies","text":"<p>tasks, configs, notebook</p> <p>This pipeline has the following functions</p> <ul> <li>load movies data</li> <li>filter them</li> <li>get basic statistics - year and duration histograms</li> <li>extract directors, movies, genres and countries of movies</li> </ul>"},{"location":"example/#features","title":"Features","text":"<p>tasks, configs, notebook</p> <p>This pipeline build on movie pipeline and has the following functions</p> <ul> <li>select the most relevant actors and directors (to use them as features)</li> <li>prepare all features - year, duration, and features based on movie's genres, countries, actors and directors (binary features) </li> <li>select requested feature types</li> </ul>"},{"location":"example/#rating-model","title":"Rating model","text":"<p>tasks, configs, notebook</p> <p>This pipeline build on features pipeline and has the following functions</p> <ul> <li>create training and eval data from features</li> <li>train a mode - models are defined here</li> <li>evaluate the model</li> </ul>"},{"location":"example/#project-files","title":"Project files","text":"<pre><code>example\n\u251c\u2500\u2500 configs\n\u2502   \u251c\u2500\u2500 features\n\u2502   \u2502   \u251c\u2500\u2500 all.yaml\n\u2502   \u2502   \u2514\u2500\u2500 basic.yaml\n\u2502   \u251c\u2500\u2500 movies\n\u2502   \u2502   \u251c\u2500\u2500 imdb.all.yaml\n\u2502   \u2502   \u2514\u2500\u2500 imdb.filtered.yaml\n\u2502   \u2514\u2500\u2500 rating_model\n\u2502       \u251c\u2500\u2500 all_features\n\u2502       \u2502   \u251c\u2500\u2500 baseline.yaml\n\u2502       \u2502   \u251c\u2500\u2500 linear_regression.yaml\n\u2502       \u2502   \u251c\u2500\u2500 nn.yaml\n\u2502       \u2502   \u2514\u2500\u2500 tf_linear_regression.yaml\n\u2502       \u2514\u2500\u2500 basic_features\n\u2502           \u251c\u2500\u2500 baseline.yaml\n\u2502           \u2514\u2500\u2500 linear_regression.yaml\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 source_data\n\u2502   \u2502   \u251c\u2500\u2500 IMDB_movies.csv\n\u2502   \u2502   \u2514\u2500\u2500 ratings.Thran.csv\n\u2502   \u2514\u2500\u2500 task_data       # here will be computed data\n\u251c\u2500\u2500 scripts\n\u2502   \u251c\u2500\u2500 features.ipynb\n\u2502   \u251c\u2500\u2500 introduction.ipynb\n\u2502   \u251c\u2500\u2500 movies.ipynb\n\u2502   \u251c\u2500\u2500 personal_rating_model.ipynb\n\u2502   \u251c\u2500\u2500 rating_model.ipynb\n\u2502   \u2514\u2500\u2500 tasks_run.py\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 movie_ratings\n        \u251c\u2500\u2500 config.py\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 models\n        \u2502   \u251c\u2500\u2500 core.py\n        \u2502   \u251c\u2500\u2500 __init__.py\n        \u2502   \u251c\u2500\u2500 sklearn.py\n        \u2502   \u2514\u2500\u2500 tensorflow.py\n        \u2514\u2500\u2500 tasks\n            \u251c\u2500\u2500 features.py\n            \u251c\u2500\u2500 __init__.py\n            \u251c\u2500\u2500 movies.py\n            \u2514\u2500\u2500 rating_model.py\n</code></pre>"},{"location":"tasks/","title":"Tasks","text":"<p>Task defines computation step in a pipeline and  has to be class inheriting <code>taskchain.Task</code>.</p>"},{"location":"tasks/#basic-structure","title":"Basic structure","text":"<p>Example</p> <pre><code>from taskchain import Task, Parameter\nimport pandas as pd\n\nclass MyTask(Task):\n\n    class Meta:\n        input_tasks = [DependencyTask]\n        parameters = [\n            Parameter('param')\n        ]\n\n    def run(dependency, param) -&gt; pd.DataFrame:\n        # ... compitation ...\n        return result\n</code></pre>"},{"location":"tasks/#meta-subclass","title":"<code>Meta</code> subclass","text":"<p>This class has to be defined inside every task and describes how should be the task handled by TaskChain. The Meta class in not meant to define any methods, it should only define some attributes:</p> <ul> <li>input_tasks (<code>List[Task | str]</code>) - dependency tasks, more below</li> <li>parameters (<code>List[Parameter]</code>) - parameters required by this task, which come from your configs, more below</li> <li>abstract (<code>bool</code>) - if <code>True</code> this task isn't instantiated and is never part of a chain, useful for inheritance</li> <li>name (<code>str</code>) - if not defined, name is derived from class name</li> <li>group (<code>str</code>)</li> <li>data_class (<code>Type[Data]</code>) - custom class to use for output data persistence</li> <li>data_type (<code>Type</code>) - output type of <code>run</code> method, alternative to typing notation </li> </ul>"},{"location":"tasks/#run-method","title":"<code>run</code> method","text":"<p>This method is called by TaskChain when output of this task is requested and data are not already persisted.</p> <p>Definition of the method has to contain return type (using typing <code>-&gt;</code> or <code>data_type</code> in <code>Meta</code> class). Value returned by <code>run</code> method is checked by TaskChain if matches defined type.  Return type is important for data persistence. </p> <p>Tip</p> <p>The method can have arguments.  TaskChain try to match and fill these arguments by values of parameters and input task.</p> <p>Warning</p> <p>Avoid expensive computation or loading data in <code>__init__</code>.  TaskChain can create task object multiple times and often task is not used at all. Put all expensive operation to <code>run</code> method.  You can use <code>@persistent</code> decorator. </p>"},{"location":"tasks/#task-names-and-groups","title":"Task names and groups","text":"<p>Name of the task (<code>str</code>) is either defined by <code>name</code> attribute of <code>Meta</code> or it is derived from class name  (converting to underscore notation and removing task at the end). Here are some examples:</p> <ul> <li><code>DataTask</code> -&gt; <code>data</code></li> <li><code>FilteredDataTask</code> -&gt; <code>filtered_data</code></li> <li><code>FilteredData</code> -&gt; <code>filtered_data</code></li> <li><code>LongNameOfTheTask</code> -&gt; <code>long_name_of_the</code></li> </ul> <p>Group of task allows keep some order in larger projects and have impact on location of persisted data.  Usually task with a same group defines pipeline. The group can be defined in <code>Meta</code> class. and if it is the fullname of the task is <code>group_name:task_name</code>.  If you need more rich structure of groups, you can use <code>:</code> to separate multiple levels of groups, e.g. <code>group:subgroup</code>. </p> <p>Tip</p> <p>Usually all task of a pipeline are defined in one module (file). To avoid defining same group in all tasks, it is possible inherit from <code>ModuleTask</code> or <code>DoubleModuleTask</code> instead of <code>Task</code>. In that case group is set to module name.</p>"},{"location":"tasks/#parameters","title":"Parameters","text":"<p>Parameters are connection between tasks and configs.  Parameters defined in <code>Meta</code> tell TaskChain which values should be extracted from configs  and provided for <code>run</code> method.</p> <p>Parameter can be accessed through arguments of <code>run</code> method  or directly from class's <code>ParameterRegistry</code>: <code>self.params.my_param_name</code> or <code>self.params['my_param_name']</code>.</p> <p>Example</p> <pre><code>class AllDataTask(Task):\n    class Meta:\n        parameters = [\n            Parameter('input_file')\n        ]\n\n    def run(self, input_file) -&gt; pd.DataFrame\n        assert input_file == self.params.input_file\n        return pd.read_csv(input_file)\n\nclass FilteredDataTask(Tasks):\n    class Meta:\n        input_tasks = [AllDataTask]\n        parameters = [\n            Parameter('min_value', default=0)\n            Parameter('max_value')\n        ]\n\n    def run(self, all_data, min_value, max_value) -&gt; pd.DataFrame\n        return all_data.query('{min_Value} &lt;= value &lt;= {max_value}')\n</code></pre>"},{"location":"tasks/#parameters-arguments","title":"Parameter's arguments","text":"<ul> <li>name - name for referencing from task</li> <li>default - value used if not provided in config, defaults to NO_DEFAULT meaning that param is required</li> <li>name_in_config - name used for search in config, defaults to <code>name</code> argument</li> <li>dtype - expected datatype</li> <li>ignore_persistence - do not use this parameter in persistence, useful for params without influence on output,          e.g. <code>verbose</code> or <code>debug</code></li> <li>dont_persist_default_value - if value of parameter is same as the default, do not use it in persistence.         This is useful for adding new parameters without recomputation of old data</li> </ul> <p>Tip</p> <p>You can use <code>pathlib.Path</code> as datatype. Expected value in config is <code>str</code>, however,  value provided by the parameter has type of <code>Path</code>.</p>"},{"location":"tasks/#reserved-parameter-names","title":"Reserved parameter names","text":"<p>Following names have special meaning in configs and cannot be used as parameter names:</p> <ul> <li><code>tasks</code>, <code>excluded_tasks</code></li> <li><code>uses</code></li> <li><code>human_readable_data_name</code></li> <li><code>configs</code>, <code>for_namespaces</code>, <code>main_part</code></li> </ul>"},{"location":"tasks/#input-tasks","title":"Input tasks","text":"<p>Input tasks are connection between tasks.  This <code>Meta</code> argument tells TaskChain which other tasks are prerequisites of this task.</p> <p>Values (data) of input tasks can be accessed through arguments of <code>run</code> method or directly from class's <code>InputTasks</code>: <code>self.input_task['my_task'].value</code>.</p> <p>It is also possible access input task by index: <code>self.input_task[0].value</code>.  This can be useful if task inheritance is used.  Then <code>run</code> method can stay unchanged and only <code>input_tasks</code> can be redefined. </p> <p>Input task can be defined in following ways:</p> <ul> <li>by class: <code>input_tasks = [MyDataTask]</code> - this way is preferred if possible</li> <li>by name: <code>input_tasks = ['my_data']</code></li> <li>by name and group: <code>input_tasks = ['group:my_data']</code></li> <li>by name, group and namespace: <code>input_tasks = ['namespace::group:my_data']</code></li> <li>by regexp string starting with <code>~</code>: <code>input_tasks = ['~my_*']</code> - this expands to all matching tasks in chain        (tasks starting with <code>my_</code>)</li> </ul>"},{"location":"tasks/#data-persistence","title":"Data persistence","text":"<p>Persisting output of tasks is main feature of the TaskChain.</p> <p>When <code>run</code> method produce value (data) TaskChain saves this value and later when the value of the task is requested again, value is just loaded  instead of calling <code>run</code> method again.</p>"},{"location":"tasks/#data-class","title":"<code>Data</code> class","text":"<p>Saving and loading of values is handled by inheritors of <code>taskchain.task.Data</code> class. Witch class is used is determined automatically by return data type of <code>run</code> method  or by <code>data_class</code> attribute of <code>Meta</code>.</p> <p>These <code>Data</code> classes are determined automatically:</p> <ul> <li>JSONData persists <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>dict</code>, <code>list</code> types into <code>.json</code> files</li> <li>NumpyData persists <code>np.ndarray</code> type into <code>.npy</code> file</li> <li>PandasData persists <code>pd.DataFrame</code> or <code>pd.Series</code> type into <code>.pd</code> file</li> <li>FigureData persists <code>plt.Figure</code> type into pickle but also saves plot as <code>.png</code> and <code>.svg</code> for easy sharing.   Use <code>pylab</code> or <code>seaborn</code> as usual and just return <code>plt.gcf()</code>. </li> <li>GeneratedData is used if return type is <code>Generator</code>. It is assumed that generated values are json-like.     Values are saved to <code>.jsonl</code> file  (json lines).</li> </ul> <p>Other useful <code>Data</code> classes which have to be explicitly defined in <code>data_class</code> attribute.</p> <ul> <li>InMemoryData - this special type is not persisting and value is saved only in memory of process.</li> </ul> <p>Example</p> <pre><code>class MyTask(Task):\n\n    class Meta:\n        data_class = InMemoryData\n\n    def run() -&gt; WhatEver:\n        # ...\n        return whatever\n</code></pre> <ul> <li>DirData - this class allows save arbitrary data to provided directory,          but data have to be handled inside <code>run</code> method manually.          Value of the task is <code>Path</code> of this directory.</li> </ul> <p>Example</p> <pre><code>class MyTask(Task):\n\n    def run(self) -&gt; DirData:\n        # ...            \n        data_object = self.get_data_object()\n        self.save(my_data_1, data_object.dir / 'my_data_1.pickle')\n        self.save(my_data_2, data_object.dir / 'my_data_2.pickle')\n        return data_object\n\n    def save(self, data, dir_path):\n        ...\n</code></pre> <ul> <li>ContinuesData -  for task with large run time, e.g. training of large models,  it is possible to make computation on multiple runs.  This allows to save partial results and next call of run method starts from last checkpoint  when computation is interrupted.</li> </ul> <p>Example</p> <pre><code>class TrainModel(Task):\n    class Meta:\n        ...\n\n    def run(self) -&gt; ContinuesData:\n        data: ContinuesData = self.get_data_object()\n        working_dir = data.dir\n\n        self.prepare_model()\n\n        checkpoint_path = working_dir / 'checkpoint'\n        if checkpoint_path.exists():\n            self.load_checkpoint(checkpoint_path)\n\n        self.train(\n            save_path=working_dir / 'trained_model'\n            checkpoint_path=checkpoint_path\n        ) # training should save checkpoint periodically and trained model at the end\n\n        data.finished()\n        return data\n</code></pre> <ul> <li>H5Data - special case of <code>ContinuesData</code> which allows to compute large data files.</li> </ul> <p>Example</p> <pre><code>class Embeddings(Task):\n\n    class Meta:\n        ...\n\n    def run(self) -&gt; H5Data:\n        data: H5Data = self.get_data_object()\n\n        with data.data_file() as data_file:\n            h5_emb_dataset = data.dataset('embedding', data_file, maxshape=(None, embedding_size), dtype=np.float32)\n            progress = h5_emb_dataset.len()\n\n            for i, row in enumerate(my_dataset[progress:]):\n                if i % 1000 == 0:\n                    gc.collect()\n                emb = self.get_embedding(row)\n                data.append_data(h5_emb_dataset, [emb], dataset_len=progress)\n                data_file.flush()\n                progress += 1\n        data.finished()\n        return data\n</code></pre> <p>You can define ad hoc <code>Data</code> classes to handle other data types.</p>"},{"location":"tasks/#returning-data-object-directly","title":"Returning <code>Data</code> object directly","text":"<p>In some cases it is convenient to return (by <code>run</code> method) <code>Data</code> object directly. <code>DirData</code> is one example.  Other use case is custom object which inherits from <code>InMemoryData</code>. See <code>TrainedModel</code> task in example project which returns <code>RatingModel</code> directly. This is the way to easily expose a important object to other tasks in the pipeline.</p>"},{"location":"tasks/#logging","title":"Logging","text":"<p>TaskChain offer two ways to save addition information about computation mainly for debug purposes.</p>"},{"location":"tasks/#run-info","title":"Run info","text":"<p>After <code>run</code> method finishes and result value is saved to disk <code>Data</code> object also save additional information about computation. It is possible add any json-like information to this info. <pre><code>class MyTask(Task):\n    ...\n    def run(self):\n        ...\n        self.save_to_run_info('some important information')\n        self.save_to_run_info({'records_processes': 42, 'errors': 0})\n</code></pre></p> <p>The run info is saved as YAML and is available under <code>task_object.run_info</code> in json-like form.</p> <p>hash.run_info.yaml</p> <pre><code>task:\nclass: Movies\nmodule: movie_ratings.tasks.movies\nname: movies:movies\nconfig:\ncontext: null\nname: imdb.filtered/movies:movies\nnamespace: null\ninput_tasks:\nmovies:all_movies: 436f7a5e06e540716b275a5f84499a78\nlog: - some important information\n- records_processes: 42\nerrors': 0\nparameters:\nfrom_year: '1945'\nmin_vote_count: '1000'\nto_year: None\nstarted: '2021-07-11 11:34:01.520866'\nended: '2021-07-11 11:34:01.850913'\ntime: 0.3300471305847168\nuser:\nname: your_system_name\n</code></pre>"},{"location":"tasks/#logger","title":"logger","text":"<p>Each task has its own standard python logger,  which can be used from <code>run</code> method. <pre><code>class MyTask(Task):\n    ...\n    def run(self):\n        ...\n        self.logger.debug('not so important information')\n</code></pre></p> <p>This logger has two handlers</p> <ul> <li>File handler managed by <code>Data</code> object which saves log along value produced by task.    Logging level of this handler is set to <code>DEBUG</code>.</li> <li>Other handler is managed by chain object and log to console.    Logging level of this handler is set to <code>WARNING</code> and can be changed from chain by <code>chain.set_log_level('DEBUG')</code>.</li> </ul>"},{"location":"tasks/#advanced-topics","title":"Advanced topics","text":""},{"location":"tasks/#tasks-inheritance","title":"Tasks inheritance","text":"<p>Tasks are classes and can be inherited.  This simplifies cases when pipeline contains tasks with similar functionality.</p> <p>You can inherit a task and change his behaviour by</p> <ul> <li>changing <code>Meta</code> class<ul> <li>you can change input tasks and then computation will be done with different input.   In this case, it is not possible have input tasks in <code>run</code> arguments,    and they can access by <code>self.input_tasks[0].value</code>.    This way, use of the task name, which is changing, is avoided. </li> <li>you can add custom attribute to <code>Meta</code> class and access it by <code>self.meta.my_attribute</code>   and make computation based on its value.</li> </ul> </li> <li>you can override some methods used by <code>run</code> method</li> </ul> <p>It is possible declare in <code>Meta</code> class <code>abstract = True</code>.  In that case, task will be not recognized by <code>project.tasks.pipeline.*</code> in config  and will not be part of your pipeline.  This can be useful for tasks, which will be inherited from.</p> <p>Example of task inheritance can be found in example project</p> <ul> <li>movies pipeline - search for <code>ExtractFeatureTask</code>. </li> <li>model pipeline - search for <code>DataSelectionTask</code>. </li> </ul>"},{"location":"testing/","title":"Testing","text":"<p>To help with testing of tasks and chains TaskChain offers some helper tools.</p>"},{"location":"testing/#testing-of-a-single-task","title":"Testing of a single task","text":"<p>A task usually exists in a context of chain with other tasks and configs. Helper function <code>create_test_task</code> create tasks and mocks the context.</p> <p>Testing of a task</p> <pre><code>...\n\nclass MyTask(Task):\n    class Meta:\n        input_tasks = [ATask, BTask]\n        parameters = [\n            Parameter('p1'),\n            Parameter('p2', default=3),\n        ]\n\n    def run(self, a, b, p1, p2) -&gt; int:\n        ...\n        return a + b + p1 + p2\n\nfrom taskchain.utils.testing import create_test_task\n\ntask = create_test_task(\n    MyTask,             # class of tested task\n    input_tasks={       # mocked input task values\n        'a': 7,         #   task can be referenced by name\n        BTask: 6,       #   or by class\n    },\n    parameters={\n        'p1': 42,       # parameters provided to tested task\n    }\n)\nassert task.value == 7 + 6 + 42 + 3\n</code></pre>"},{"location":"testing/#testing-of-a-part-of-a-chain","title":"Testing of a part of a chain","text":"<p>You can also test more tasks together.  Class <code>TestChain</code> creates chain where some tasks are mocked,  i.e. their values are not computed but provided on creation of test chain.</p> <p>Testing of a part of chain</p> <pre><code>...\n\nclass KTask(Task):\n    class Meta:\n        input_tasks = [JTask, BTask]\n        parameters = [Parameter('p2')]\n\n    def run(self, j, b, p2) -&gt; int:\n        ...\n\nclass LTask(Task):\n    class Meta:\n        input_tasks = [KTask]\n        parameters = [Parameter('p2')]\n\n    def run(self, k, pc) -&gt; int:\n        ...\n\nfrom taskchain.utils.testing import TestChain\n\nchain = TestChain(\n    tasks = [KTask, LTask],\n    mock_tasks = {\n        'j': ...,\n        BTask: ...,\n    },\n    parameters={\n        'p1': ...,\n        'p2': ...,\n    },\n)\nassert chain.k.value == ...\nassert chain.l.value == ...\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#ic-icecream","title":"<code>ic</code> ( IceCream )","text":"<p>If you import taskschain <code>ic</code> is installed and you can use it whithout import.</p>"},{"location":"utils/#caching","title":"Caching","text":"<p>For ease up saving \"expensive\" computation, TaskChain have simple caching tools. This can be used for saving e.g. api calls.</p> <pre><code>from taskchain.cache import JsonCache\n\ncache = JsonCache('/path/to/cache_dir')\n\ninput_ = 42\n\ndef _computation():\n    return expensive(input_)\n\ncache_key = f'key_{input_}'\nresult = cache.get_or_compute(cache_key, _computation)\n</code></pre> <p>Result is loaded from cache if presented or computed and saved in cache.</p> <p>You can also use <code>@cached</code>  decorator which can handle creation of cache key automatically from arguments.</p> <pre><code>from taskchain.cache import JsonCache, cached\n\nclass MyClass:\n    @cached(JsonCache('/path/to/cache_dir'))\n    def cached_method(self, input_):\n        return expensive(input_)\n\nmy = MyClass()\nresult = my.cached_method(42)\n</code></pre> <p>There are multiple <code>Cache</code> classes prepared</p> <ul> <li><code>DummyCache</code> - no caching</li> <li><code>InMemoryCache</code> - values are cached only in memory, all types are allowed</li> <li><code>JsonCache</code> - saves json-like objects to json</li> <li><code>DataFrameCache</code></li> <li><code>NumpyArrayCache</code></li> <li><code>FileCache</code> - abstract class useful for implementing own type of caches</li> </ul>"},{"location":"utils/#persistent-decorator","title":"<code>@persistent</code> decorator","text":"<p>This decorator can be used on class methods without arguments. Result of this method is stored in <code>self.__method_name</code> attribute after first call. On other calls stored value is returned. </p> <p>Tip</p> <p>You can also combine <code>@persistent</code> with <code>@property</code> decorator,  just make sure that <code>@property</code> is before <code>@persistent</code>.</p> <p>This can be useful in implementation of lazy behaviour of your classes.</p> <p>Example</p> lazy solution <pre><code>class MyClass:\n\n    @property\n    @persistent\n    def foo(self):\n        return expensive_computation()\n</code></pre> classic solution <pre><code>class MyClass:\n\n    def __init__(self):\n        self.foo = expensive_computation()\n</code></pre>"},{"location":"utils/#parallel_map","title":"<code>parallel_map</code>","text":"<p>You can use <code>parallel_map</code> for easy utilization of threading.</p> <pre><code>from taskchain.utils.threading import parallel_map\n\n\ndef download_urls(urls: list, threads=2):\n    def _fun(url):\n        return download(url)\n\n    return parallel_map(_fun, urls, threads=threads, desc='Downloading', total=len(urls))\n</code></pre>"},{"location":"utils/#repeat_on_error-decorator","title":"<code>@repeat_on_error</code> decorator","text":"<p>This decorator is useful for calling api or downloading data from internet.  It tries to run a method again if error occurs.</p> <pre><code>from taskchain.utils.clazz import repeat_on_error\n\nclass Downloader:\n    # first retry is after 2 second, second after 4, third after 8\n    @repeat_on_error(waiting_time=2, wait_extension=2, retries=3)\n    def download(self, url, exact_match=True):\n        ...\n</code></pre>"},{"location":"code/chain/","title":"Chain","text":""},{"location":"code/chain/#taskchain.chain.ChainObject","title":"<code>ChainObject</code>","text":"<p>If ParameterObject inherits this class, chain call <code>init_chain</code> on initialization and allow object to access whole chain.</p> Source code in <code>taskchain/chain.py</code> <pre><code>class ChainObject:\n\"\"\"\n    If ParameterObject inherits this class, chain call `init_chain` on initialization and allow\n    object to access whole chain.\n    \"\"\"\n\n    @abc.abstractmethod\n    def init_chain(self, chain):\n        pass\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain","title":"<code>Chain</code>","text":"<p>         Bases: <code>dict</code></p> <p>Chain takes a config, recursively load prerequisite configs, initialize tasks connect them to DAG vie input tasks.</p> Source code in <code>taskchain/chain.py</code> <pre><code>class Chain(dict):\n\"\"\"\n    Chain takes a config, recursively load prerequisite configs, initialize tasks connect them to DAG vie input tasks.\n    \"\"\"\n\n    log_handler = log_handler\n\n    @classmethod\n    def set_log_level(cls, level):\n\"\"\" Set log level to log handler responsible for console output of task loggers. \"\"\"\n        Chain.log_handler.setLevel(level)\n\n    def __init__(self, config: Config,\n                 shared_tasks: Dict[Tuple[str, str], Task] = None,\n                 parameter_mode: bool = True\n                 ):\n        super().__init__()\n        self.tasks: Dict[str, Task] = {}\n        self._configs: Dict[str, Config] = {}\n\n        self._parameter_mode = parameter_mode\n        self._base_config = config\n        self._task_registry = shared_tasks if shared_tasks is not None else {}\n        self.graph: Union[None, nx.DiGraph] = None\n\n        if not parameter_mode and config.context is not None:\n            logging.warning('Using context without parameter mode can break persistence!')\n\n        self._prepare()\n\n    def __str__(self):\n        a = max(len(n.split(\":\")[-1]) for n in self.tasks)\n        b = max(len(n) for n in self.tasks)\n        return '\\n'.join(f'{n.split(\":\")[-1]:&lt;{a}}  {n:&lt;{b}}  {t.get_config()}' for n, t in self.tasks.items())\n\n    def __repr__(self):\n        return f'&lt;chain for config `{self._base_config}`&gt;'\n\n    def _repr_markdown_(self):\n\"\"\" Nice display in jupyter notebooks. \"\"\"\n        return self.tasks_df[['name', 'group', 'namespace', 'computed']].to_markdown()\n\n    @property\n    def tasks_df(self) -&gt; pd.DataFrame:\n\"\"\" Dataframe with rows ass all tasks in chain. \"\"\"\n        rows = {}\n        for name, task in self.tasks.items():\n            rows[name] = {\n                'name': task.slugname.split(':')[-1],\n                'group': task.group,\n                'namespace': task.get_config().namespace,\n                'computed': task.has_data if task.data_path else None,\n                'data_path': task.data_path,\n                'parameters': list(task.parameters.keys()),\n                'input_tasks': list(task.input_tasks.keys()),\n                'config': str(task.get_config()).split(\"/\")[0],\n            }\n\n        return pd.DataFrame.from_dict(rows, orient='index').sort_values(['namespace', 'group'], na_position='first')\n\n    def __getitem__(self, item):\n\"\"\" Get task by name in dict-like fashion. \"\"\"\n        return self.get(item)\n\n    def __getattr__(self, item):\n\"\"\" Get task by name as atribute. \"\"\"\n        if item in self:\n            return self.get(item)\n        return self.__getattribute__(item)\n\n    def get(self, item, default=None):\n\"\"\" Get task by name. \"\"\"\n        if default is not None:\n            raise ValueError('Default task is not allowed')\n        return self.tasks.get(_find_task_full_name(item, self.tasks.keys()))\n\n    def __contains__(self, item):\n        try:\n            return _find_task_full_name(item, self.tasks.keys()) in self.tasks\n        except KeyError:\n            return False\n\n    def _prepare(self):\n\"\"\" Initialize chain. \"\"\"\n        self._process_config(self._base_config)\n        tasks = self._create_tasks(task_registry={} if self._parameter_mode else self._task_registry)\n        self._process_dependencies(tasks)\n\n        if self._parameter_mode:\n            self.tasks = self._recreate_tasks_with_parameter_config(tasks, self._task_registry)\n            self._process_dependencies(self.tasks)\n        else:\n            self.tasks = tasks\n\n        self._build_graph()\n        self._init_objects()\n\n    def _process_config(self, config: Config):\n\"\"\" Look for prerequisite configs, instantiate them and process them recursively. \"\"\"\n        if config.repr_name in self._configs:\n            return\n        self._configs[config.repr_name] = config\n        for use in list_or_str_to_list(config.get('uses', [])):\n            if isinstance(use, str):\n                pattern = r'(.*) as (.*)'\n                if matched := re.match(pattern, use):\n                    # uses config with namespace\n                    used_config = Config(\n                        config.base_dir,\n                        filepath=matched[1],\n                        namespace=f'{config.namespace}::{matched[2]}' if config.namespace else matched[2],\n                        global_vars=config.global_vars,\n                        context=config.context,\n                    )\n                else:\n                    # uses config without namespace\n                    used_config = Config(\n                        config.base_dir,\n                        use,\n                        namespace=config.namespace if config.namespace else None,\n                        global_vars=config.global_vars,\n                        context=config.context,\n                    )\n            else:\n                # mainly for testing\n                assert isinstance(use, Config)\n                assert config.base_dir == use.base_dir, f'Base dirs of configs `{config}` and `{use}` do not match'\n                if config.namespace:\n                    if use.namespace:\n                        use.namespace = f'{config.namespace}::{use.namespace}'\n                    else:\n                        use.namespace = config.namespace\n                use.context = config.context\n                use._prepare()\n                used_config = use\n            self._process_config(used_config)\n\n    def _create_tasks(self, task_registry=None) -&gt; Dict[str, Task]:\n\"\"\" Look to configs and instantiate their tasks. \"\"\"\n        tasks = {}\n\n        def _register_task(_task: Task, task_name: str):\n            if task_name in tasks and tasks[task_name].get_config() != _task.get_config():\n                raise ValueError(f'Conflict of task name `{task_name}` '\n                                 f'with configs `{tasks[task_name].get_config()}` and `{_task.get_config()}`')\n            tasks[task_name] = _task\n\n        for config in self._configs.values():\n            # first find excluded tasks, then register all other tasks\n            excluded_tasks = set()\n            for field_name, exclude in [('excluded_tasks', True), ('tasks', False)]:\n\n                def _process_task(_task_class):\n                    if exclude:\n                        excluded_tasks.add(_task_class)\n                        return\n                    if _task_class in excluded_tasks:\n                        return\n                    task = self._create_task(_task_class, config, task_registry)\n                    _register_task(task, _task_class.fullname(config))\n\n                for task_description in list_or_str_to_list(config.get(field_name, [])):\n                    if type(task_description) is str:\n                        for task_class in get_classes_by_import_string(task_description, Task):\n                            if task_class.meta.get('abstract', False):\n                                continue\n                            _process_task(task_class)\n                    elif issubclass(task_description, Task):\n                        # mainly for testing\n                        _process_task(task_description)\n                    else:\n                        raise ValueError(f'Unknown task description `{task_description}` in config `{config}`')\n        return tasks\n\n    def _recreate_tasks_with_parameter_config(self, tasks: Dict[str, Task], task_registry: Dict) -&gt; Dict[str, Task]:\n\"\"\"\n        Helper function for parameter mode.\n        Take tasks and instantiate them again but with TaskParameterConfig\n        which contain only parameters needed for the tasks and knows all input tasks (new ones)\n        so it can create hash for persistence of task data.\n        \"\"\"\n        new_tasks: Dict[str, Task] = {}\n\n        def _get_task(_task_name, _task):\n            if _task.fullname in new_tasks:\n                if _task_name not in _task.fullname:\n                    # this is for support of multiple names (through different namespaces paths) of one task\n                    new_tasks[_task_name] = new_tasks[_task.fullname]\n                return new_tasks[_task.fullname]\n            # this triggers recursion\n            input_tasks = {n: _get_task(n, t) for n, t in _task.input_tasks.items() if isinstance(t, Task)}\n            config = TaskParameterConfig(_task, input_tasks)\n            new_task = self._create_task(_task.__class__, config, task_registry)\n            new_tasks[_task.fullname] = new_task\n            return new_task\n\n        for task_name, task in tasks.items():\n            _get_task(task_name, task)\n\n        return new_tasks\n\n    @staticmethod\n    def _create_task(task_class: Type[Task], config: Config, task_registry: Dict = None):\n\"\"\"\n        Instantiate task object and save to task registry.\n        If task is in registry already, return it.\n        \"\"\"\n        task = task_class(config)\n        task.logger.addHandler(Chain.log_handler)\n\n        if isinstance(config, TaskParameterConfig):\n            key = task.slugname, task.name_for_persistence\n        else:\n            key = task.slugname, config.repr_name_without_namespace\n        if task_registry and key in task_registry:\n            del task\n            return task_registry[key]\n        if task_registry is not None:\n            task_registry[key] = task\n        return task\n\n    @staticmethod\n    def _expand_tasks(input_tasks: List[Union[str, Type[Task]]], tasks: Iterable[str]) -&gt; List[Union[str, Type[Task]]]:\n\"\"\"Expand input tasks definition starting with `~` to all matching tasks\"\"\"\n        expanded_tasks = []\n        for input_task in input_tasks:\n            if type(input_task) is not str or not input_task.startswith('~'):\n                expanded_tasks.append(input_task)\n            else:\n                for task_name in tasks:\n                    if re.match(input_task[1:], task_name):\n                        expanded_tasks.append(task_name)\n        return expanded_tasks\n\n    @staticmethod\n    def _process_dependencies(tasks: Dict[str, Task]):\n\"\"\" Process input tasks and inject input task object to tasks. \"\"\"\n        for task_name, task in tasks.items():\n            input_tasks = InputTasks()\n            for input_task in chain(Chain._expand_tasks(task.meta.get('input_tasks', []), tasks), task.meta.get('parameters', [])):\n                if isinstance(input_task, AbstractParameter):\n                    if not isinstance(input_task, InputTaskParameter):\n                        continue\n                    assert input_task.dont_persist_default_value\n                    assert not input_task.ignore_persistence\n                    required, default = input_task.required, input_task.default\n                    input_task = input_task.task_identifier\n                else:\n                    required, default = True, InputTaskParameter.NO_DEFAULT\n                if type(input_task) is str:\n                    input_task_name = input_task\n                else:  # for reference by class\n                    input_task_name = input_task.slugname\n                if type(input_task_name) is str and task.get_config().namespace and not input_task_name.startswith(task.get_config().namespace):\n                    input_task_name = f'{task.get_config().namespace}::{input_task_name}'     # add current config to reference\n                if input_task_name in input_tasks.keys():\n                    raise ValueError(f'Multiple input tasks with same name `{input_task_name}`')\n                try:\n                    found_name = _find_task_full_name(input_task_name, tasks, determine_namespace=False)\n                    if type(input_task) is str:\n                        input_task_name = found_name\n                except KeyError:\n                    if not required:\n                        input_tasks[input_task_name] = default\n                        continue\n                    raise ValueError(f'Input task `{input_task_name}` of task `{task}` not found')\n                input_tasks[input_task_name] = tasks[input_task_name]\n            task.set_input_tasks(input_tasks)\n\n    def _build_graph(self):\n\"\"\" Go through task and their input tasks and build nx.DiGraph\"\"\"\n        self.graph = G = nx.DiGraph()\n        G.add_nodes_from(self.tasks.values())\n\n        for task in self.tasks.values():\n            for input_task in task.input_tasks.values():\n                if not isinstance(input_task, Task):\n                    continue\n                G.add_edge(input_task, task)\n\n        if not nx.is_directed_acyclic_graph(G):\n            raise ValueError('Chain is not acyclic')\n\n    def _init_objects(self):\n        for config in self._configs.values():\n            for obj in config.data.values():\n                if isinstance(obj, ChainObject):\n                    obj.init_chain(self)\n\n    def get_task(self, task: Union[str, Task]) -&gt; Task:\n        if isinstance(task, Task):\n            return task\n        if task not in self:\n            raise ValueError(f'Task `{task}` not found')\n        return self.get(task)\n\n    def is_task_dependent_on(self, task: Union[str, Task], dependency_task: Union[str, Task]) -&gt; bool:\n\"\"\" Check whether a task is dependant on dependency task. \"\"\"\n        task = self.get_task(task)\n        dependency_task = self.get_task(dependency_task)\n\n        return nx.has_path(self.graph, dependency_task, task)\n\n    def dependent_tasks(self, task: Union[str, Task], include_self: bool = False) -&gt; Set[Task]:\n\"\"\" Get all tasks which depend ald given task. \"\"\"\n        task = self.get_task(task)\n        descendants = nx.descendants(self.graph, task)\n        if include_self:\n            descendants.add(task)\n        return descendants\n\n    def required_tasks(self, task: Union[str, Task], include_self: bool = False) -&gt; Set[Task]:\n\"\"\" Get all task which are required fot given task. \"\"\"\n        task = self.get_task(task)\n        ancestors = nx.ancestors(self.graph, task)\n        if include_self:\n            ancestors.add(task)\n        return ancestors\n\n    def force(self, tasks: Union[str, Task, Iterable[Union[str, Task]]], recompute=False, delete_data=False):\n\"\"\"\n        Force recomputation of of given tasks and all dependant tasks.\n        If either additional argument is used, recomputation must be done manually,\n        e.g. by calling `chain.my_task.value` for each task.\n\n        Args:\n            tasks: task as objects or names or list of them\n            recompute: automatically recompute all forced tasks\n            delete_data: also delete persisted data of forced tasks\n        \"\"\"\n        if type(tasks) is str or isinstance(tasks, Task):\n            tasks = [tasks]\n        forced_tasks = set()\n        for task in tasks:\n            forced_tasks |= self.dependent_tasks(task, include_self=True)\n\n        for task in forced_tasks:\n            task.force(delete_data=delete_data)\n\n        if recompute:\n            for task in list(forced_tasks)[::-1]:\n                _ = task.value\n\n    @property\n    def fullname(self):\n        return self._base_config.name\n\n    def draw(self, groups_to_show=None):\n\"\"\"\n        Draw graph of tasks. Color is based on tasks' group.\n        Border is based on data state:\n\n        - **none** - is not persisting data (`InMemoryData`)\n        - **dashed** - data not computed\n        - **solid** - data computed\n\n        Args:\n            groups_to_show (str or list[str]): limit drawn tasks to given groups and their neighbours\n        \"\"\"\n        import graphviz as gv\n        import seaborn as sns\n\n        groups_to_show = list_or_str_to_list(groups_to_show)\n\n        node_attr = {'shape': 'box', 'width': '2'}\n        graph_attr = {'splines': 'ortho'}\n        edge_attr = {}\n\n        groups = list({(n.get_config().namespace, n.group) for n in self.graph.nodes})\n        colors = sns.color_palette('pastel', len(groups)).as_hex()\n\n        G = gv.Digraph(\n            format='png',\n            engine='dot',\n            graph_attr=graph_attr,\n            node_attr=node_attr,\n            edge_attr=edge_attr\n        )\n\n        def _is_node_in_groups(node):\n            if not groups_to_show:\n                return True\n            return node.group in groups_to_show\n\n        nodes = set()\n        for edge in self.graph.edges:\n            if _is_node_in_groups(edge[0]) or _is_node_in_groups(edge[1]):\n                nodes.add(edge[0])\n                nodes.add(edge[1])\n\n        def _get_slugname(task: Task):\n            return f'{task.slugname.split(\":\")[-1]}#{task.get_config().get_name_for_persistence(task)}'\n\n        for node in nodes:\n            color = colors[groups.index((node.get_config().namespace, node.group))]\n            style = ['filled']\n            if not (node.has_data or issubclass(node.data_class, InMemoryData)):\n                style.append('dashed')\n            attrs = {\n                'label': f\"&lt;&lt;FONT POINT-SIZE='10'&gt;{':'.join(node.fullname.split(':')[:-1])}&lt;/FONT&gt; &lt;BR/&gt; {node.fullname.split(':')[-1]}&gt;\",\n                'fillcolor': color,\n                'color': color if issubclass(node.data_class, InMemoryData) else 'black' ,\n                'style': ','.join(style),\n            }\n            if not _is_node_in_groups(node):\n                attrs['shape'] = 'note'\n\n            G.node(\n                _get_slugname(node),\n                **attrs,\n            )\n\n        for edge in self.graph.edges:\n            if edge[0] in nodes and edge[1] in nodes:\n                G.edge(_get_slugname(edge[0]), _get_slugname(edge[1]))\n        return G\n\n    def create_readable_filenames(self, groups=None, name=None, verbose=False, keep_existing=False):\n\"\"\"\n        Create human readable symlink to data of tasks in the chain.\n        Symlink is in same directory as data, i.e. in directory with all task's data.\n        Name of link is taken from first available in order:\n\n        - this method's parameter\n        - task's config, parameter `human_readable_data_name`\n        - name of task's config\n\n        Args:\n            groups (optional): name of group or list of names of groups, for which should be symlinks created\n            name (optional): name of links\n            verbose:\n            keep_existing: rewrite link if link already exists\n        \"\"\"\n        symlink_actions = defaultdict(list)\n        groups = list_or_str_to_list(groups)\n\n        for task_name, task in self.tasks.items():\n            if groups is not None and task.group not in groups:\n                continue\n            if not task.has_data:\n                continue\n            an, n, sp = self._create_softlink_to_task_data(task, name, keep_existing=keep_existing)\n            symlink_actions[n].append((an, sp))\n\n        if verbose:\n            for name, actions in symlink_actions.items():\n                print(f'{name}')\n                for action_name, symlink_path in actions:\n                    print(f'   {action_name} link to {symlink_path}')\n\n    def _create_softlink_to_task_data(self, task, name=None, keep_existing=False):\n        if name is None:\n            config = task.get_config().get_original_config()\n            if 'human_readable_data_name' in config:\n                name = config['human_readable_data_name']\n            else:\n                name = config.name\n\n        symlink_path = task.path / f'{name}{task.data_path.suffix}'\n\n        action_name = 'keep existing'\n        if symlink_path.exists() and not keep_existing:\n            symlink_path.unlink()\n            action_name = 'rewriting'\n        if not symlink_path.exists():\n            symlink_path.symlink_to(task.data_path.relative_to(symlink_path.parent), task.data_path.is_dir())\n            if action_name != 'rewriting':\n                action_name = 'creating'\n        return action_name, name, symlink_path\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.tasks_df","title":"<code>tasks_df: pd.DataFrame</code>  <code>property</code>","text":"<p>Dataframe with rows ass all tasks in chain.</p>"},{"location":"code/chain/#taskchain.chain.Chain.set_log_level","title":"<code>set_log_level(level)</code>  <code>classmethod</code>","text":"<p>Set log level to log handler responsible for console output of task loggers.</p> Source code in <code>taskchain/chain.py</code> <pre><code>@classmethod\ndef set_log_level(cls, level):\n\"\"\" Set log level to log handler responsible for console output of task loggers. \"\"\"\n    Chain.log_handler.setLevel(level)\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Get task by name in dict-like fashion.</p> Source code in <code>taskchain/chain.py</code> <pre><code>def __getitem__(self, item):\n\"\"\" Get task by name in dict-like fashion. \"\"\"\n    return self.get(item)\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.__getattr__","title":"<code>__getattr__(item)</code>","text":"<p>Get task by name as atribute.</p> Source code in <code>taskchain/chain.py</code> <pre><code>def __getattr__(self, item):\n\"\"\" Get task by name as atribute. \"\"\"\n    if item in self:\n        return self.get(item)\n    return self.__getattribute__(item)\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.get","title":"<code>get(item, default=None)</code>","text":"<p>Get task by name.</p> Source code in <code>taskchain/chain.py</code> <pre><code>def get(self, item, default=None):\n\"\"\" Get task by name. \"\"\"\n    if default is not None:\n        raise ValueError('Default task is not allowed')\n    return self.tasks.get(_find_task_full_name(item, self.tasks.keys()))\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.is_task_dependent_on","title":"<code>is_task_dependent_on(task, dependency_task)</code>","text":"<p>Check whether a task is dependant on dependency task.</p> Source code in <code>taskchain/chain.py</code> <pre><code>def is_task_dependent_on(self, task: Union[str, Task], dependency_task: Union[str, Task]) -&gt; bool:\n\"\"\" Check whether a task is dependant on dependency task. \"\"\"\n    task = self.get_task(task)\n    dependency_task = self.get_task(dependency_task)\n\n    return nx.has_path(self.graph, dependency_task, task)\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.dependent_tasks","title":"<code>dependent_tasks(task, include_self=False)</code>","text":"<p>Get all tasks which depend ald given task.</p> Source code in <code>taskchain/chain.py</code> <pre><code>def dependent_tasks(self, task: Union[str, Task], include_self: bool = False) -&gt; Set[Task]:\n\"\"\" Get all tasks which depend ald given task. \"\"\"\n    task = self.get_task(task)\n    descendants = nx.descendants(self.graph, task)\n    if include_self:\n        descendants.add(task)\n    return descendants\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.required_tasks","title":"<code>required_tasks(task, include_self=False)</code>","text":"<p>Get all task which are required fot given task.</p> Source code in <code>taskchain/chain.py</code> <pre><code>def required_tasks(self, task: Union[str, Task], include_self: bool = False) -&gt; Set[Task]:\n\"\"\" Get all task which are required fot given task. \"\"\"\n    task = self.get_task(task)\n    ancestors = nx.ancestors(self.graph, task)\n    if include_self:\n        ancestors.add(task)\n    return ancestors\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.force","title":"<code>force(tasks, recompute=False, delete_data=False)</code>","text":"<p>Force recomputation of of given tasks and all dependant tasks. If either additional argument is used, recomputation must be done manually, e.g. by calling <code>chain.my_task.value</code> for each task.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Union[str, Task, Iterable[Union[str, Task]]]</code> <p>task as objects or names or list of them</p> required <code>recompute</code> <p>automatically recompute all forced tasks</p> <code>False</code> <code>delete_data</code> <p>also delete persisted data of forced tasks</p> <code>False</code> Source code in <code>taskchain/chain.py</code> <pre><code>def force(self, tasks: Union[str, Task, Iterable[Union[str, Task]]], recompute=False, delete_data=False):\n\"\"\"\n    Force recomputation of of given tasks and all dependant tasks.\n    If either additional argument is used, recomputation must be done manually,\n    e.g. by calling `chain.my_task.value` for each task.\n\n    Args:\n        tasks: task as objects or names or list of them\n        recompute: automatically recompute all forced tasks\n        delete_data: also delete persisted data of forced tasks\n    \"\"\"\n    if type(tasks) is str or isinstance(tasks, Task):\n        tasks = [tasks]\n    forced_tasks = set()\n    for task in tasks:\n        forced_tasks |= self.dependent_tasks(task, include_self=True)\n\n    for task in forced_tasks:\n        task.force(delete_data=delete_data)\n\n    if recompute:\n        for task in list(forced_tasks)[::-1]:\n            _ = task.value\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.draw","title":"<code>draw(groups_to_show=None)</code>","text":"<p>Draw graph of tasks. Color is based on tasks' group. Border is based on data state:</p> <ul> <li>none - is not persisting data (<code>InMemoryData</code>)</li> <li>dashed - data not computed</li> <li>solid - data computed</li> </ul> <p>Parameters:</p> Name Type Description Default <code>groups_to_show</code> <code>str or list[str]</code> <p>limit drawn tasks to given groups and their neighbours</p> <code>None</code> Source code in <code>taskchain/chain.py</code> <pre><code>def draw(self, groups_to_show=None):\n\"\"\"\n    Draw graph of tasks. Color is based on tasks' group.\n    Border is based on data state:\n\n    - **none** - is not persisting data (`InMemoryData`)\n    - **dashed** - data not computed\n    - **solid** - data computed\n\n    Args:\n        groups_to_show (str or list[str]): limit drawn tasks to given groups and their neighbours\n    \"\"\"\n    import graphviz as gv\n    import seaborn as sns\n\n    groups_to_show = list_or_str_to_list(groups_to_show)\n\n    node_attr = {'shape': 'box', 'width': '2'}\n    graph_attr = {'splines': 'ortho'}\n    edge_attr = {}\n\n    groups = list({(n.get_config().namespace, n.group) for n in self.graph.nodes})\n    colors = sns.color_palette('pastel', len(groups)).as_hex()\n\n    G = gv.Digraph(\n        format='png',\n        engine='dot',\n        graph_attr=graph_attr,\n        node_attr=node_attr,\n        edge_attr=edge_attr\n    )\n\n    def _is_node_in_groups(node):\n        if not groups_to_show:\n            return True\n        return node.group in groups_to_show\n\n    nodes = set()\n    for edge in self.graph.edges:\n        if _is_node_in_groups(edge[0]) or _is_node_in_groups(edge[1]):\n            nodes.add(edge[0])\n            nodes.add(edge[1])\n\n    def _get_slugname(task: Task):\n        return f'{task.slugname.split(\":\")[-1]}#{task.get_config().get_name_for_persistence(task)}'\n\n    for node in nodes:\n        color = colors[groups.index((node.get_config().namespace, node.group))]\n        style = ['filled']\n        if not (node.has_data or issubclass(node.data_class, InMemoryData)):\n            style.append('dashed')\n        attrs = {\n            'label': f\"&lt;&lt;FONT POINT-SIZE='10'&gt;{':'.join(node.fullname.split(':')[:-1])}&lt;/FONT&gt; &lt;BR/&gt; {node.fullname.split(':')[-1]}&gt;\",\n            'fillcolor': color,\n            'color': color if issubclass(node.data_class, InMemoryData) else 'black' ,\n            'style': ','.join(style),\n        }\n        if not _is_node_in_groups(node):\n            attrs['shape'] = 'note'\n\n        G.node(\n            _get_slugname(node),\n            **attrs,\n        )\n\n    for edge in self.graph.edges:\n        if edge[0] in nodes and edge[1] in nodes:\n            G.edge(_get_slugname(edge[0]), _get_slugname(edge[1]))\n    return G\n</code></pre>"},{"location":"code/chain/#taskchain.chain.Chain.create_readable_filenames","title":"<code>create_readable_filenames(groups=None, name=None, verbose=False, keep_existing=False)</code>","text":"<p>Create human readable symlink to data of tasks in the chain. Symlink is in same directory as data, i.e. in directory with all task's data. Name of link is taken from first available in order:</p> <ul> <li>this method's parameter</li> <li>task's config, parameter <code>human_readable_data_name</code></li> <li>name of task's config</li> </ul> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>optional</code> <p>name of group or list of names of groups, for which should be symlinks created</p> <code>None</code> <code>name</code> <code>optional</code> <p>name of links</p> <code>None</code> <code>verbose</code> <code>False</code> <code>keep_existing</code> <p>rewrite link if link already exists</p> <code>False</code> Source code in <code>taskchain/chain.py</code> <pre><code>def create_readable_filenames(self, groups=None, name=None, verbose=False, keep_existing=False):\n\"\"\"\n    Create human readable symlink to data of tasks in the chain.\n    Symlink is in same directory as data, i.e. in directory with all task's data.\n    Name of link is taken from first available in order:\n\n    - this method's parameter\n    - task's config, parameter `human_readable_data_name`\n    - name of task's config\n\n    Args:\n        groups (optional): name of group or list of names of groups, for which should be symlinks created\n        name (optional): name of links\n        verbose:\n        keep_existing: rewrite link if link already exists\n    \"\"\"\n    symlink_actions = defaultdict(list)\n    groups = list_or_str_to_list(groups)\n\n    for task_name, task in self.tasks.items():\n        if groups is not None and task.group not in groups:\n            continue\n        if not task.has_data:\n            continue\n        an, n, sp = self._create_softlink_to_task_data(task, name, keep_existing=keep_existing)\n        symlink_actions[n].append((an, sp))\n\n    if verbose:\n        for name, actions in symlink_actions.items():\n            print(f'{name}')\n            for action_name, symlink_path in actions:\n                print(f'   {action_name} link to {symlink_path}')\n</code></pre>"},{"location":"code/chain/#taskchain.chain.TaskParameterConfig","title":"<code>TaskParameterConfig</code>","text":"<p>         Bases: <code>Config</code></p> <p>Helper config used in parameter mode.</p> <p>It takes</p> <ul> <li>instance of Task and create config with only parameters which are used by this task</li> <li>input tasks (already with TaskParameterConfig)</li> </ul> <p>From this data creates unique hash used in persistence</p> <p>Note: These configs creates a kind of blockchain. Each config is block, parameters are content of blocks and input_tasks are dependencies between blocks. Change in one config invalidates all dependant configs, which is property desired and required for correct functionality of TaskChains data persistence.</p> Source code in <code>taskchain/chain.py</code> <pre><code>class TaskParameterConfig(Config):\n\"\"\"\n    Helper config used in parameter mode.\n\n    It takes\n\n     - instance of Task and create config with only parameters which are used by this task\n     - input tasks (already with TaskParameterConfig)\n\n    From this data creates unique hash used in persistence\n\n    Note: These configs creates a kind of blockchain. Each config is block,\n    parameters are content of blocks and input_tasks are dependencies between blocks.\n    Change in one config invalidates all dependant configs, which is property desired\n    and required for correct functionality of TaskChains data persistence.\n    \"\"\"\n    def __init__(self, original_task: Task, input_tasks: Dict[str, Task]):\n        super(Config, self).__init__()\n\n        self.original_config = original_config = original_task.get_config()\n        self.base_dir = original_config.base_dir\n        self.namespace = original_config.namespace\n        self.global_vars = original_config.global_vars\n        self.context = original_config.context\n        self._name = f'{original_config.name}/{original_task}'\n        self._part = None\n\n        self._data = {}\n        for parameter in original_task.parameters.values():\n            if parameter.name_in_config in original_config:\n                self._data[parameter.name_in_config] = original_config[parameter.name_in_config]\n\n        self.input_tasks = {name: task.get_config().get_name_for_persistence(task) for name, task in input_tasks.items() if isinstance(task, Task)}\n\n    def get_name_for_persistence(self, task: Task) -&gt; str:\n        def _get_input_task_repr(_name, _task):\n            if outer_namespace := task.get_config().namespace:\n                # remove namespace of this task from task name\n                assert _name.startswith(outer_namespace)\n                _name = _name[len(outer_namespace) + 2:]\n            return f'{_name}={_task}'\n\n        parameter_repr = task.parameters.repr\n        input_tasks_repr = '###'.join(_get_input_task_repr(n, it) for n, it in sorted(self.input_tasks.items()))\n        return sha256(f'{parameter_repr}$$${input_tasks_repr}'.encode()).hexdigest()[:32]\n\n    @property\n    def repr_name(self) -&gt; str:\n        return self.name\n\n    @property\n    def repr_name_without_namespace(self):\n        return self.repr_name\n\n    def get_original_config(self):\n        return self.original_config\n</code></pre>"},{"location":"code/chain/#taskchain.chain.MultiChain","title":"<code>MultiChain</code>","text":"<p>Hold multiple chains which share task object, i.e. it can be more memory efficient then dict of chains. Otherwise behaves as dict of chains.</p> Source code in <code>taskchain/chain.py</code> <pre><code>class MultiChain:\n\"\"\"\n    Hold multiple chains which share task object,\n    i.e. it can be more memory efficient then dict of chains.\n    Otherwise behaves as dict of chains.\n    \"\"\"\n\n    logger = logging.getLogger('tasks_chain')\n\n    @classmethod\n    def from_dir(cls, data_dir: Path, dir_path: Path, **kwargs) -&gt; 'MultiChain':\n\"\"\"\n        Create MultiConfig from directory of configs.\n\n        Args:\n            data_dir: tasks data persistence path\n            dir_path: directory with configs\n            **kwargs: other arguments passed to Config, e.g. global_vars\n\n        Returns:\n            MultiChain based on all configs in dir\n        \"\"\"\n        configs = []\n        for config_file in dir_path.iterdir():\n            configs.append(\n                Config(data_dir, config_file, **kwargs)\n            )\n        return MultiChain(configs)\n\n    def __init__(self, configs: Sequence[Config], parameter_mode: bool = True):\n\"\"\"\n        Args:\n            configs: list of Config objects from which Chains are created.\n            parameter_mode:\n        \"\"\"\n        self._tasks: Dict[Tuple[str, str], Task] = {}\n        self.chains: Dict[str, Chain] = {}\n        self._base_configs = configs\n        self.parameter_mode = parameter_mode\n\n        self._prepare()\n\n    def _prepare(self):\n        for config in self._base_configs:\n            assert config.name not in self.chains, f'Multiple configs with same name `{config.name}`'\n            self.chains[config.name] = Chain(config, self._tasks, parameter_mode=self.parameter_mode)\n\n    def __getitem__(self, chain_name: str):\n        if chain_name not in self.chains:\n            raise ValueError(f'Unknown chain name `{chain_name}`')\n        return self.chains[chain_name]\n\n    def force(self, tasks: Union[str, Iterable[Union[str, Task]]], **kwargs):\n\"\"\" Pass force to all chains. \"\"\"\n        for chain in self.chains.values():\n            chain.force(tasks, **kwargs)\n\n    def latest(self, chain_name: str=None):\n\"\"\" Get latest chain based on name (alphabetically last)\n\n        Args:\n            chain_name: return latest chain from chain with name containing `chain_name`\n        \"\"\"\n        for fullname, chain in sorted(self.chains.items(), reverse=True):\n            if chain_name is None or chain_name in fullname:\n                return chain\n\n    @classmethod\n    def set_log_level(cls, level):\n\"\"\" Pass log level to all chains. \"\"\"\n        Chain.log_handler.setLevel(level)\n\n    def items(self):\n        yield from sorted(self.chains.items())\n\n    def keys(self):\n        yield from sorted(self.chains.keys())\n\n    def values(self):\n        for _, val in sorted(self.chains.items()):\n            yield val\n\n    def __len__(self):\n        return len(self.chains)\n\n    def __repr__(self):\n        return 'multichain:\\n - ' + '\\n - '.join(map(repr, self.values()))\n\n    def __str__(self):\n        return '\\n'.join(fullname for fullname in self.keys())\n</code></pre>"},{"location":"code/chain/#taskchain.chain.MultiChain.from_dir","title":"<code>from_dir(data_dir, dir_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create MultiConfig from directory of configs.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>tasks data persistence path</p> required <code>dir_path</code> <code>Path</code> <p>directory with configs</p> required <code>**kwargs</code> <p>other arguments passed to Config, e.g. global_vars</p> <code>{}</code> <p>Returns:</p> Type Description <code>MultiChain</code> <p>MultiChain based on all configs in dir</p> Source code in <code>taskchain/chain.py</code> <pre><code>@classmethod\ndef from_dir(cls, data_dir: Path, dir_path: Path, **kwargs) -&gt; 'MultiChain':\n\"\"\"\n    Create MultiConfig from directory of configs.\n\n    Args:\n        data_dir: tasks data persistence path\n        dir_path: directory with configs\n        **kwargs: other arguments passed to Config, e.g. global_vars\n\n    Returns:\n        MultiChain based on all configs in dir\n    \"\"\"\n    configs = []\n    for config_file in dir_path.iterdir():\n        configs.append(\n            Config(data_dir, config_file, **kwargs)\n        )\n    return MultiChain(configs)\n</code></pre>"},{"location":"code/chain/#taskchain.chain.MultiChain.__init__","title":"<code>__init__(configs, parameter_mode=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>configs</code> <code>Sequence[Config]</code> <p>list of Config objects from which Chains are created.</p> required <code>parameter_mode</code> <code>bool</code> <code>True</code> Source code in <code>taskchain/chain.py</code> <pre><code>def __init__(self, configs: Sequence[Config], parameter_mode: bool = True):\n\"\"\"\n    Args:\n        configs: list of Config objects from which Chains are created.\n        parameter_mode:\n    \"\"\"\n    self._tasks: Dict[Tuple[str, str], Task] = {}\n    self.chains: Dict[str, Chain] = {}\n    self._base_configs = configs\n    self.parameter_mode = parameter_mode\n\n    self._prepare()\n</code></pre>"},{"location":"code/chain/#taskchain.chain.MultiChain.force","title":"<code>force(tasks, **kwargs)</code>","text":"<p>Pass force to all chains.</p> Source code in <code>taskchain/chain.py</code> <pre><code>def force(self, tasks: Union[str, Iterable[Union[str, Task]]], **kwargs):\n\"\"\" Pass force to all chains. \"\"\"\n    for chain in self.chains.values():\n        chain.force(tasks, **kwargs)\n</code></pre>"},{"location":"code/chain/#taskchain.chain.MultiChain.latest","title":"<code>latest(chain_name=None)</code>","text":"<p>Get latest chain based on name (alphabetically last)</p> <p>Parameters:</p> Name Type Description Default <code>chain_name</code> <code>str</code> <p>return latest chain from chain with name containing <code>chain_name</code></p> <code>None</code> Source code in <code>taskchain/chain.py</code> <pre><code>def latest(self, chain_name: str=None):\n\"\"\" Get latest chain based on name (alphabetically last)\n\n    Args:\n        chain_name: return latest chain from chain with name containing `chain_name`\n    \"\"\"\n    for fullname, chain in sorted(self.chains.items(), reverse=True):\n        if chain_name is None or chain_name in fullname:\n            return chain\n</code></pre>"},{"location":"code/chain/#taskchain.chain.MultiChain.set_log_level","title":"<code>set_log_level(level)</code>  <code>classmethod</code>","text":"<p>Pass log level to all chains.</p> Source code in <code>taskchain/chain.py</code> <pre><code>@classmethod\ndef set_log_level(cls, level):\n\"\"\" Pass log level to all chains. \"\"\"\n    Chain.log_handler.setLevel(level)\n</code></pre>"},{"location":"code/config/","title":"Config","text":""},{"location":"code/config/#taskchain.config.Config","title":"<code>Config</code>","text":"<p>         Bases: <code>dict</code></p> <p>Object carrying parameters needed for task execution. Config also describe which tasks configures (<code>tasks</code> field) and and on which other configs depends (<code>uses</code> field). Thus, config carry all information needed to assemble task chain.</p> <p>Typical usage: <pre><code>chain = Config(task_data_dir, 'config.yaml').chain()\n</code></pre></p> Source code in <code>taskchain/config.py</code> <pre><code>class Config(dict):\n\"\"\"\n    Object carrying parameters needed for task execution.\n    Config also describe which tasks configures (`tasks` field) and\n    and on which other configs depends (`uses` field).\n    Thus, config carry all information needed to assemble task chain.\n\n    Typical usage:\n    ```python\n    chain = Config(task_data_dir, 'config.yaml').chain()\n    ```\n    \"\"\"\n\n    RESERVED_PARAMETER_NAMES = [\n        'tasks',\n        'excluded_tasks',\n        'uses',\n        'human_readable_data_name',\n        'configs',\n        'for_namespaces',\n        'main_part',\n    ]\n\n    def __init__(\n        self,\n        base_dir: Union[Path, str, None] = None,\n        filepath: Union[Path, str] = None,\n        global_vars: Union[Any, None] = None,\n        context: Union[None, dict, str, Path, Context, Iterable] = None,\n        name: str = None,\n        namespace: str = None,\n        data: Dict = None,\n        part: str = None,\n    ):\n\"\"\"\n\n        Args:\n            base_dir: dir with task data, required for task data persistence\n            filepath: json or yaml with config data\n            global_vars: data to fill placeholders inf config data such as `{DATA_DIR}`\n            context: config which amend or overwrite data of this config\n            name: specify name of config directly, required when not using filepath\n            namespace: used by chains, allow work with same tasks with multiple configs in one chain\n            data: alternative for `filepath`, inject data directly\n            part: for multi config files, name of file part\n        \"\"\"\n        super().__init__()\n\n        self.base_dir = base_dir\n        self._name = None\n        self.namespace = namespace\n        self._data = None\n        self.context = Context.prepare_context(context, global_vars=global_vars)\n        self.global_vars = global_vars\n        self._filepath = filepath\n        self._part = part\n\n        if filepath is not None:\n            if '#' in str(self._filepath):\n                self._filepath, self._part = str(self._filepath).split('#')\n            filepath = Path(self._filepath)\n            name_parts = filepath.name.split('.')\n            extension = name_parts[-1]\n            self._name = '.'.join(name_parts[:-1])\n            if extension == 'json':\n                self._data = json.load(filepath.open())\n            elif extension == 'yaml':\n                self._data = yaml.load(filepath.open(), Loader=yaml.Loader)\n            else:\n                raise ValueError(f'Unknown file extension for config file `{filepath}`')\n\n        if data is not None:\n            self._data = data\n        if name is not None:\n            self._name = name\n\n        self._prepare()\n\n    def _prepare(self, create_objects=True):\n        if self._data and 'configs' in self._data:\n            self._get_part()\n            self._update_uses()\n        if self.context is not None:\n            self.apply_context(self.context)\n        self._validate_data()\n        if self.global_vars is not None:\n            self.apply_global_vars(self.global_vars)\n        if create_objects:\n            self.prepare_objects()\n\n    def _get_part(self):\n        assert len(self._data) == 1, 'Multipart configs should contain only field `configs`'\n\n        if self._part:\n            try:\n                self._data = self._data['configs'][self._part]\n            except KeyError:\n                raise KeyError(f'Part `{self._part}` not found in config `{self}`')\n            return\n\n        assert (\n            len([c for c in self._data['configs'] if 'main_part' in c]) &lt; 2\n        ), f'More then one part of config `self` are marked as main'\n        for part_name, part in self._data['configs'].items():\n            if part.get('main_part', False):\n                self._data = part\n                self._part = part_name\n                return\n\n        raise KeyError(f'No part specified for multi config `{self}`')\n\n    def _update_uses(self):\n        if self._filepath is None or 'uses' not in self._data:\n            return\n        self._data['uses'] = list_or_str_to_list(self._data['uses'])\n        for i, use in enumerate(self._data['uses']):\n            if isinstance(use, str) and use.startswith('#'):\n                self._data['uses'][i] = str(self._filepath) + use\n\n    @property\n    def name(self) -&gt; str:\n        if self._name is None:\n            raise ValueError(f'Missing config name')\n        if self._part:\n            return f'{self._name}#{self._part}'\n        return self._name\n\n    def get_name_for_persistence(self, *args, **kwargs) -&gt; str:\n\"\"\"Used for creating filename in task data persistence, should uniquely define config\"\"\"\n        return self.name\n\n    @property\n    def fullname(self):\n\"\"\"Name with namespace\"\"\"\n        if self.namespace is None:\n            return f'{self.name}'\n        return f'{self.namespace}::{self.name}'\n\n    @property\n    def repr_name(self) -&gt; str:\n\"\"\"Should be unique representation of this config\"\"\"\n        if self._filepath:\n            if self.namespace is None:\n                name = str(self._filepath)\n            else:\n                name = f'{self.namespace}::{self._filepath}'\n            if self._part:\n                return f'{name}#{self._part}'\n            return name\n        return self.fullname\n\n    @property\n    def repr_name_without_namespace(self) -&gt; str:\n\"\"\"Unique representation of this config without namespace\"\"\"\n        return self.repr_name.split('::')[-1]\n\n    def __str__(self):\n        return self.fullname\n\n    def __repr__(self):\n        return f'&lt;config: {self}&gt;'\n\n    @property\n    def data(self):\n        if self._data is None:\n            raise ValueError(f'Data of config `{self}` not initialized')\n        return self._data\n\n    def __getitem__(self, item):\n        return self.data[item]\n\n    def __getattr__(self, item):\n        if item in self:\n            return self.data[item]\n        return self.__getattribute__(item)\n\n    def get(self, item, default=None):\n        return self.data.get(item, default)\n\n    def __contains__(self, item):\n        return item in self.data\n\n    def apply_context(self, context: Context):\n\"\"\"Amend or rewrite data of config by data from context\"\"\"\n        self._data.update(deepcopy(context.data))\n        if self.namespace:\n            for namespace, data in context.for_namespaces.items():\n                if self.namespace == namespace:\n                    self._data.update(deepcopy(data))\n\n    def _validate_data(self):\n\"\"\"Check correct format of data\"\"\"\n        if self._data is None:\n            return\n\n        data = self._data\n        uses = data.get('uses', [])\n        if not (isinstance(uses, Iterable) or isinstance(uses, str)):\n            raise ValueError(f'`uses` of config `{self}` have to be list or str')\n\n        tasks = data.get('tasks', [])\n        if not (isinstance(tasks, Iterable) or isinstance(tasks, str)):\n            raise ValueError(f'`tasks` of config `{self}` have to list or str')\n\n    def apply_global_vars(self, global_vars):\n        search_and_replace_placeholders(self._data, global_vars)\n\n    def prepare_objects(self):\n\"\"\"Instantiate objects described in config\"\"\"\n        if self._data is None:\n            return\n\n        def _instancelize_clazz(clazz, args, kwargs):\n            obj = instantiate_clazz(clazz, args, kwargs)\n            if not isinstance(obj, ParameterObject):\n                LOGGER.warning(f'Object `{obj}` in config `{self}` is not instance of ParameterObject')\n            if not hasattr(obj, 'repr'):\n                raise Exception(f'Object `{obj}` does not implement `repr` property')\n            return obj\n\n        for key, value in self._data.items():\n            self._data[key] = find_and_instantiate_clazz(value, instancelize_clazz_fce=_instancelize_clazz)\n\n    def chain(self, parameter_mode=True, **kwargs):\n\"\"\"Create chain from this config\"\"\"\n        from .chain import Chain\n\n        return Chain(self, parameter_mode=parameter_mode, **kwargs)\n\n    def get_original_config(self):\n\"\"\"Get self of config from which this one is derived\"\"\"\n        return self\n</code></pre>"},{"location":"code/config/#taskchain.config.Config.fullname","title":"<code>fullname</code>  <code>property</code>","text":"<p>Name with namespace</p>"},{"location":"code/config/#taskchain.config.Config.repr_name","title":"<code>repr_name: str</code>  <code>property</code>","text":"<p>Should be unique representation of this config</p>"},{"location":"code/config/#taskchain.config.Config.repr_name_without_namespace","title":"<code>repr_name_without_namespace: str</code>  <code>property</code>","text":"<p>Unique representation of this config without namespace</p>"},{"location":"code/config/#taskchain.config.Config.__init__","title":"<code>__init__(base_dir=None, filepath=None, global_vars=None, context=None, name=None, namespace=None, data=None, part=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>Union[Path, str, None]</code> <p>dir with task data, required for task data persistence</p> <code>None</code> <code>filepath</code> <code>Union[Path, str]</code> <p>json or yaml with config data</p> <code>None</code> <code>global_vars</code> <code>Union[Any, None]</code> <p>data to fill placeholders inf config data such as <code>{DATA_DIR}</code></p> <code>None</code> <code>context</code> <code>Union[None, dict, str, Path, Context, Iterable]</code> <p>config which amend or overwrite data of this config</p> <code>None</code> <code>name</code> <code>str</code> <p>specify name of config directly, required when not using filepath</p> <code>None</code> <code>namespace</code> <code>str</code> <p>used by chains, allow work with same tasks with multiple configs in one chain</p> <code>None</code> <code>data</code> <code>Dict</code> <p>alternative for <code>filepath</code>, inject data directly</p> <code>None</code> <code>part</code> <code>str</code> <p>for multi config files, name of file part</p> <code>None</code> Source code in <code>taskchain/config.py</code> <pre><code>def __init__(\n    self,\n    base_dir: Union[Path, str, None] = None,\n    filepath: Union[Path, str] = None,\n    global_vars: Union[Any, None] = None,\n    context: Union[None, dict, str, Path, Context, Iterable] = None,\n    name: str = None,\n    namespace: str = None,\n    data: Dict = None,\n    part: str = None,\n):\n\"\"\"\n\n    Args:\n        base_dir: dir with task data, required for task data persistence\n        filepath: json or yaml with config data\n        global_vars: data to fill placeholders inf config data such as `{DATA_DIR}`\n        context: config which amend or overwrite data of this config\n        name: specify name of config directly, required when not using filepath\n        namespace: used by chains, allow work with same tasks with multiple configs in one chain\n        data: alternative for `filepath`, inject data directly\n        part: for multi config files, name of file part\n    \"\"\"\n    super().__init__()\n\n    self.base_dir = base_dir\n    self._name = None\n    self.namespace = namespace\n    self._data = None\n    self.context = Context.prepare_context(context, global_vars=global_vars)\n    self.global_vars = global_vars\n    self._filepath = filepath\n    self._part = part\n\n    if filepath is not None:\n        if '#' in str(self._filepath):\n            self._filepath, self._part = str(self._filepath).split('#')\n        filepath = Path(self._filepath)\n        name_parts = filepath.name.split('.')\n        extension = name_parts[-1]\n        self._name = '.'.join(name_parts[:-1])\n        if extension == 'json':\n            self._data = json.load(filepath.open())\n        elif extension == 'yaml':\n            self._data = yaml.load(filepath.open(), Loader=yaml.Loader)\n        else:\n            raise ValueError(f'Unknown file extension for config file `{filepath}`')\n\n    if data is not None:\n        self._data = data\n    if name is not None:\n        self._name = name\n\n    self._prepare()\n</code></pre>"},{"location":"code/config/#taskchain.config.Config.get_name_for_persistence","title":"<code>get_name_for_persistence(*args, **kwargs)</code>","text":"<p>Used for creating filename in task data persistence, should uniquely define config</p> Source code in <code>taskchain/config.py</code> <pre><code>def get_name_for_persistence(self, *args, **kwargs) -&gt; str:\n\"\"\"Used for creating filename in task data persistence, should uniquely define config\"\"\"\n    return self.name\n</code></pre>"},{"location":"code/config/#taskchain.config.Config.apply_context","title":"<code>apply_context(context)</code>","text":"<p>Amend or rewrite data of config by data from context</p> Source code in <code>taskchain/config.py</code> <pre><code>def apply_context(self, context: Context):\n\"\"\"Amend or rewrite data of config by data from context\"\"\"\n    self._data.update(deepcopy(context.data))\n    if self.namespace:\n        for namespace, data in context.for_namespaces.items():\n            if self.namespace == namespace:\n                self._data.update(deepcopy(data))\n</code></pre>"},{"location":"code/config/#taskchain.config.Config.prepare_objects","title":"<code>prepare_objects()</code>","text":"<p>Instantiate objects described in config</p> Source code in <code>taskchain/config.py</code> <pre><code>def prepare_objects(self):\n\"\"\"Instantiate objects described in config\"\"\"\n    if self._data is None:\n        return\n\n    def _instancelize_clazz(clazz, args, kwargs):\n        obj = instantiate_clazz(clazz, args, kwargs)\n        if not isinstance(obj, ParameterObject):\n            LOGGER.warning(f'Object `{obj}` in config `{self}` is not instance of ParameterObject')\n        if not hasattr(obj, 'repr'):\n            raise Exception(f'Object `{obj}` does not implement `repr` property')\n        return obj\n\n    for key, value in self._data.items():\n        self._data[key] = find_and_instantiate_clazz(value, instancelize_clazz_fce=_instancelize_clazz)\n</code></pre>"},{"location":"code/config/#taskchain.config.Config.chain","title":"<code>chain(parameter_mode=True, **kwargs)</code>","text":"<p>Create chain from this config</p> Source code in <code>taskchain/config.py</code> <pre><code>def chain(self, parameter_mode=True, **kwargs):\n\"\"\"Create chain from this config\"\"\"\n    from .chain import Chain\n\n    return Chain(self, parameter_mode=parameter_mode, **kwargs)\n</code></pre>"},{"location":"code/config/#taskchain.config.Config.get_original_config","title":"<code>get_original_config()</code>","text":"<p>Get self of config from which this one is derived</p> Source code in <code>taskchain/config.py</code> <pre><code>def get_original_config(self):\n\"\"\"Get self of config from which this one is derived\"\"\"\n    return self\n</code></pre>"},{"location":"code/config/#taskchain.config.Context","title":"<code>Context</code>","text":"<p>         Bases: <code>Config</code></p> <p>Config intended for amend or rewrite other configs</p> Source code in <code>taskchain/config.py</code> <pre><code>class Context(Config):\n\"\"\"\n    Config intended for amend or rewrite other configs\n    \"\"\"\n\n    def __repr__(self):\n        return f'&lt;context: {self}&gt;'\n\n    @staticmethod\n    def prepare_context(\n        context_config: Union[None, dict, str, Path, Context, Iterable], namespace=None, global_vars=None\n    ) -&gt; Union[Context, None]:\n\"\"\"Helper function for instantiating Context from various sources\"\"\"\n        context = None\n        if context_config is None:\n            return\n        elif type(context_config) is str or isinstance(context_config, Path):\n            context = Context(filepath=context_config, namespace=namespace)\n        elif type(context_config) is dict:\n            value_reprs = [f'{k}:{v}' for k, v in sorted(context_config.items())]\n            context = Context(data=context_config, name=f'dict_context({\",\".join(value_reprs)})', namespace=namespace)\n        elif isinstance(context_config, Context):\n            context = context_config\n        elif isinstance(context_config, Iterable):\n            contexts = map(\n                partial(Context.prepare_context, namespace=namespace, global_vars=global_vars), context_config\n            )\n            context = Context.merge_contexts(contexts)\n\n        if context is None:\n            raise ValueError(f'Unknown context type `{type(context_config)}`')\n\n        current_context_data = context.for_namespaces[namespace] if namespace else context\n        if 'uses' not in current_context_data:\n            return context\n\n        if global_vars is not None:\n            search_and_replace_placeholders(current_context_data['uses'], global_vars)\n\n        contexts = [context]\n        for use in list_or_str_to_list(current_context_data['uses']):\n            if matched := re.match(r'(.*) as (.*)', use):\n                # uses context with namespace\n                filepath = matched[1]\n                sub_namespace = f'{context.namespace}::{matched[2]}' if context.namespace else matched[2]\n            else:\n                filepath = use\n                sub_namespace = context.namespace if context.namespace else None\n            contexts.append(Context.prepare_context(filepath, sub_namespace, global_vars=global_vars))\n        if namespace:\n            del context.for_namespaces[namespace]['uses']\n        else:\n            del context._data['uses']\n        return Context.prepare_context(contexts)\n\n    @staticmethod\n    def merge_contexts(contexts: Iterable[Context]) -&gt; Context:\n\"\"\"\n        Helper function for merging multiple Context to one\n\n        Later contexts have higher priority and rewrite data of earlier contexts if there is conflict in data.\n        \"\"\"\n        data = {}\n        names = []\n        for_namespaces = defaultdict(dict)\n        for context in contexts:\n            data.update(context.data)\n            names.append(context.name)\n            for namespace, values in context.for_namespaces.items():\n                for_namespaces[namespace].update(values)\n        data['for_namespaces'] = for_namespaces\n        return Context(data=data, name=';'.join(names))\n\n    def _prepare(self):\n        if 'for_namespaces' in self._data:\n            self.for_namespaces = self._data['for_namespaces']\n        else:\n            self.for_namespaces = {}\n\n        if self.namespace is not None:\n            self.for_namespaces = {f'{self.namespace}::{k}': v for k, v in self.for_namespaces.items()}\n            self.for_namespaces[self.namespace] = {\n                k: v for k, v in self._data.items() if k not in Context.RESERVED_PARAMETER_NAMES or k == 'uses'\n            }\n            self._data = {}\n        super()._prepare(create_objects=False)\n</code></pre>"},{"location":"code/config/#taskchain.config.Context.prepare_context","title":"<code>prepare_context(context_config, namespace=None, global_vars=None)</code>  <code>staticmethod</code>","text":"<p>Helper function for instantiating Context from various sources</p> Source code in <code>taskchain/config.py</code> <pre><code>@staticmethod\ndef prepare_context(\n    context_config: Union[None, dict, str, Path, Context, Iterable], namespace=None, global_vars=None\n) -&gt; Union[Context, None]:\n\"\"\"Helper function for instantiating Context from various sources\"\"\"\n    context = None\n    if context_config is None:\n        return\n    elif type(context_config) is str or isinstance(context_config, Path):\n        context = Context(filepath=context_config, namespace=namespace)\n    elif type(context_config) is dict:\n        value_reprs = [f'{k}:{v}' for k, v in sorted(context_config.items())]\n        context = Context(data=context_config, name=f'dict_context({\",\".join(value_reprs)})', namespace=namespace)\n    elif isinstance(context_config, Context):\n        context = context_config\n    elif isinstance(context_config, Iterable):\n        contexts = map(\n            partial(Context.prepare_context, namespace=namespace, global_vars=global_vars), context_config\n        )\n        context = Context.merge_contexts(contexts)\n\n    if context is None:\n        raise ValueError(f'Unknown context type `{type(context_config)}`')\n\n    current_context_data = context.for_namespaces[namespace] if namespace else context\n    if 'uses' not in current_context_data:\n        return context\n\n    if global_vars is not None:\n        search_and_replace_placeholders(current_context_data['uses'], global_vars)\n\n    contexts = [context]\n    for use in list_or_str_to_list(current_context_data['uses']):\n        if matched := re.match(r'(.*) as (.*)', use):\n            # uses context with namespace\n            filepath = matched[1]\n            sub_namespace = f'{context.namespace}::{matched[2]}' if context.namespace else matched[2]\n        else:\n            filepath = use\n            sub_namespace = context.namespace if context.namespace else None\n        contexts.append(Context.prepare_context(filepath, sub_namespace, global_vars=global_vars))\n    if namespace:\n        del context.for_namespaces[namespace]['uses']\n    else:\n        del context._data['uses']\n    return Context.prepare_context(contexts)\n</code></pre>"},{"location":"code/config/#taskchain.config.Context.merge_contexts","title":"<code>merge_contexts(contexts)</code>  <code>staticmethod</code>","text":"<p>Helper function for merging multiple Context to one</p> <p>Later contexts have higher priority and rewrite data of earlier contexts if there is conflict in data.</p> Source code in <code>taskchain/config.py</code> <pre><code>@staticmethod\ndef merge_contexts(contexts: Iterable[Context]) -&gt; Context:\n\"\"\"\n    Helper function for merging multiple Context to one\n\n    Later contexts have higher priority and rewrite data of earlier contexts if there is conflict in data.\n    \"\"\"\n    data = {}\n    names = []\n    for_namespaces = defaultdict(dict)\n    for context in contexts:\n        data.update(context.data)\n        names.append(context.name)\n        for namespace, values in context.for_namespaces.items():\n            for_namespaces[namespace].update(values)\n    data['for_namespaces'] = for_namespaces\n    return Context(data=data, name=';'.join(names))\n</code></pre>"},{"location":"code/parameter/","title":"Parameter","text":""},{"location":"code/parameter/#taskchain.parameter.AbstractParameter","title":"<code>AbstractParameter</code>","text":"<p>         Bases: <code>abc.ABC</code></p> Source code in <code>taskchain/parameter.py</code> <pre><code>class AbstractParameter(abc.ABC):\n\n    NO_DEFAULT = NO_DEFAULT\n    NO_VALUE = NO_VALUE\n\n    def __init__(self,\n                 default: Any = NO_DEFAULT,\n                 ignore_persistence: bool = False,\n                 dont_persist_default_value: bool = True,\n                 ):\n\"\"\"\n        Args:\n            default: value used if not provided in config, default to NO_DEFAULT meaning that param is required\n            ignore_persistence: do not use this parameter in persistence, useful params without influence on output\n            dont_persist_default_value: if value of parameter is same as default, do not use it in persistence,\n                useful for adding new parameters without recomputation of data\n        \"\"\"\n        self.default = default\n        self.ignore_persistence = ignore_persistence\n        self.dont_persist_default_value = dont_persist_default_value\n\n    @property\n    def required(self) -&gt; bool:\n        return self.default == self.NO_DEFAULT\n\n    @property\n    @abc.abstractmethod\n    def value(self) -&gt; Any:\n        pass\n\n    @property\n    @abc.abstractmethod\n    def name(self) -&gt; str:\n        pass\n\n    def value_repr(self):\n        if isinstance(self.value, ParameterObject):\n            return self.value.repr()\n        if isinstance(self.value, Path):\n            return repr(self._value)\n        return repr(self.value)\n\n    @property\n    def repr(self) -&gt; Union[str, None]:\n        if self.ignore_persistence:\n            return None\n\n        if self.dont_persist_default_value and self.value == self.default:\n            return None\n\n        return f'{self.name}={self.value_repr()}'\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.AbstractParameter.__init__","title":"<code>__init__(default=NO_DEFAULT, ignore_persistence=False, dont_persist_default_value=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>default</code> <code>Any</code> <p>value used if not provided in config, default to NO_DEFAULT meaning that param is required</p> <code>NO_DEFAULT</code> <code>ignore_persistence</code> <code>bool</code> <p>do not use this parameter in persistence, useful params without influence on output</p> <code>False</code> <code>dont_persist_default_value</code> <code>bool</code> <p>if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data</p> <code>True</code> Source code in <code>taskchain/parameter.py</code> <pre><code>def __init__(self,\n             default: Any = NO_DEFAULT,\n             ignore_persistence: bool = False,\n             dont_persist_default_value: bool = True,\n             ):\n\"\"\"\n    Args:\n        default: value used if not provided in config, default to NO_DEFAULT meaning that param is required\n        ignore_persistence: do not use this parameter in persistence, useful params without influence on output\n        dont_persist_default_value: if value of parameter is same as default, do not use it in persistence,\n            useful for adding new parameters without recomputation of data\n    \"\"\"\n    self.default = default\n    self.ignore_persistence = ignore_persistence\n    self.dont_persist_default_value = dont_persist_default_value\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.Parameter","title":"<code>Parameter</code>","text":"<p>         Bases: <code>AbstractParameter</code></p> Source code in <code>taskchain/parameter.py</code> <pre><code>class Parameter(AbstractParameter):\n\n    NO_DEFAULT = NO_DEFAULT\n    NO_VALUE = NO_VALUE\n\n    def __init__(self,\n                 name: str,\n                 dtype: Union[type] = None,\n                 default: Any = NO_DEFAULT,\n                 name_in_config: str = None,\n                 ignore_persistence: bool = False,\n                 dont_persist_default_value: bool = False,\n                 ):\n\"\"\"\n        Args:\n            name: name for referencing from task\n            dtype: expected datatype\n            default: value used if not provided in config, default to NO_DEFAULT meaning that param is required\n            name_in_config: name used for search in config, defaults to `name` argument\n            ignore_persistence: do not use this parameter in persistence, useful params without influence on output\n            dont_persist_default_value: if value of parameter is same as default, do not use it in persistence,\n                useful for adding new parameters without recomputation of data\n        \"\"\"\n        super().__init__(\n            default=default,\n            ignore_persistence=ignore_persistence,\n            dont_persist_default_value=dont_persist_default_value,\n        )\n        assert name not in taskchain.config.Config.RESERVED_PARAMETER_NAMES\n        self._name = name\n        self.dtype = dtype\n        self.name_in_config = name if name_in_config is None else name_in_config\n        self._value = self.NO_VALUE\n\n    def __str__(self):\n        return self.name\n\n    @property\n    def name(self) -&gt; str:\n        return self._name\n\n    @property\n    def value(self) -&gt; Any:\n        if self._value == self.NO_VALUE:\n            raise ValueError(f'Value not set for parameter `{self}`')\n\n        if self.dtype is Path and self._value is not None:\n            return Path(self._value)\n        return self._value\n\n    def set_value(self, config) -&gt; Any:\n        if self.name_in_config in config:\n            value = config[self.name_in_config]\n        else:\n            if self.required:\n                raise ValueError(f'Value for parameter `{self}` not found in config `{config}`')\n            value = self.default\n\n        if self.dtype is not None:\n            if value is not None and not isinstance(value, self.dtype) and not(self.dtype is Path and isinstance(value, str)):\n                raise ValueError(f'Value `{value}` of parameter `{self}` has type {type(value)} instead of `{self.dtype}`')\n\n        self._value = value\n        return value\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.Parameter.__init__","title":"<code>__init__(name, dtype=None, default=NO_DEFAULT, name_in_config=None, ignore_persistence=False, dont_persist_default_value=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name for referencing from task</p> required <code>dtype</code> <code>Union[type]</code> <p>expected datatype</p> <code>None</code> <code>default</code> <code>Any</code> <p>value used if not provided in config, default to NO_DEFAULT meaning that param is required</p> <code>NO_DEFAULT</code> <code>name_in_config</code> <code>str</code> <p>name used for search in config, defaults to <code>name</code> argument</p> <code>None</code> <code>ignore_persistence</code> <code>bool</code> <p>do not use this parameter in persistence, useful params without influence on output</p> <code>False</code> <code>dont_persist_default_value</code> <code>bool</code> <p>if value of parameter is same as default, do not use it in persistence, useful for adding new parameters without recomputation of data</p> <code>False</code> Source code in <code>taskchain/parameter.py</code> <pre><code>def __init__(self,\n             name: str,\n             dtype: Union[type] = None,\n             default: Any = NO_DEFAULT,\n             name_in_config: str = None,\n             ignore_persistence: bool = False,\n             dont_persist_default_value: bool = False,\n             ):\n\"\"\"\n    Args:\n        name: name for referencing from task\n        dtype: expected datatype\n        default: value used if not provided in config, default to NO_DEFAULT meaning that param is required\n        name_in_config: name used for search in config, defaults to `name` argument\n        ignore_persistence: do not use this parameter in persistence, useful params without influence on output\n        dont_persist_default_value: if value of parameter is same as default, do not use it in persistence,\n            useful for adding new parameters without recomputation of data\n    \"\"\"\n    super().__init__(\n        default=default,\n        ignore_persistence=ignore_persistence,\n        dont_persist_default_value=dont_persist_default_value,\n    )\n    assert name not in taskchain.config.Config.RESERVED_PARAMETER_NAMES\n    self._name = name\n    self.dtype = dtype\n    self.name_in_config = name if name_in_config is None else name_in_config\n    self._value = self.NO_VALUE\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.ParameterObject","title":"<code>ParameterObject</code>","text":"<p>         Bases: <code>abc.ABC</code></p> <p>Every class used in configs has to be inherit from this class.</p> Source code in <code>taskchain/parameter.py</code> <pre><code>class ParameterObject(abc.ABC):\n\"\"\"\n    Every class used in configs has to be inherit from this class.\n    \"\"\"\n\n    @abc.abstractmethod\n    def repr(self) -&gt; str:\n\"\"\"\n        Representation which should uniquely describe object,\n        i.e. be based on all arguments of __init__.\n        \"\"\"\n        raise NotImplemented\n\n    def __repr__(self):\n        return self.repr()\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.ParameterObject.repr","title":"<code>repr()</code>  <code>abstractmethod</code>","text":"<p>Representation which should uniquely describe object, i.e. be based on all arguments of init.</p> Source code in <code>taskchain/parameter.py</code> <pre><code>@abc.abstractmethod\ndef repr(self) -&gt; str:\n\"\"\"\n    Representation which should uniquely describe object,\n    i.e. be based on all arguments of __init__.\n    \"\"\"\n    raise NotImplemented\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.AutoParameterObject","title":"<code>AutoParameterObject</code>","text":"<p>         Bases: <code>ParameterObject</code></p> <p>ParameterObject with automatic <code>repr</code> method based on arguments of init method.</p> <p>For correct functionality, is necessary store all init argument values as self.arg_name or self._arg_name</p> Source code in <code>taskchain/parameter.py</code> <pre><code>class AutoParameterObject(ParameterObject):\n\"\"\"\n    ParameterObject with automatic `repr` method based on arguments of __init__ method.\n\n    For correct functionality, is necessary store all __init__ argument values as self.arg_name or self._arg_name\n    \"\"\"\n\n    def repr(self) -&gt; str:\n        parameters = signature(self.__init__).parameters\n\n        ignore_persistence_args = self.ignore_persistence_args()\n        dont_persist_default_value_args = self.dont_persist_default_value_args()\n        args = {}\n        for i, (arg, parameter) in enumerate(parameters.items()):\n            if arg in ignore_persistence_args:\n                continue\n            if hasattr(self, '_' + arg):\n                value = getattr(self, '_' + arg)\n            elif hasattr(self, arg):\n                value = getattr(self, arg)\n            else:\n                raise AttributeError(f'Value of __init__ argument `{arg}` not found for class `{fullname(self.__class__)}`, '\n                                     f'make sure that value is saved in `self.{arg}` or `self._{arg}`')\n            if isinstance(value, IgnoreForPersistence):\n                continue\n            if arg in dont_persist_default_value_args and value == parameter.default:\n                continue\n            args[arg] = IgnoreForPersistence.remove(value)\n        args_repr = ', '.join(f'{k}={repr(v)}' for k, v in sorted(args.items()))\n        assert 'object at 0x' not in args_repr, f'repr for arguments is fragile: {args_repr}'\n        return f'{self.__class__.__name__}({args_repr})'\n\n    @staticmethod\n    def ignore_persistence_args() -&gt; List[str]:\n\"\"\" List of __init__ argument names which are ignored in persistence. \"\"\"\n        return ['verbose', 'debug']\n\n    @staticmethod\n    def dont_persist_default_value_args() -&gt; List[str]:\n\"\"\"\n        List of __init__ argument names which are ignored in persistence\n        when they take default value.\n        \"\"\"\n        return []\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.AutoParameterObject.ignore_persistence_args","title":"<code>ignore_persistence_args()</code>  <code>staticmethod</code>","text":"<p>List of init argument names which are ignored in persistence.</p> Source code in <code>taskchain/parameter.py</code> <pre><code>@staticmethod\ndef ignore_persistence_args() -&gt; List[str]:\n\"\"\" List of __init__ argument names which are ignored in persistence. \"\"\"\n    return ['verbose', 'debug']\n</code></pre>"},{"location":"code/parameter/#taskchain.parameter.AutoParameterObject.dont_persist_default_value_args","title":"<code>dont_persist_default_value_args()</code>  <code>staticmethod</code>","text":"<p>List of init argument names which are ignored in persistence when they take default value.</p> Source code in <code>taskchain/parameter.py</code> <pre><code>@staticmethod\ndef dont_persist_default_value_args() -&gt; List[str]:\n\"\"\"\n    List of __init__ argument names which are ignored in persistence\n    when they take default value.\n    \"\"\"\n    return []\n</code></pre>"},{"location":"code/task/","title":"Task","text":""},{"location":"code/task/#taskchain.task.Task","title":"<code>Task</code>","text":"<p>         Bases: <code>object</code></p> <p>Object representing one computation step in chains.</p> Source code in <code>taskchain/task.py</code> <pre><code>class Task(object, metaclass=MetaTask):\n\"\"\"\n    Object representing one computation step in chains.\n    \"\"\"\n\n    def __init__(self, config: Config = None):\n\"\"\"\n\n        Args:\n            config: config with parameters for this task\n        \"\"\"\n        self._config: Config = config\n        self._data: Union[None, Data, DirData] = None\n        self._input_tasks: Union[None, InputTasks] = None\n        self._forced = False\n\n        self.meta = self.__class__.meta\n        self.group = self.__class__.group\n        self.slugname = self.__class__.slugname\n        self.fullname = self.__class__.fullname(config)\n        self.data_class = self.__class__.data_class\n        self.data_type = self.__class__.data_type\n\n        self.logger = logging.getLogger(f'task_{self.fullname}')\n        self.logger.setLevel(logging.DEBUG)\n\n        self._prepare_parameters()\n\n    def _prepare_parameters(self):\n\"\"\" Create task's parameter registry and load their values from config \"\"\"\n        parameters = self.meta.get('parameters')\n        if parameters is not None:\n            parameters = [\n                p\n                for p in deepcopy(parameters)\n                if isinstance(p, Parameter)\n            ]\n        self.params = self.parameters = ParameterRegistry(parameters)\n        self.parameters.set_values(self._config)\n\n    @abc.abstractmethod\n    def run(self, *args):\n\"\"\"\n        Abstract method which is called by a chain when data are needed.\n        This method represents computation.\n\n        Args:\n            *args: can be names of parameters and input tasks.\n            Their values are then provided by the chain.\n        \"\"\"\n        pass\n\n    @property\n    def data(self) -&gt; Data:\n\"\"\"\n        Get data object of this tasks.\n        This also triggers loading or computation of data same as `.value`\n        \"\"\"\n        if hasattr(self, '_data') and self._data is not None:\n            return self._data\n\n        if len(inspect.signature(self.data_class).parameters) == 0 and not inspect.isabstract(self.data_class):\n            # data class is not meant to be created out of run method -&gt; data cannot be loaded\n            self._data = self.data_class()\n            self._init_persistence(self._data)\n\n        if self._data and self._data.is_persisting and self._data.exists() and not self._forced:\n            self._data.load()\n        else:\n            try:\n                self._init_run_info()\n                if self._data and self._data.is_logging:\n                    data_log_handler = self._data.get_log_handler()\n                    self.logger.addHandler(data_log_handler)\n                else:\n                    data_log_handler = None\n                self.logger.info(f'{self} - run started with params: {self.params.repr}')\n                run_result = self.run(*self._get_run_arguments())\n                self.logger.info(f'{self} - run ended')\n                self.logger.removeHandler(data_log_handler)\n                self._process_run_result(run_result)\n            except Exception as error:\n                if self._data:\n                    self._data.on_run_error()\n                    self._data = None\n                raise error\n            self._finish_run_info()\n        return self._data\n\n    def _get_run_arguments(self):\n\"\"\"\n        Looks on arguments of `run` method and get values for them.\n        It looks to task's parameters and input tasks.\n        \"\"\"\n        args = []\n        for arg, parameter in inspect.signature(self.run).parameters.items():\n            if parameter.default != inspect.Parameter.empty:\n                raise AttributeError('Kwargs arguments in run method not allowed')\n\n            input_tasks_arg = self.input_tasks[arg] if arg in self.input_tasks else NO_VALUE\n            if isinstance(input_tasks_arg, Task):\n                input_tasks_arg = input_tasks_arg.value\n            parameter_arg = self.parameters[arg] if arg in self.parameters else NO_VALUE\n\n            if input_tasks_arg is NO_VALUE and arg not in self.parameters:\n                raise KeyError(f'Argument `{arg}` of run method of {self} not found in input_tasks nor parameters')\n\n            if input_tasks_arg is not NO_VALUE and arg in self.parameters:\n                raise KeyError(f'Argument `{arg}` of run method of {self} found in both input_tasks and parameters')\n            args.append(input_tasks_arg if input_tasks_arg is not NO_VALUE else parameter_arg)\n        return args\n\n    def get_data_object(self):\n\"\"\"\n        This is meant to be run only from `run` method.\n        Needed when task return Data object directly, e.g. DirData or ContinuesData.\n\n        Returns:\n            Data: object handling data persistence of this task.\n        \"\"\"\n        if not hasattr(self, '_data'):\n            raise ValueError('Data object is not initialized, run this only from task')\n        return self._data\n\n    @property\n    def value(self) -&gt; Any:\n\"\"\" Return result of computation. Load persisted data or compute them by `run` method. \"\"\"\n        return self.data.value\n\n    def __str__(self):\n        return self.fullname\n\n    def __repr__(self):\n        return f'&lt;task: {self}&gt;'\n\n    def _repr_markdown_(self):\n        repr = f'**{self.slugname.split(\":\")[-1]}** \\n' \\\n               f' - fullname: `{self.fullname}` \\n' \\\n               f' - group: `{self.group}` \\n' \\\n               f' - config: `{self.get_config()}` \\n'\n\n        if self.has_data:\n            repr += f' - data: `{self.data_path}` \\n' \\\n\n        return repr\n\n    def get_config(self):\n\"\"\" Return config used to configure this task. \"\"\"\n        return self._config\n\n    @property\n    def path(self) -&gt; Path:\n\"\"\" Path where all data of this task are persisted. \"\"\"\n        if self._config.base_dir is None:\n            raise ValueError(f'Config `{self._config}` has not base dir set')\n        path = self._config.base_dir / self.slugname.replace(':', '/')\n        return path\n\n    @property\n    def _data_without_value(self) -&gt; Data:\n\"\"\" Get data object but avoid loading or computation of data. \"\"\"\n        if hasattr(self, '_data') and self._data is not None:\n            return self._data\n        data = self.data_class()\n        self._init_persistence(data)\n        return data\n\n    @property\n    def has_data(self) -&gt; bool:\n\"\"\" Check if this task has data already computed and persisted. \"\"\"\n        if issubclass(self.data_class, InMemoryData):\n            return False\n        return self._data_without_value.exists()\n\n    @property\n    def data_path(self) -&gt; Path:\n\"\"\"\n        Path to data.\n        Path can be not existent if data are not yet computed.\n        Returns None if task does not persisting.\n        \"\"\"\n        if issubclass(self.data_class, InMemoryData):\n            return None\n        return self._data_without_value.path\n\n    def reset_data(self):\n        self._data = None\n        return self\n\n    def force(self, delete_data=False):\n\"\"\"\n        Switch task to forced state to allow data recomputation.\n        Next time value is requested persisted data are ignored and computation is triggered.\n\n        Args:\n            delete_data (bool): whether persisted data should be immediately deleted from disk.\n        Returns:\n            Task: allows chaining `task.force().value`\n        \"\"\"\n        if delete_data:\n            data = self._data_without_value\n            if data.exists():\n                data.delete()\n\n        self._forced = True\n        self._data = None\n        return self\n\n    @property\n    def is_forced(self):\n        return self._forced\n\n    @property\n    def input_tasks(self) -&gt; 'InputTasks':\n\"\"\"\n        Get task's registry which allows access input tasks by name or index.\n        \"\"\"\n        if self._input_tasks is None:\n            raise ValueError(f'Input tasks for task `{self}` not initialized')\n        return self._input_tasks\n\n    def set_input_tasks(self, task_map: 'InputTasks'):\n        self._input_tasks = task_map\n\n    def _process_run_result(self, run_result: Any):\n\"\"\"\n        Handle result of computation by processing then by data object.\n\n        Args:\n            run_result: return value of run method\n        \"\"\"\n        if isclass(self.data_type) and issubclass(self.data_type, Data) and isinstance(run_result, self.data_type):\n            self._data = run_result\n            self._init_persistence(self._data)\n        elif isinstance(run_result, self.data_type) or custom_isinstance(run_result, self.data_type) or fullname(self.data_type) == 'typing.Generator':\n            assert self._data is not None, f'{fullname(self.__class__)}: attribute \"_data\" cannot be None'\n            self._data.set_value(run_result)\n        else:\n            raise ValueError(f'{fullname(self.__class__)}: Invalid result data type: {type(run_result)} instead of {self.data_type}')\n\n        if self._data.is_persisting:\n            self._data.save()\n\n    def _init_persistence(self, data):\n        if self._config is not None and not data.is_persisting:\n            data.init_persistence(self.path, self.name_for_persistence)\n\n    @property\n    def name_for_persistence(self):\n\"\"\"\n        Get unique string representation of this object used in persistence.\n        This value is provided by config.\n\n        Returns:\n            str: hash based on input tasks and parameters in parameter mode, name of config otherwise\n        \"\"\"\n        return self._config.get_name_for_persistence(self)\n\n    @property\n    def run_info(self) -&gt; Dict:\n\"\"\" Info about last call of `run` method. \"\"\"\n        data = self._data_without_value\n        return data.load_run_info()\n\n    def _init_run_info(self):\n        self._run_info = {\n            'task': {\n                'name': self.slugname,\n                'class': self.__class__.__name__,\n                'module': self.__class__.__module__,\n            },\n            'parameters': {p.name: p.value_repr() for p in self.parameters.values()},\n            'user': {\n                'name': getpass.getuser(),\n                'taskchain_version': taskchain.__version__,\n            },\n            'log': [],\n        }\n        if self._config is not None:\n            self._run_info['config'] = {\n                'name': self._config.name,\n                'namespace': self._config.namespace,\n                'context': self._config.context.name if self._config.context is not None else None,\n            }\n\n            from .chain import TaskParameterConfig\n            if isinstance(self._config, TaskParameterConfig):\n                self._run_info['input_tasks'] = self._config.input_tasks\n        self._run_info['started'] = datetime.timestamp(datetime.now())\n\n    def save_to_run_info(self, record):\n\"\"\"\n        Save information to run info. Should be called from `run` method.\n\n        Args:\n            record: any json-like object\n        \"\"\"\n        if isinstance(record, defaultdict):\n            record = dict(record)\n        self._run_info['log'].append(record)\n\n    def _finish_run_info(self):\n        now = datetime.now()\n        self._run_info['ended'] = str(now)\n        self._run_info['time'] = datetime.timestamp(now) - self._run_info['started']\n        self._run_info['started'] = str(datetime.fromtimestamp(self._run_info['started']))\n\n        if self._data and self._data.is_logging:\n            self._data.save_run_info(self._run_info)\n\n    @property\n    def log(self) -&gt; Union[None, List[str]]:\n\"\"\" Log (from `self.logger`) from last run as list of rows. \"\"\"\n        data = self._data_without_value\n        if data:\n            return data.log\n        return None\n</code></pre>"},{"location":"code/task/#taskchain.task.Task.data","title":"<code>data: Data</code>  <code>property</code>","text":"<p>Get data object of this tasks. This also triggers loading or computation of data same as <code>.value</code></p>"},{"location":"code/task/#taskchain.task.Task.value","title":"<code>value: Any</code>  <code>property</code>","text":"<p>Return result of computation. Load persisted data or compute them by <code>run</code> method.</p>"},{"location":"code/task/#taskchain.task.Task.path","title":"<code>path: Path</code>  <code>property</code>","text":"<p>Path where all data of this task are persisted.</p>"},{"location":"code/task/#taskchain.task.Task.has_data","title":"<code>has_data: bool</code>  <code>property</code>","text":"<p>Check if this task has data already computed and persisted.</p>"},{"location":"code/task/#taskchain.task.Task.data_path","title":"<code>data_path: Path</code>  <code>property</code>","text":"<p>Path to data. Path can be not existent if data are not yet computed. Returns None if task does not persisting.</p>"},{"location":"code/task/#taskchain.task.Task.input_tasks","title":"<code>input_tasks: InputTasks</code>  <code>property</code>","text":"<p>Get task's registry which allows access input tasks by name or index.</p>"},{"location":"code/task/#taskchain.task.Task.name_for_persistence","title":"<code>name_for_persistence</code>  <code>property</code>","text":"<p>Get unique string representation of this object used in persistence. This value is provided by config.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>hash based on input tasks and parameters in parameter mode, name of config otherwise</p>"},{"location":"code/task/#taskchain.task.Task.run_info","title":"<code>run_info: Dict</code>  <code>property</code>","text":"<p>Info about last call of <code>run</code> method.</p>"},{"location":"code/task/#taskchain.task.Task.log","title":"<code>log: Union[None, List[str]]</code>  <code>property</code>","text":"<p>Log (from <code>self.logger</code>) from last run as list of rows.</p>"},{"location":"code/task/#taskchain.task.Task.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>config with parameters for this task</p> <code>None</code> Source code in <code>taskchain/task.py</code> <pre><code>def __init__(self, config: Config = None):\n\"\"\"\n\n    Args:\n        config: config with parameters for this task\n    \"\"\"\n    self._config: Config = config\n    self._data: Union[None, Data, DirData] = None\n    self._input_tasks: Union[None, InputTasks] = None\n    self._forced = False\n\n    self.meta = self.__class__.meta\n    self.group = self.__class__.group\n    self.slugname = self.__class__.slugname\n    self.fullname = self.__class__.fullname(config)\n    self.data_class = self.__class__.data_class\n    self.data_type = self.__class__.data_type\n\n    self.logger = logging.getLogger(f'task_{self.fullname}')\n    self.logger.setLevel(logging.DEBUG)\n\n    self._prepare_parameters()\n</code></pre>"},{"location":"code/task/#taskchain.task.Task.run","title":"<code>run(*args)</code>  <code>abstractmethod</code>","text":"<p>Abstract method which is called by a chain when data are needed. This method represents computation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>can be names of parameters and input tasks.</p> <code>()</code> Source code in <code>taskchain/task.py</code> <pre><code>@abc.abstractmethod\ndef run(self, *args):\n\"\"\"\n    Abstract method which is called by a chain when data are needed.\n    This method represents computation.\n\n    Args:\n        *args: can be names of parameters and input tasks.\n        Their values are then provided by the chain.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"code/task/#taskchain.task.Task.get_data_object","title":"<code>get_data_object()</code>","text":"<p>This is meant to be run only from <code>run</code> method. Needed when task return Data object directly, e.g. DirData or ContinuesData.</p> <p>Returns:</p> Name Type Description <code>Data</code> <p>object handling data persistence of this task.</p> Source code in <code>taskchain/task.py</code> <pre><code>def get_data_object(self):\n\"\"\"\n    This is meant to be run only from `run` method.\n    Needed when task return Data object directly, e.g. DirData or ContinuesData.\n\n    Returns:\n        Data: object handling data persistence of this task.\n    \"\"\"\n    if not hasattr(self, '_data'):\n        raise ValueError('Data object is not initialized, run this only from task')\n    return self._data\n</code></pre>"},{"location":"code/task/#taskchain.task.Task.get_config","title":"<code>get_config()</code>","text":"<p>Return config used to configure this task.</p> Source code in <code>taskchain/task.py</code> <pre><code>def get_config(self):\n\"\"\" Return config used to configure this task. \"\"\"\n    return self._config\n</code></pre>"},{"location":"code/task/#taskchain.task.Task.force","title":"<code>force(delete_data=False)</code>","text":"<p>Switch task to forced state to allow data recomputation. Next time value is requested persisted data are ignored and computation is triggered.</p> <p>Parameters:</p> Name Type Description Default <code>delete_data</code> <code>bool</code> <p>whether persisted data should be immediately deleted from disk.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Task</code> <p>allows chaining <code>task.force().value</code></p> Source code in <code>taskchain/task.py</code> <pre><code>def force(self, delete_data=False):\n\"\"\"\n    Switch task to forced state to allow data recomputation.\n    Next time value is requested persisted data are ignored and computation is triggered.\n\n    Args:\n        delete_data (bool): whether persisted data should be immediately deleted from disk.\n    Returns:\n        Task: allows chaining `task.force().value`\n    \"\"\"\n    if delete_data:\n        data = self._data_without_value\n        if data.exists():\n            data.delete()\n\n    self._forced = True\n    self._data = None\n    return self\n</code></pre>"},{"location":"code/task/#taskchain.task.Task.save_to_run_info","title":"<code>save_to_run_info(record)</code>","text":"<p>Save information to run info. Should be called from <code>run</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <p>any json-like object</p> required Source code in <code>taskchain/task.py</code> <pre><code>def save_to_run_info(self, record):\n\"\"\"\n    Save information to run info. Should be called from `run` method.\n\n    Args:\n        record: any json-like object\n    \"\"\"\n    if isinstance(record, defaultdict):\n        record = dict(record)\n    self._run_info['log'].append(record)\n</code></pre>"},{"location":"code/task/#taskchain.task.ModuleTask","title":"<code>ModuleTask</code>","text":"<p>         Bases: <code>Task</code></p> <p>Task which group is based on python module name (file with the task)</p> Source code in <code>taskchain/task.py</code> <pre><code>class ModuleTask(Task, metaclass=MetaModuleTask):\n\"\"\"\n    Task which group is based on python module name (file with the task)\n    \"\"\"\n</code></pre>"},{"location":"code/task/#taskchain.task.DoubleModuleTask","title":"<code>DoubleModuleTask</code>","text":"<p>         Bases: <code>Task</code></p> <p>Task which groups are based on python module name (file with the task) and package (dir with that file). Full name of the task: <code>package_name:module_name:task_name</code></p> Source code in <code>taskchain/task.py</code> <pre><code>class DoubleModuleTask(Task, metaclass=MetaDoubleModuleTask):\n\"\"\"\n    Task which groups are based on python module name (file with the task) and package (dir with that file).\n    Full name of the task: `package_name:module_name:task_name`\n    \"\"\"\n</code></pre>"},{"location":"code/task/#taskchain.task.InputTasks","title":"<code>InputTasks</code>","text":"<p>         Bases: <code>dict</code></p> <p>Registry of input tasks. Main feature of this class is that it allow access task in multiple ways:</p> <ul> <li>by full name of the task (including namespace and groups)</li> <li>by shorter name without namespace or groups as long as it is unambiguous</li> <li>by index, order is given by order in <code>Meta</code></li> </ul> Source code in <code>taskchain/task.py</code> <pre><code>class InputTasks(dict):\n\"\"\"\n    Registry of input tasks.\n    Main feature of this class is that it allow access task in multiple ways:\n\n    - by full name of the task (including namespace and groups)\n    - by shorter name without namespace or groups as long as it is unambiguous\n    - by index, order is given by order in `Meta`\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.task_list = []\n\n    def __setitem__(self, key, value):\n        if not super().__contains__(key):\n            self.task_list.append(value)\n        super().__setitem__(key, value)\n\n    def __getitem__(self, item):\n        return self.get(item)\n\n    def get(self, item, default=None):\n\"\"\"\"\"\"\n        if type(item) is int:\n            return self.task_list[item]\n        if default is not None:\n            raise ValueError('Default task is not allowed')\n        return super().get(_find_task_full_name(item, self.keys()))\n\n    def __contains__(self, item):\n        try:\n            return super().__contains__(_find_task_full_name(item, self.keys()))\n        except KeyError:\n            return False\n</code></pre>"},{"location":"code/testing/","title":"Testing","text":""},{"location":"code/testing/#taskchain.utils.testing.TestChain","title":"<code>TestChain</code>","text":"<p>         Bases: <code>Chain</code></p> Source code in <code>taskchain/utils/testing.py</code> <pre><code>class TestChain(Chain):\n\n    def __init__(self,\n                 tasks: List[Type[Task]],\n                 mock_tasks: Dict[Union[str, Type[Task]], Any] = None,\n                 parameters: Dict[str, Any] = None,\n                 base_dir: Path = None,\n                 ):\n\"\"\"\n        Helper class for testing part of a chain. Some tasks are present fully, some are mocked.\n        Config is not needed, parameters are provided directly.\n\n        Args:\n            tasks: list of task classes which should be part of the chain\n            mock_tasks: other task which are needed (as input_tasks) in chain but their output is only mocked\n                (by values of this dict)\n            parameters: parameter names and their values\n            base_dir: path for data persistence, if None tmp dir is created\n        \"\"\"\n        self._tasks = tasks\n        self._mock_tasks = mock_tasks or {}\n\n        if base_dir is None:\n            base_dir = Path(tempfile.TemporaryDirectory().name)\n\n        if parameters is None:\n            parameters = {}\n        self.config = Config(base_dir, name='test', data=parameters)\n\n        super().__init__(self.config)\n\n    def _prepare(self):\n        self._process_config(self._base_config)\n        self.tasks = self._create_tasks()\n        self._process_dependencies(self.tasks)\n\n        self._build_graph()\n        self._init_objects()\n\n    def _create_tasks(self) -&gt; Dict[str, Task]:\n        tasks = {}\n        for task_class in self._tasks:\n            task = self._create_task(task_class, self.config)\n            tasks[task_class.fullname(self.config)] = task\n\n        for mock_task, value in self._mock_tasks.items():\n            name = mock_task if isinstance(mock_task, str) else mock_task.fullname(self.config)\n            tasks[name] = MockTask(value)\n\n        return tasks\n</code></pre>"},{"location":"code/testing/#taskchain.utils.testing.TestChain.__init__","title":"<code>__init__(tasks, mock_tasks=None, parameters=None, base_dir=None)</code>","text":"<p>Helper class for testing part of a chain. Some tasks are present fully, some are mocked. Config is not needed, parameters are provided directly.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>List[Type[Task]]</code> <p>list of task classes which should be part of the chain</p> required <code>mock_tasks</code> <code>Dict[Union[str, Type[Task]], Any]</code> <p>other task which are needed (as input_tasks) in chain but their output is only mocked (by values of this dict)</p> <code>None</code> <code>parameters</code> <code>Dict[str, Any]</code> <p>parameter names and their values</p> <code>None</code> <code>base_dir</code> <code>Path</code> <p>path for data persistence, if None tmp dir is created</p> <code>None</code> Source code in <code>taskchain/utils/testing.py</code> <pre><code>def __init__(self,\n             tasks: List[Type[Task]],\n             mock_tasks: Dict[Union[str, Type[Task]], Any] = None,\n             parameters: Dict[str, Any] = None,\n             base_dir: Path = None,\n             ):\n\"\"\"\n    Helper class for testing part of a chain. Some tasks are present fully, some are mocked.\n    Config is not needed, parameters are provided directly.\n\n    Args:\n        tasks: list of task classes which should be part of the chain\n        mock_tasks: other task which are needed (as input_tasks) in chain but their output is only mocked\n            (by values of this dict)\n        parameters: parameter names and their values\n        base_dir: path for data persistence, if None tmp dir is created\n    \"\"\"\n    self._tasks = tasks\n    self._mock_tasks = mock_tasks or {}\n\n    if base_dir is None:\n        base_dir = Path(tempfile.TemporaryDirectory().name)\n\n    if parameters is None:\n        parameters = {}\n    self.config = Config(base_dir, name='test', data=parameters)\n\n    super().__init__(self.config)\n</code></pre>"},{"location":"code/testing/#taskchain.utils.testing.create_test_task","title":"<code>create_test_task(task, input_tasks=None, parameters=None, base_dir=None)</code>","text":"<p>Helper function which instantiate task in such way, that parameters and input_tasks are given with arguments of this function.</p> <p>Parameters:</p> Name Type Description Default <code>input_tasks</code> <code>Dict[Union[str, Type[Task]], Any]</code> <p>mocked values of input_tasks of tested class</p> <code>None</code> <code>parameters</code> <code>Dict[str, Any]</code> <p>parameter names and their values</p> <code>None</code> <code>base_dir</code> <code>Path</code> <p>path for data persistence, if None tmp dir is created</p> <code>None</code> Source code in <code>taskchain/utils/testing.py</code> <pre><code>def create_test_task(\n        task: Type[Task],\n        input_tasks: Dict[Union[str, Type[Task]], Any] = None,\n        parameters: Dict[str, Any] = None,\n        base_dir: Path = None,\n        ) -&gt; Task:\n\"\"\"\n    Helper function which instantiate task in such way, that parameters and input_tasks are given with arguments\n    of this function.\n\n    Args:\n        tasks class of tested tasks\n        input_tasks: mocked values of input_tasks of tested class\n        parameters: parameter names and their values\n        base_dir: path for data persistence, if None tmp dir is created\n    \"\"\"\n    test_chain = TestChain([task],  parameters=parameters, mock_tasks=input_tasks, base_dir=base_dir)\n    return test_chain[task.fullname(test_chain.config)]\n</code></pre>"},{"location":"code/utils/","title":"Utils","text":""},{"location":"code/utils/#taskchain.cache","title":"<code>taskchain.cache</code>","text":""},{"location":"code/utils/#taskchain.cache.Cache","title":"<code>Cache</code>","text":"<p>         Bases: <code>abc.ABC</code></p> <p>Cache interface.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class Cache(abc.ABC):\n\"\"\"Cache interface.\"\"\"\n\n    @abc.abstractmethod\n    def get(self, key: str) -&gt; Any:\n\"\"\"\n        Get value for given key if cached.\n\n        Args:\n            key: key under which value is cached\n\n        Returns:\n            cached value or NO_VALUE\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_or_compute(self, key: str, computer: Callable, force: bool = False) -&gt; Any:\n\"\"\"\n        Get value for given key if cached or compute and cache it.\n\n        Args:\n            key: key under which value is cached\n            computer: function which returns value if not cached\n            force: recompute value even if it is in cache\n\n        Returns:\n            cached or computed value\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def subcache(self, *args) -&gt; 'Cache':\n\"\"\"Create separate sub-cache of this cache.\"\"\"\n        pass\n</code></pre>"},{"location":"code/utils/#taskchain.cache.Cache.get","title":"<code>get(key)</code>  <code>abstractmethod</code>","text":"<p>Get value for given key if cached.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key under which value is cached</p> required <p>Returns:</p> Type Description <code>Any</code> <p>cached value or NO_VALUE</p> Source code in <code>taskchain/cache.py</code> <pre><code>@abc.abstractmethod\ndef get(self, key: str) -&gt; Any:\n\"\"\"\n    Get value for given key if cached.\n\n    Args:\n        key: key under which value is cached\n\n    Returns:\n        cached value or NO_VALUE\n    \"\"\"\n    pass\n</code></pre>"},{"location":"code/utils/#taskchain.cache.Cache.get_or_compute","title":"<code>get_or_compute(key, computer, force=False)</code>  <code>abstractmethod</code>","text":"<p>Get value for given key if cached or compute and cache it.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key under which value is cached</p> required <code>computer</code> <code>Callable</code> <p>function which returns value if not cached</p> required <code>force</code> <code>bool</code> <p>recompute value even if it is in cache</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>cached or computed value</p> Source code in <code>taskchain/cache.py</code> <pre><code>@abc.abstractmethod\ndef get_or_compute(self, key: str, computer: Callable, force: bool = False) -&gt; Any:\n\"\"\"\n    Get value for given key if cached or compute and cache it.\n\n    Args:\n        key: key under which value is cached\n        computer: function which returns value if not cached\n        force: recompute value even if it is in cache\n\n    Returns:\n        cached or computed value\n    \"\"\"\n    pass\n</code></pre>"},{"location":"code/utils/#taskchain.cache.Cache.subcache","title":"<code>subcache(*args)</code>  <code>abstractmethod</code>","text":"<p>Create separate sub-cache of this cache.</p> Source code in <code>taskchain/cache.py</code> <pre><code>@abc.abstractmethod\ndef subcache(self, *args) -&gt; 'Cache':\n\"\"\"Create separate sub-cache of this cache.\"\"\"\n    pass\n</code></pre>"},{"location":"code/utils/#taskchain.cache.DummyCache","title":"<code>DummyCache</code>","text":"<p>         Bases: <code>Cache</code></p> <p>No caching.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class DummyCache(Cache):\n\"\"\"No caching.\"\"\"\n\n    def get(self, key: str):\n        return NO_VALUE\n\n    def get_or_compute(self, key: str, computer: Callable, force: bool = False):\n\"\"\"\"\"\"\n        return computer()\n\n    def subcache(self, *args):\n\"\"\"\"\"\"\n        return self\n</code></pre>"},{"location":"code/utils/#taskchain.cache.InMemoryCache","title":"<code>InMemoryCache</code>","text":"<p>         Bases: <code>Cache</code></p> <p>Cache only in memory.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class InMemoryCache(Cache):\n\"\"\"Cache only in memory.\"\"\"\n\n    def __init__(self):\n        self._memory = defaultdict(dict)\n        self._subcaches = defaultdict(dict)\n\n    def get(self, key: str):\n        return self._memory[get_ident()].get(key, NO_VALUE)\n\n    def get_or_compute(self, key: str, computer: Callable, force: bool = False):\n\"\"\"\"\"\"\n        if key not in self._memory[get_ident()] or force:\n            self._memory[get_ident()][key] = computer()\n        return self._memory[get_ident()][key]\n\n    def subcache(self, name):\n\"\"\"\"\"\"\n        if name not in self._subcaches[get_ident()]:\n            self._subcaches[get_ident()][name] = InMemoryCache()\n        return self._subcaches[get_ident()][name]\n\n    def __len__(self):\n        return len(self._memory[get_ident()])\n</code></pre>"},{"location":"code/utils/#taskchain.cache.FileCache","title":"<code>FileCache</code>","text":"<p>         Bases: <code>Cache</code></p> <p>General cache for saving values in files.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class FileCache(Cache):\n\"\"\"General cache for saving values in files.\"\"\"\n\n    def __init__(self, directory: Union[str, Path]):\n        self.directory = Path(directory)\n        self.directory.mkdir(exist_ok=True, parents=True)\n\n    def filepath(self, key: str) -&gt; Path:\n        key_hash = sha256(key.encode()).hexdigest()\n        directory = self.directory / key_hash[:5]\n        directory.mkdir(exist_ok=True)\n        return directory / f'{key_hash[5:]}.{self.extension}'\n\n    def get(self, key):\n        filepath = self.filepath(key)\n        lock = FileLock(str(filepath) + '.lock')\n        with lock:\n            filepath_exists = filepath.exists()\n        if filepath_exists:\n            try:\n                return self.load_value(filepath, key)\n            except CacheException as error:\n                raise error\n            except Exception as error:\n                logger.warning('Cannot load cached value.')\n                logger.exception(error)\n        return NO_VALUE\n\n    def get_or_compute(self, key, computer, force=False):\n\"\"\"\"\"\"\n        filepath = self.filepath(key)\n        lock = FileLock(str(filepath) + '.lock')\n        with lock:\n            filepath_exists = filepath.exists()\n        if filepath_exists and not force:\n            try:\n                return self.load_value(filepath, key)\n            except CacheException as error:\n                raise error\n            except Exception as error:\n                logger.warning('Cannot load cached value.')\n                logger.exception(error)\n\n        with lock:\n            logger.debug(f'Computing cache for key {key} | file: {filepath}')\n            value = computer()\n            self.save_value(filepath, key, value)\n        return value\n\n    @abc.abstractmethod\n    def save_value(self, filepath: Path, key: str, value: Any):\n        pass\n\n    @abc.abstractmethod\n    def load_value(self, filepath: Path, key: str) -&gt; Any:\n        pass\n\n    def subcache(self, directory: Union[str, Path]):\n\"\"\"\"\"\"\n        return self.__class__(self.directory / directory)\n\n    @property\n    @abc.abstractmethod\n    def extension(self):\n        pass\n</code></pre>"},{"location":"code/utils/#taskchain.cache.JsonCache","title":"<code>JsonCache</code>","text":"<p>         Bases: <code>FileCache</code></p> <p>Cache json-like objects in <code>.json</code> files.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class JsonCache(FileCache):\n\"\"\"Cache json-like objects in `.json` files.\"\"\"\n\n    def __init__(self, directory, allow_nones=True):\n        super().__init__(directory)\n        self.allow_nones = allow_nones\n\n    def save_value(self, filepath: Path, key: str, value: Any):\n        if value is None and not self.allow_nones:\n            raise CacheException(f'The cache value for key {key} is None')\n        with filepath.open('w') as f:\n            json.dump({'key': key, 'value': value}, f)\n\n    def load_value(self, filepath: Path, key: str) -&gt; Any:\n        with filepath.open('r') as file:\n            loaded = json.load(file)\n            if key != loaded['key']:\n                raise CacheException(\n                    f'The expected cache key {key} does not match to the retrieved one {loaded[\"key\"]}'\n                )\n            if loaded['value'] is None and not self.allow_nones:\n                raise CacheException(f'The cache value for key {key} is None, file: {filepath}')\n            return loaded['value']\n\n    @property\n    def extension(self):\n        return 'json'\n</code></pre>"},{"location":"code/utils/#taskchain.cache.DataFrameCache","title":"<code>DataFrameCache</code>","text":"<p>         Bases: <code>FileCache</code></p> <p>Cache pandas DataFrame objects in <code>.pd</code> files.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class DataFrameCache(FileCache):\n\"\"\"Cache pandas DataFrame objects in `.pd` files.\"\"\"\n\n    def save_value(self, filepath: Path, key: str, value: Any):\n        value.to_pickle(filepath)\n\n    def load_value(self, filepath: Path, key: str) -&gt; Any:\n        return pd.read_pickle(filepath)\n\n    @property\n    def extension(self):\n        return 'pd'\n</code></pre>"},{"location":"code/utils/#taskchain.cache.NumpyArrayCache","title":"<code>NumpyArrayCache</code>","text":"<p>         Bases: <code>FileCache</code></p> <p>Cache numpy arrays in <code>.npy</code> files.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class NumpyArrayCache(FileCache):\n\"\"\"Cache numpy arrays in `.npy` files.\"\"\"\n\n    def save_value(self, filepath: Path, key: str, value: Any):\n        np.save(filepath, value)\n\n    def load_value(self, filepath: Path, key: str) -&gt; Any:\n        return np.load(filepath, allow_pickle=True)\n\n    @property\n    def extension(self):\n        return 'npy'\n</code></pre>"},{"location":"code/utils/#taskchain.cache.cached","title":"<code>cached</code>","text":"<p>Decorator for automatic caching of method results. Decorated method is for given arguments called only once a result is cached. Cache key is automatically constructed based on method arguments. Cache can be defined in decorator or as attribute of object.</p> Source code in <code>taskchain/cache.py</code> <pre><code>class cached:\n\"\"\"\n    Decorator for automatic caching of method results.\n    Decorated method is for given arguments called only once a result is cached.\n    Cache key is automatically constructed based on method arguments.\n    Cache can be defined in decorator or as attribute of object.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache_object: Cache = None,\n        key: Callable = None,\n        cache_attr: str = 'cache',\n        ignore_kwargs: List[str] = None,\n    ):\n\"\"\"\n        Args:\n            cache_object: Cache used for caching.\n            key: custom function for computing key from arguments\n            cache_attr: if `cache_object` is None, object attribute with this name is used\n            ignore_kwargs: kwargs to ignore in key construction, e.g. `verbose`\n        \"\"\"\n        if callable(cache_object):\n            self.method = cache_object\n            cache_object = None\n        self.cache_object = cache_object\n        self.key = key\n        self.cache_attr = cache_attr\n        self.ignore_params = ignore_kwargs if ignore_kwargs else []\n\n    def __call__(self, method):\n        @functools.wraps(method)\n        def decorated(obj, *args, force_cache=False, store_cache_value=NO_VALUE, only_cache=False, **kwargs):\n            assert store_cache_value is NO_VALUE or not only_cache\n            if self.cache_object is None:\n                assert hasattr(obj, self.cache_attr), f'Missing cache argument for obj {obj}'\n                cache = getattr(obj, self.cache_attr).subcache(method.__name__)\n            else:\n                cache = self.cache_object\n\n            if self.key is None:\n                for i, (arg, parameter) in enumerate(signature(method).parameters.items()):\n                    if i == 0:\n                        # skip self\n                        continue\n                    if i - 1 &lt; len(args):\n                        kwargs[arg] = args[i - 1]\n                    if parameter.default != Parameter.empty and arg not in kwargs:\n                        kwargs[arg] = parameter.default\n                args = []\n                key_kwargs = {k: v for k, v in kwargs.items() if k not in self.ignore_params}\n                # we use json module from standard library to ensure backward\n                # compatibility\n                cache_key = orig_json.dumps(key_kwargs, sort_keys=True)\n            else:\n                cache_key = self.key(*args, **kwargs)\n\n            if only_cache:\n                return cache.get(cache_key)\n\n            if store_cache_value is NO_VALUE:\n                computer = lambda: method(obj, *args, **kwargs)  # noqa: E731\n            else:\n                computer = lambda: store_cache_value  # noqa: E731\n\n            return cache.get_or_compute(cache_key, computer, force=force_cache)\n\n        return decorated\n\n    def __get__(self, instance, instancetype):\n        return functools.wraps(self.method)(functools.partial(self(self.method), instance))\n</code></pre>"},{"location":"code/utils/#taskchain.cache.cached.__init__","title":"<code>__init__(cache_object=None, key=None, cache_attr='cache', ignore_kwargs=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cache_object</code> <code>Cache</code> <p>Cache used for caching.</p> <code>None</code> <code>key</code> <code>Callable</code> <p>custom function for computing key from arguments</p> <code>None</code> <code>cache_attr</code> <code>str</code> <p>if <code>cache_object</code> is None, object attribute with this name is used</p> <code>'cache'</code> <code>ignore_kwargs</code> <code>List[str]</code> <p>kwargs to ignore in key construction, e.g. <code>verbose</code></p> <code>None</code> Source code in <code>taskchain/cache.py</code> <pre><code>def __init__(\n    self,\n    cache_object: Cache = None,\n    key: Callable = None,\n    cache_attr: str = 'cache',\n    ignore_kwargs: List[str] = None,\n):\n\"\"\"\n    Args:\n        cache_object: Cache used for caching.\n        key: custom function for computing key from arguments\n        cache_attr: if `cache_object` is None, object attribute with this name is used\n        ignore_kwargs: kwargs to ignore in key construction, e.g. `verbose`\n    \"\"\"\n    if callable(cache_object):\n        self.method = cache_object\n        cache_object = None\n    self.cache_object = cache_object\n    self.key = key\n    self.cache_attr = cache_attr\n    self.ignore_params = ignore_kwargs if ignore_kwargs else []\n</code></pre>"},{"location":"code/utils/#taskchain.utils.clazz","title":"<code>taskchain.utils.clazz</code>","text":""},{"location":"code/utils/#taskchain.utils.clazz.persistent","title":"<code>persistent</code>","text":"<p>Method decorator. Has to be used on decorator without arguments. Saves result in <code>self.__method_name</code> and next time does not call decorated method and only return saved value.</p> Source code in <code>taskchain/utils/clazz.py</code> <pre><code>class persistent:\n\"\"\"\n    Method decorator.\n    Has to be used on decorator without arguments.\n    Saves result in `self.__method_name`\n    and next time does not call decorated method and only return saved value.\n    \"\"\"\n\n    def __init__(self, method):\n        self.method = method\n\n    def __call__(self, obj):\n        attr = f'__{self.method.__name__}'\n        if not hasattr(obj, attr) or getattr(obj, attr) is None:\n            setattr(obj, attr, self.method(obj))\n        return getattr(obj, attr)\n\n    def __get__(self, instance, instancetype):\n        return functools.partial(self.__call__, instance)\n</code></pre>"},{"location":"code/utils/#taskchain.utils.clazz.repeat_on_error","title":"<code>repeat_on_error</code>","text":"<p>Method decorator which calls method again on error.</p> Source code in <code>taskchain/utils/clazz.py</code> <pre><code>class repeat_on_error:\n\"\"\" Method decorator which calls method again on error. \"\"\"\n\n    def __init__(self, retries: int = 10, waiting_time: int = 1, wait_extension: float = 1.):\n\"\"\"\n        Args:\n            retries: how many times try to call again\n            waiting_time: how many seconds wait before first retry\n            wait_extension: how many times increase waiting time after each retry\n        \"\"\"\n        if callable(retries):\n            self.method = retries\n            retries = 10\n        self.retries = retries\n        self.waiting_time = waiting_time\n        self.wait_extension = wait_extension\n\n    def __call__(self, method):\n        def decorated(*args, **kwargs):\n            waiting_time = self.waiting_time\n            for i in range(self.retries):\n                try:\n                    return method(*args, **kwargs)\n                except Exception as error:\n                    if i + 1 == self.retries:\n                        raise error\n                    sleep(waiting_time)\n                    waiting_time *= self.wait_extension\n            assert False\n        return decorated\n\n    def __get__(self, instance, instancetype):\n        return functools.partial(self(self.method), instance)\n</code></pre>"},{"location":"code/utils/#taskchain.utils.clazz.repeat_on_error.__init__","title":"<code>__init__(retries=10, waiting_time=1, wait_extension=1.0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>how many times try to call again</p> <code>10</code> <code>waiting_time</code> <code>int</code> <p>how many seconds wait before first retry</p> <code>1</code> <code>wait_extension</code> <code>float</code> <p>how many times increase waiting time after each retry</p> <code>1.0</code> Source code in <code>taskchain/utils/clazz.py</code> <pre><code>def __init__(self, retries: int = 10, waiting_time: int = 1, wait_extension: float = 1.):\n\"\"\"\n    Args:\n        retries: how many times try to call again\n        waiting_time: how many seconds wait before first retry\n        wait_extension: how many times increase waiting time after each retry\n    \"\"\"\n    if callable(retries):\n        self.method = retries\n        retries = 10\n    self.retries = retries\n    self.waiting_time = waiting_time\n    self.wait_extension = wait_extension\n</code></pre>"},{"location":"code/utils/#taskchain.utils.io","title":"<code>taskchain.utils.io</code>","text":""},{"location":"code/utils/#taskchain.utils.io.write_jsons","title":"<code>write_jsons(jsons, filename, use_tqdm=True, overwrite=True, nan_to_null=True, **kwargs)</code>","text":"<p>Write json-like object to <code>.jsonl</code> file (json lines).</p> <p>Parameters:</p> Name Type Description Default <code>jsons</code> <code>Iterable</code> <p>Iterable of json-like objects.</p> required <code>filename</code> <code>Path | str</code> required <code>use_tqdm</code> <code>bool</code> <p>Show progress bar.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite existing file.</p> <code>True</code> <code>nan_to_null</code> <code>bool</code> <p>Change nan values to nulls.</p> <code>True</code> <code>**kwargs</code> <p>other arguments to tqdm.</p> <code>{}</code> Source code in <code>taskchain/utils/io.py</code> <pre><code>def write_jsons(jsons, filename, use_tqdm=True, overwrite=True, nan_to_null=True, **kwargs):\n\"\"\"\n    Write json-like object to `.jsonl` file (json lines).\n    Args:\n        jsons (Iterable): Iterable of json-like objects.\n        filename (Path | str):\n        use_tqdm (bool): Show progress bar.\n        overwrite (bool): Overwrite existing file.\n        nan_to_null (bool): Change nan values to nulls.\n        **kwargs: other arguments to tqdm.\n    \"\"\"\n    filename = Path(filename)\n    assert not filename.exists() or overwrite, 'File already exists'\n    with filename.open('w') as f:\n        for j in progress_bar(jsons, disable=not use_tqdm, desc=f'Writing to {f.name}', **kwargs):\n            f.write(json.dumps(j) + '\\n')\n</code></pre>"},{"location":"code/utils/#taskchain.utils.io.iter_json_file","title":"<code>iter_json_file(filename, use_tqdm=True, **kwargs)</code>","text":"<p>Yield loaded jsons from <code>.jsonl</code> file (json lines).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Path | str</code> required <code>use_tqdm</code> <code>bool</code> <code>True</code> <code>**kwargs</code> <p>additional arguments to tqdm</p> <code>{}</code> Source code in <code>taskchain/utils/io.py</code> <pre><code>def iter_json_file(filename, use_tqdm=True, **kwargs):\n\"\"\"\n    Yield loaded jsons from `.jsonl` file (json lines).\n\n    Args:\n        filename (Path | str):\n        use_tqdm (bool):\n        **kwargs: additional arguments to tqdm\n\n    Returns:\n\n    \"\"\"\n    filename = Path(filename)\n    with filename.open() as f:\n        for row in progress_bar(f, disable=not use_tqdm, desc=f'Reading from {f.name}', **kwargs):\n            yield json.loads(row.strip())\n</code></pre>"},{"location":"code/utils/#taskchain.utils.iter","title":"<code>taskchain.utils.iter</code>","text":""},{"location":"code/utils/#taskchain.utils.iter.list_or_str_to_list","title":"<code>list_or_str_to_list(value)</code>","text":"<p>Helper function for cases where list of string is expected but single string is also ok.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[None, List[str], str]</code> required <p>Returns:</p> Type Description <code>List[str]</code> <p>original list or original string in list</p> Source code in <code>taskchain/utils/iter.py</code> <pre><code>def list_or_str_to_list(value: Union[None, List[str], str]) -&gt; List[str]:\n\"\"\" Helper function for cases where list of string is expected but single string is also ok.\n\n    Args:\n        value:\n\n    Returns:\n        original list or original string in list\n    \"\"\"\n    if isinstance(value, str):\n        return [value]\n    return value\n</code></pre>"},{"location":"code/utils/#taskchain.utils.migration","title":"<code>taskchain.utils.migration</code>","text":""},{"location":"code/utils/#taskchain.utils.migration.migrate_to_parameter_mode","title":"<code>migrate_to_parameter_mode(config, target_dir, dry=True, verbose=True)</code>","text":"<p>Migrate a chain to parameter mode.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>config defining the chain</p> required <code>target_dir</code> <p>dir to migrate data to</p> required <code>dry</code> <code>bool</code> <p>show only info, do not copy data</p> <code>True</code> <code>verbose</code> <code>bool</code> <code>True</code> Source code in <code>taskchain/utils/migration.py</code> <pre><code>def migrate_to_parameter_mode(config: Config, target_dir, dry: bool = True, verbose: bool = True):\n\"\"\"\n    Migrate a chain to parameter mode.\n\n    Args:\n        config: config defining the chain\n        target_dir: dir to migrate data to\n        dry: show only info, do not copy data\n        verbose:\n    \"\"\"\n    assert config.base_dir != target_dir, 'target_dir has to be different from configs base_dir'\n    old_chain = {\n        t.fullname: t\n        for t in config.chain(parameter_mode=False).tasks.values()\n    }\n    new_chain = {\n        t.fullname: t\n        for t in Config(target_dir, config._filepath, global_vars=config.global_vars, context=config.context).chain().tasks.values()\n    }\n    print(f'Set dry=False to make copies')\n    for name, old_task in old_chain.items():\n        print()\n        new_task = new_chain[name]\n        print(f'{name}  -  {new_task.name_for_persistence}')\n        if verbose:\n            print(f'  parameters: `{new_task.params.repr}`')\n            print(f' input tasks: `{\"###\".join(f\"{n}={it}\" for n, it in sorted(new_task.get_config().input_tasks.items()))}`')\n\n        if issubclass(old_task.data_class, InMemoryData):\n            print('   not persisting')\n            continue\n\n        if not old_task.has_data:\n            print('   no data found')\n            continue\n\n        print(f'\\n    original: `{old_task.data_path}`')\n        print(f'      target: `{new_task.data_path}`')\n\n        if new_task.has_data:\n            # HACK: pd files do not have to have the same size with the same data\n            if new_task.data_path.name.endswith('.pd'):\n                assert isclose(new_task.data_path.stat().st_size, old_task.data_path.stat().st_size, rel_tol=2e-7, abs_tol=10), f'{new_task.data_path.stat().st_size} vs. {old_task.data_path.stat().st_size}'\n            else:\n                assert new_task.data_path.stat().st_size == old_task.data_path.stat().st_size\n            print(f'    target already exists')\n            continue\n\n        if dry:\n            print('    to copy')\n        else:\n            print('    copying')\n            if old_task.data_path.is_file():\n                copyfile(old_task.data_path, new_task.data_path)\n            else:\n                copytree(old_task.data_path, new_task.data_path)\n            print('    copied')\n</code></pre>"},{"location":"code/utils/#taskchain.utils.threading","title":"<code>taskchain.utils.threading</code>","text":""},{"location":"code/utils/#taskchain.utils.threading.parallel_map","title":"<code>parallel_map(fun, iterable, threads=10, sort=True, use_tqdm=True, desc='Running tasks in parallel.', total=None, chunksize=1000)</code>","text":"<p>Map function to iterable in multiple threads.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>function to apply</p> required <code>iterable</code> <code>Iterable</code> required <code>threads</code> <code>int</code> <p>number of threads</p> <code>10</code> <code>sort</code> <code>bool</code> <p>return values in same order as itarable</p> <code>True</code> <code>use_tqdm</code> <code>bool</code> <p>show progressbar</p> <code>True</code> <code>desc</code> <code>str</code> <p>text of progressbar</p> <code>'Running tasks in parallel.'</code> <code>total</code> <code>int</code> <p>size of iterable to allow show better progressbar</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <p>of returned values by fce</p> Source code in <code>taskchain/utils/threading.py</code> <pre><code>def parallel_map(\n    fun: Callable,\n    iterable: Iterable,\n    threads: int = 10,\n    sort: bool = True,\n    use_tqdm: bool = True,\n    desc: str = 'Running tasks in parallel.',\n    total: int = None,\n    chunksize: int = 1000,\n):\n\"\"\"\n    Map function to iterable in multiple threads.\n\n    Args:\n        fun: function to apply\n        iterable:\n        threads: number of threads\n        sort: return values in same order as itarable\n        use_tqdm: show progressbar\n        desc: text of progressbar\n        total: size of iterable to allow show better progressbar\n\n    Returns:\n        list: of returned values by fce\n    \"\"\"\n    iterable = iterable\n    if isinstance(iterable, list) and total is None:\n        total = len(iterable)\n    if threads == 1:\n        return [fun(v) for v in (tqdm(iterable, desc=desc, total=total, maxinterval=2) if use_tqdm else iterable)]\n\n    def _fun(i, arg):\n        return i, fun(arg)\n\n    pbar = tqdm(desc=desc, total=total, maxinterval=2) if use_tqdm else None\n\n    async def _run(chunk):\n        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n            loop = asyncio.get_event_loop()\n            futures = [loop.run_in_executor(executor, _fun, i, input_value) for i, input_value in enumerate(chunk)]\n            result = []\n            for output_value in asyncio.as_completed(futures):\n                to_append = await output_value\n                if use_tqdm:\n                    pbar.update()\n                result.append(to_append)\n            return result\n        gc.collect()\n\n    loop = asyncio.get_event_loop()\n    result = []\n    for chunk in chunked(iterable, chunksize=chunksize):\n        chunk_result = loop.run_until_complete(_run(chunk))\n        for _, res in sorted(chunk_result, key=lambda ires: ires[0]) if sort else chunk_result:\n            result.append(res)\n    return result\n</code></pre>"},{"location":"code/utils/#taskchain.utils.threading.parallel_starmap","title":"<code>parallel_starmap(fun, iterable, **kwargs)</code>","text":"<p>Allows use <code>parallel_map</code> for function with multiple arguments.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <code>Callable</code> <p>function with multiple arguments</p> required <code>iterable</code> <code>Iterable</code> <p>lists or tuples of arguments</p> required Source code in <code>taskchain/utils/threading.py</code> <pre><code>def parallel_starmap(fun: Callable, iterable: Iterable, **kwargs):\n\"\"\"\n    Allows use `parallel_map` for function with multiple arguments.\n\n    Args:\n        fun: function with multiple arguments\n        iterable: lists or tuples of arguments\n    \"\"\"\n\n    def _call(d):\n        return fun(*d)\n\n    return parallel_map(_call, iterable, **kwargs)\n</code></pre>"}]}